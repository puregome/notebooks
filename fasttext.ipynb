{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext tweet classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pipes\n",
    "import random\n",
    "import re\n",
    "import tempfile\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE = \"distance\"\n",
    "FACEMASK = \"mondkapje\"\n",
    "TOPIC = DISTANCE # <====================================\n",
    "\n",
    "DATADIR = \"../data/\"\n",
    "if TOPIC == FACEMASK: FILETWEETS = TOPIC+\"-tweets+nunl.csv\"\n",
    "else: FILETWEETS = TOPIC+\"-tweets.csv\"\n",
    "FILEANNOTATIONS = FILETWEETS+\".human-labels.txt\"\n",
    "FILEFASTTEXT = \"fasttext-\"+TOPIC+\".csv\"\n",
    "LABELPREFIX = \"__label__\"\n",
    "LARGEINT = 9999999999\n",
    "LABEL = \"label\"\n",
    "TEXT = \"text\"\n",
    "USER = \"user\"\n",
    "IDSTR = \"id_str\"\n",
    "IRRELEVANT = \"IRRELEVANT\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "ANDERS = \"ANDERS\"\n",
    "EENS = \"EENS\"\n",
    "ONEENS = \"ONEENS\"\n",
    "SUPPORTS = \"SUPPORTS\"\n",
    "REJECTS = \"REJECTS\"\n",
    "SOURCE = \"source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(DATADIR+FILETWEETS,index_col=IDSTR)\n",
    "annotations = pd.read_csv(DATADIR+FILEANNOTATIONS,header=None,sep=\" \")\n",
    "mainAnnotator = annotations.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    text = re.sub(r\"\\\\n\",\" \",text)\n",
    "    text = re.sub(r\"https://\\S+\",\"\",text)\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return(text)\n",
    "\n",
    "def tokenize(text):\n",
    "    return(\" \".join(TweetTokenizer().tokenize(text)))\n",
    "\n",
    "def preprocess(text):\n",
    "    return(tokenize(cleanup(text)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttextData = {}\n",
    "for i in range(0,len(annotations)):\n",
    "    annotator = annotations.iloc[i][0]\n",
    "    tweetId = annotations.iloc[i][2]\n",
    "    if annotator == mainAnnotator and tweetId in list(tweets.index):\n",
    "        tweetUser = tweets.loc[tweetId][1]\n",
    "        tweetLabel = annotations.iloc[i][4]\n",
    "        if tweetLabel == NEUTRAL: tweetLabel = IRRELEVANT\n",
    "        if tweetLabel == ANDERS: tweetLabel = IRRELEVANT\n",
    "        if tweetLabel == EENS: tweetLabel = SUPPORTS\n",
    "        if tweetLabel == ONEENS: tweetLabel = REJECTS\n",
    "        fasttextData[tweetId] = {LABEL:LABELPREFIX+tweetLabel,\\\n",
    "                                 USER:tweetUser,\\\n",
    "                                 TEXT:preprocess(tweets.loc[tweetId][TEXT])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5977, 6835, 7286)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fasttextData),len(tweets),len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open(FILEFASTTEXT,\"w\")\n",
    "seenTexts = {}\n",
    "for tweetId in fasttextData:\n",
    "    text = cleanup(fasttextData[tweetId][TEXT])\n",
    "    if not text in seenTexts:\n",
    "        print(fasttextData[tweetId][LABEL],text,file=outFile)\n",
    "        seenTexts[text] = True\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets annotated per minute: 5.3 ; 1000 tweets take: 189 minutes\n"
     ]
    }
   ],
   "source": [
    "annotations1 = annotations[annotations[0]==mainAnnotator]\n",
    "nbrOfAnnotationMinutes = len(set([str(x)[:12] for x in annotations1[1]]))\n",
    "nbrOfAnnotatedTweets = len(set([str(x)[:12] for x in annotations1[2]]))\n",
    "print(\"tweets annotated per minute:\",round(nbrOfAnnotatedTweets/nbrOfAnnotationMinutes,1),\"; 1000 tweets take:\",\\\n",
    "      round(1000/(nbrOfAnnotatedTweets/nbrOfAnnotationMinutes)),\"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweets annotated per minute: 3.6 ; 1000 tweets take: 278 minutes\n"
     ]
    }
   ],
   "source": [
    "annotations2 = annotations[annotations[0]!=mainAnnotator]\n",
    "nbrOfAnnotationMinutes = len(set([str(x)[:12] for x in annotations2[1]]))\n",
    "nbrOfAnnotatedTweets = len(set([str(x)[:12] for x in annotations2[2]]))\n",
    "print(\"tweets annotated per minute:\",round(nbrOfAnnotatedTweets/nbrOfAnnotationMinutes,1),\"; 1000 tweets take:\",\\\n",
    "      round(1000/(nbrOfAnnotatedTweets/nbrOfAnnotationMinutes)),\"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext run and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINEDDIR = \"/home/erikt/projects/newsgac/fasttext-runs/\"\n",
    "WIKIFILENAME = \"wiki.nl.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5731"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttextData = []\n",
    "inFile = open(FILEFASTTEXT,\"r\")\n",
    "for line in inFile: fasttextData.append(line.strip())\n",
    "inFile.close()\n",
    "len(fasttextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 100\n",
    "EPOCH = 100\n",
    "LR = 0.05\n",
    "N = 10\n",
    "TRAIN = \"TRAIN\"+str(int(random.random()*LARGEINT))\n",
    "TEST = \"TEST\"+str(int(random.random()*LARGEINT))\n",
    "VALIDATION = \"VALIDATION\"+str(int(random.random()*LARGEINT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranSelect(inList,size):\n",
    "    outList = []\n",
    "    selectionList = list(inList)\n",
    "    while len(outList) < size and len(selectionList) > 0:\n",
    "        index = int(random.random()*len(selectionList))\n",
    "        outList.append(selectionList[index])\n",
    "        del(selectionList[index])\n",
    "    return(outList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFasttext(fasttextData,dim=DIM,epoch=EPOCH,lr=LR,n=N,squealFlag=True,maxTrain=None):\n",
    "    predictionCountsTest = []\n",
    "    predictionLabelsTest = []\n",
    "    predictionCountsValidation = []\n",
    "    predictionLabelsValidation = []\n",
    "    for fold in range(0,n):\n",
    "        if squealFlag:\n",
    "            clear_output(wait=True)\n",
    "            print(\"starting fold\",fold)\n",
    "        validationStart = round(fold*len(fasttextData)/n)\n",
    "        validationEnd = round((fold+1)*len(fasttextData)/n)\n",
    "        if fold < n-1: nextFold = fold+1\n",
    "        else: nextFold = 0\n",
    "        testStart = round(nextFold*len(fasttextData)/n)\n",
    "        testEnd = round((nextFold+1)*len(fasttextData)/n)\n",
    "        trainFile = open(TRAIN,\"w\")\n",
    "        testFile = open(TEST,\"w\")\n",
    "        validationFile = open(VALIDATION,\"w\")\n",
    "        trainData = []\n",
    "        validationData = []\n",
    "        testData = []\n",
    "        for i in range(0,len(fasttextData)):\n",
    "            data = fasttextData[i]\n",
    "            if i >= testStart and i < testEnd: \n",
    "                print(data,file=testFile)\n",
    "                testData.append(data)\n",
    "            elif i >= validationStart and i < validationEnd: \n",
    "                print(data,file=validationFile)\n",
    "                validationData.append(data)\n",
    "            else: \n",
    "                print(data,file=trainFile)\n",
    "                trainData.append(data)\n",
    "        testFile.close()\n",
    "        trainFile.close()\n",
    "        validationFile.close()\n",
    "        if not maxTrain == None and maxTrain < len(trainData):\n",
    "            trainData = ranSelect(trainData,maxTrain)\n",
    "            trainFile = open(TRAIN,\"w\")\n",
    "            for i in range(0,len(trainData)):\n",
    "                print(trainData[i],file=trainFile)\n",
    "            trainFile.close()\n",
    "        model = fasttext.train_supervised(TRAIN, dim=dim, epoch=epoch, lr=lr, pretrainedVectors=\"twiqs-model-2020.vec\")\n",
    "        predictionCountsValidation.append([*model.test(VALIDATION)])\n",
    "        predictionLabelsValidation.append(model.predict(validationData))\n",
    "        predictionCountsTest.append([*model.test(TEST)])\n",
    "        predictionLabelsTest.append(model.predict(testData))\n",
    "        print(fold, end=\" \")\n",
    "        evaluate(predictionCountsTest, predictionLabelsTest, fasttextData[testStart:testEnd])\n",
    "        os.unlink(TRAIN)\n",
    "        os.unlink(TEST)\n",
    "        os.unlink(VALIDATION)\n",
    "        pd.DataFrame(predictionLabelsValidation).to_csv(\"csv/predictionLabelsValidation.csv\")\n",
    "        pd.DataFrame(predictionLabelsTest).to_csv(\"csv/predictionLabelsTest.csv\")\n",
    "    if squealFlag:\n",
    "        clear_output(wait=True)\n",
    "        print(\"finished\")\n",
    "    return(predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrix(fasttextData,predictionLabels):\n",
    "    goldLabels = pd.Series([fasttextData[i].split()[0] for i in range(0,len(fasttextData))])\n",
    "    predictedLabels = pd.Series([x[0] for row in predictionLabels for x in row[0]])\n",
    "    return(pd.crosstab(goldLabels,predictedLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictionCounts,predictionLabels,fasttextData,printResults=True):\n",
    "    caseTotal = 0\n",
    "    pTotal = 0\n",
    "    rTotal = 0\n",
    "    for i in range(0,len(predictionCounts)):\n",
    "        caseTotal += predictionCounts[i][0]\n",
    "        pTotal += predictionCounts[i][0]*predictionCounts[i][1]\n",
    "        rTotal += predictionCounts[i][0]*predictionCounts[i][2]\n",
    "    precision = round(pTotal/caseTotal,3)\n",
    "    recall = round(rTotal/caseTotal,3)\n",
    "    cf = makeConfusionMatrix(fasttextData,predictionLabels)\n",
    "    for label in ['__label__REJECTS','__label__SUPPORTS','__label__IRRELEVANT']:\n",
    "        if not label in cf: cf[label] = [0 for i in range(0,len(cf))]\n",
    "    numberOfPredictedSupportLabels = sum([cf.iloc[i]['__label__SUPPORTS'] for i in range(0,len(cf))])\n",
    "    numberOfGoldSupportLabels = sum(cf.loc['__label__SUPPORTS'])\n",
    "    numberOfPredictedRejectLabels = sum([cf.iloc[i]['__label__REJECTS'] for i in range(0,len(cf))])\n",
    "    numberOfGoldRejectLabels = sum(cf.loc['__label__REJECTS'])\n",
    "    fraction = (numberOfPredictedRejectLabels/numberOfPredictedSupportLabels)/(numberOfGoldRejectLabels/numberOfGoldSupportLabels)\n",
    "    if printResults: \n",
    "        print(\"cases: {0}; precision: {1:0.3f}; recall: {2:0.3f}; fraction: {3:0.3f}\".format(caseTotal,precision,recall,fraction))\n",
    "    return((precision,fraction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresValidation = {}\n",
    "scoresTest = {}\n",
    "for dim in [10,20,50,100,200,300]:\n",
    "    for epoch in [10,20,50,100,200]:\n",
    "        for lr in [0.05,0.1,0.2]:\n",
    "            predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest = \\\n",
    "                runFasttext(fasttextData,dim=dim,epoch=epoch,lr=lr,squealFlag=False)\n",
    "            key = \",\".join([str(dim),str(epoch),str(lr)])\n",
    "            print(f\"{dim:3} {epoch:3} {lr:0.2f}\",end=\" validation: \")\n",
    "            scoresValidation[key] = evaluate(predictionCountsValidation,predictionLabelsValidation,fasttextData)\n",
    "            print(f\"{dim:3} {epoch:3} {lr:0.2f}\",end=\"       test: \")\n",
    "            scoresTest[key] = evaluate(predictionCountsTest,predictionLabelsTest,fasttextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"optimizing accuracy via validation data\")\n",
    "[(scoresValidation[k],k) for k in sorted(scoresValidation.keys(),\\\n",
    "                                         key=lambda k:scoresValidation[k][0],reverse=True)][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"checking best accuracy for test data\")\n",
    "[(scoresTest[k],k) for k in sorted(scoresTest.keys(),\\\n",
    "                                         key=lambda k:scoresTest[k][0],reverse=True)][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facemasks: best validation accuracy 0.56 for '20,50,0.1', associated test accuracy 0.56 (5th score).\n",
    "\n",
    "Social distancing: best validation accuracy 0.65 for '300,10,0.2', associated test accuracy 0.65 (top score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"optimizing fraction via validation data\")\n",
    "[(scoresValidation[k],k) for k in sorted(scoresValidation.keys(),\n",
    "                                         key=lambda k:scoresValidation[k][1],reverse=True)][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"checking fraction for test data\")\n",
    "[(scoresTest[k],k) for k in sorted(scoresTest.keys(),\n",
    "                                         key=lambda k:scoresTest[k][1],reverse=True)][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facemasks: best validation ratio: 1.48 for '10,200,0.2'; associated test ratio: 1.70 (8th scsore)\n",
    "\n",
    "Social distancing: best validation ratio 0.88 for '200,200,0.2'; associated test accuracy 0.83 (top score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check relation accuracy and fraction (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.62\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "ax1 = plt.subplot(121)\n",
    "x1 = [scoresTest[key][0] for key in scoresValidation]\n",
    "y1 = [scoresTest[key][1] for key in scoresValidation]\n",
    "ax1.scatter(x1,y1)\n",
    "ax2 = plt.subplot(122)\n",
    "x2 = [scoresTest[key][0] for key in scoresValidation if scoresTest[key][0] > THRESHOLD]\n",
    "y2 = [scoresTest[key][1] for key in scoresValidation if scoresTest[key][0] > THRESHOLD]\n",
    "ax2.scatter(x2,y2)\n",
    "plt.xlabel(\"fraction\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x1,y1)[0][1],np.corrcoef(x2,y2)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "scoresN = {} \n",
    "triplets = [(20,300,0.2),(50,200,0.2),(300,200,0.2),(20,200,0.05),(10,300,0.2)]\n",
    "\n",
    "for triplet in triplets:\n",
    "    dim,epoch,lr = triplet\n",
    "    key = \" \".join([str(dim),str(epoch),str(lr)])\n",
    "    scoresN[key] = []\n",
    "    for i in range(0,N): \n",
    "        predictionCounts,predictionLabels = runFasttext(fasttextData,dim=dim,epoch=epoch,lr=lr,squealFlag=False)\n",
    "        print(dim,epoch,lr,end=\" \")\n",
    "        scoresN[key].append(evaluate(predictionCounts,predictionLabels,fasttextData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresNaverage = {}\n",
    "for key in scoresN: scoresNaverage[key] = round(np.average([x[1] for x in scoresN[key]]),3)\n",
    "{key:scoresNaverage[key] for key in sorted(scoresNaverage.keys(),key=lambda k:scoresNaverage[k],reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [10]:\n",
    "    for epoch in [900]:\n",
    "        for lr in [0.2]:\n",
    "            predictionCounts,predictionLabels = runFasttext(fasttextData,dim=dim,epoch=epoch,lr=lr,squealFlag=False)\n",
    "            print(dim,epoch,lr,end=\" \")\n",
    "            key = \",\".join([str(dim),str(epoch),str(lr)])\n",
    "            scores[key] = evaluate(predictionCounts,predictionLabels,fasttextData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with best parameters (skip, except for first block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setBestParameters():\n",
    "    if TOPIC == DISTANCE:\n",
    "        # social distancing (202008: 20,300,0.9) (202009a:20,10,0.05) (202009b:10,10,0.2) (202010:200,200,0.2) (emnlp:10,10,0.2)\n",
    "        BESTDIM = 300 # 20210304: was 200\n",
    "        BESTEPOCH = 200\n",
    "        BESTLR = 0.2\n",
    "    elif TOPIC == FACEMASK:\n",
    "        # facemasks (202008: 10,900,0.2) (202010:10,200,0.2) (emnlp:300,10,0.2)\n",
    "        BESTDIM = 300 # from emnlp\n",
    "        BESTEPOCH = 10\n",
    "        BESTLR = 0.2\n",
    "    else:\n",
    "        print(\"unknown topic!\",file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "    return(BESTDIM,BESTEPOCH,BESTLR)\n",
    "        \n",
    "BESTDIM,BESTEPOCH,BESTLR = setBestParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17915\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2150225 lr:  0.000000 avg.loss:  0.002696 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cases: 573; precision: 0.763; recall: 0.763; fraction: 1.117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17712\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2096472 lr:  0.000000 avg.loss:  0.002432 ETA:   0h 0m 0s% words/sec/thread: 2186427 lr:  0.174010 avg.loss:  0.009796 ETA:   0h 1m45s 17.1% words/sec/thread: 2115132 lr:  0.165897 avg.loss:  0.007813 ETA:   0h 1m43s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 cases: 1146; precision: 0.668; recall: 0.668; fraction: 0.401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17526\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2094014 lr:  0.000000 avg.loss:  0.002215 ETA:   0h 0m 0s 0.113827 avg.loss:  0.003358 ETA:   0h 1m11s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 cases: 1719; precision: 0.661; recall: 0.661; fraction: 0.639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17711\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2101702 lr:  0.000000 avg.loss:  0.002535 ETA:   0h 0m 0s 24.7% words/sec/thread: 2137944 lr:  0.150505 avg.loss:  0.005931 ETA:   0h 1m33s 82.0% words/sec/thread: 2106315 lr:  0.035902 avg.loss:  0.002778 ETA:   0h 0m22s100.0% words/sec/thread: 2101703 lr: -0.000000 avg.loss:  0.002535 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 cases: 2293; precision: 0.647; recall: 0.647; fraction: 0.571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17796\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2091839 lr:  0.000000 avg.loss:  0.002329 ETA:   0h 0m 0s 46.4% words/sec/thread: 2111494 lr:  0.107200 avg.loss:  0.003374 ETA:   0h 1m 7s 85.8% words/sec/thread: 2093422 lr:  0.028350 avg.loss:  0.002462 ETA:   0h 0m17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 cases: 2866; precision: 0.647; recall: 0.647; fraction: 0.354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17704\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2105888 lr:  0.000000 avg.loss:  0.002258 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 cases: 3439; precision: 0.647; recall: 0.647; fraction: 0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17881\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2078664 lr:  0.000000 avg.loss:  0.002333 ETA:   0h 0m 0s ETA:   0h 0m25s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 cases: 4012; precision: 0.653; recall: 0.653; fraction: 0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  18290\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2117978 lr:  0.000000 avg.loss:  0.002495 ETA:   0h 0m 0s 0.028339 ETA:   0h 1m53s  6.1% words/sec/thread: 2162775 lr:  0.187900 avg.loss:  0.017829 ETA:   0h 1m55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 cases: 4585; precision: 0.646; recall: 0.646; fraction: 0.374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  18096\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2108698 lr:  0.000000 avg.loss:  0.002612 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 cases: 5158; precision: 0.646; recall: 0.646; fraction: 0.387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  17811\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2106539 lr:  0.000000 avg.loss:  0.002474 ETA:   0h 0m 0s 0.057752 avg.loss:  0.002883 ETA:   0h 0m36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 cases: 5731; precision: 0.648; recall: 0.648; fraction: 0.848\n"
     ]
    }
   ],
   "source": [
    "predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest = \\\n",
    "    runFasttext(fasttextData, dim=BESTDIM, epoch=BESTEPOCH, lr=BESTLR, squealFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest = \\\n",
    "    runFasttext(fasttextData, dim=BESTDIM, epoch=BESTEPOCH, lr=BESTLR, squealFlag=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases: 5731; precision: 0.651; recall: 0.651; fraction: 0.910\n",
      "cases: 5731; precision: 0.648; recall: 0.648; fraction: 0.841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.648, 0.8405158775232037)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(predictionCountsValidation,predictionLabelsValidation,fasttextData)\n",
    "evaluate(predictionCountsTest,predictionLabelsTest,fasttextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for text in fasttextData:\n",
    "    gold.append(re.sub(\" .*\", \"\", text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for fold in range(0, len(predictionLabelsValidation)):\n",
    "    for label in predictionLabelsValidation[fold][0]:\n",
    "        predicted.append(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []\n",
    "for fold in range(0, len(predictionLabelsTest)):\n",
    "    for label in predictionLabelsTest[fold-1][0]:\n",
    "        predicted.append(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = {}\n",
    "wrong = {}\n",
    "missed = {}\n",
    "for i in range(0, len(predicted)):\n",
    "    if predicted[i] == gold[i]:\n",
    "        if gold[i] not in correct:\n",
    "            correct[gold[i]] = 0\n",
    "        correct[gold[i]] += 1\n",
    "    else:\n",
    "        if gold[i] not in missed:\n",
    "            missed[gold[i]] = 0\n",
    "        missed[gold[i]] += 1\n",
    "        if predicted[i] not in wrong:\n",
    "            wrong[predicted[i]] = 0\n",
    "        wrong[predicted[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preci Recal Label\n",
      "45.5% 41.0% IRRELEVANT\n",
      "58.5% 52.9% REJECTS\n",
      "73.5% 79.1% SUPPORTS\n"
     ]
    }
   ],
   "source": [
    "print(\"Preci Recal Label\")\n",
    "for label in sorted(set(list(missed.keys()) + list(wrong.keys()) + list(correct.keys()))):\n",
    "    if label not in correct:\n",
    "        correct[label] = 0\n",
    "    if label not in missed:\n",
    "        missed[label] = 0\n",
    "    if label not in wrong:\n",
    "        wrong[label] = 0\n",
    "    print(f\"{100*correct[label]/(correct[label]+wrong[label]):0.1f}% {100*correct[label]/(correct[label]+missed[label]):0.1f}%\", end=\" \")\n",
    "    print(re.sub(\"__label__\", \"\", label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Precision | Recall | Label | dataset    |\n",
    "| --------- | ------ | ------| ---------- |\n",
    "| 46.6% | 40.8% | IRRELEVANT | validation |\n",
    "| 57.5% | 55.7% | REJECTS    | validation |\n",
    "| 73.9% | 78.7% | SUPPORTS   | validation |\n",
    "| 45.5% | 41.0% | IRRELEVANT | test       |\n",
    "| 58.5% | 52.9% | REJECTS    | test       |\n",
    "| 73.5% | 79.1% | SUPPORTS   | test       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest = \\\n",
    "    runFasttext(fasttextData, dim=BESTDIM, epoch=BESTEPOCH, lr=BESTLR, squealFlag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCountsGold = {}\n",
    "for i in range(0,len(fasttextData)):\n",
    "    label = fasttextData[i].split()[0]\n",
    "    if label in labelCountsGold: labelCountsGold[label] += 1\n",
    "    else: labelCountsGold[label] = 1\n",
    "labelCountsGold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"baseline accuracy:\",round(max(labelCountsGold.values())/sum(labelCountsGold.values()),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCountsPredicted = {}\n",
    "for i in range(0,len(predictionLabelsTest)):\n",
    "    for label in predictionLabelsTest[i][0]:\n",
    "        if label[0] in labelCountsPredicted: labelCountsPredicted[label[0]] += 1\n",
    "        else: labelCountsPredicted[label[0]] = 1\n",
    "labelCountsPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"total absolute deviation\",sum([abs(labelCountsPredicted[l]-labelCountsGold[l]) for l in labelCountsGold]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"relative deviation per label:\",\\\n",
    "      {l:round(abs(labelCountsPredicted[l]-labelCountsGold[l])/labelCountsGold[l],3) for l in labelCountsGold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeConfusionMatrix(fasttextData,predictionLabelsTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the mondkapje data, fasttext predicts 63% of the labels correctly without external dictionary and 66% with a Wikipedia dictionary (baseline: 46%). It overestimates the presence of negative labels and underestimates the level of positive and neutral labels. The amount of irrelevant labels is about right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiplicationFactors = {}\n",
    "for label in labelCountsGold:\n",
    "    multiplicationFactors[label] = labelCountsGold[label]/labelCountsPredicted[label]\n",
    "multiplicationFactors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mondkapje tweets, the multiplication factor for the positive label is unrealistically high so we will not use these factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedNumberOfLabels = 0\n",
    "for label in labelCountsGold:\n",
    "    predictedNumberOfLabels += multiplicationFactors[label]*labelCountsPredicted[label]\n",
    "print(predictedNumberOfLabels,sum(labelCountsGold.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation training data size - accuracy (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "expResultsAll = {}\n",
    "for trainingSize in [100,200,500,1000,2000,5000,10000]:\n",
    "    expResultsTrainingSize = []\n",
    "    for i in range(0,N):\n",
    "        selection = ranSelect(fasttextData,trainingSize)\n",
    "        predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest = \\\n",
    "            runFasttext(fasttextData,dim=BESTDIM,epoch=BESTEPOCH,lr=BESTLR,squealFlag=False,maxTrain=trainingSize)\n",
    "        expResultsTrainingSize.append(evaluate(predictionCountsTest,predictionLabelsTest,fasttextData,printResults=False))\n",
    "    average = (np.average([x[0] for x in expResultsTrainingSize]),np.average([x[1] for x in expResultsTrainingSize]))\n",
    "    expResultsAll[len(selection)] = average\n",
    "    print(\"{0:4} {1:0.3f} {2:0.3f}\".format(len(selection),average[0],average[1]))\n",
    "    if len(selection) >= len(fasttextData): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(expResultsAll[list(expResultsAll.keys())[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTFILENAME = \"training-size.png\"\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "plt.plot(list(expResultsAll.keys()),[x[0] for x in list(expResultsAll.values())])\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"training data size\")\n",
    "plt.title(\"Accuracy related to training size\")\n",
    "\n",
    "ax1 = plt.subplot(122)\n",
    "plt.plot(list(expResultsAll.keys()),[x[1] for x in list(expResultsAll.values())])\n",
    "plt.xscale(\"log\")\n",
    "plt.ylabel(\"fraction\")\n",
    "plt.xlabel(\"training data size\")\n",
    "plt.title(\"Fraction score related to training size\")\n",
    "plt.savefig(PLOTFILENAME)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying unlabeled tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BESTDIM,BESTEPOCH,BESTLR = setBestParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = \"/home/erikt/projects/puregome/data/\"\n",
    "DATADIRECTORY = BASEDIR+\"text/\"\n",
    "DATADIRECTORYREDDIT = BASEDIR+\"reddit/text/\"\n",
    "DATADIRECTORYNUNL = BASEDIR+\"nunl/text/\"\n",
    "TWITTER = \"twitter\"\n",
    "REDDIT = \"reddit\"\n",
    "NUNL = \"nunl\"\n",
    "SOURCES = [TWITTER,REDDIT,NUNL]\n",
    "DATADIRECTORIES = {TWITTER:DATADIRECTORY,REDDIT:DATADIRECTORYREDDIT,NUNL:DATADIRECTORYNUNL}\n",
    "FILEFASTTEXT = \"fasttext-\"+TOPIC+\".csv\"\n",
    "LABELPREFIX = \"__label__\"\n",
    "NEGATIVE = LABELPREFIX+\"NEGATIVE\"\n",
    "POSITIVE = LABELPREFIX+\"POSITIVE\"\n",
    "NEUTRAL = LABELPREFIX+\"NEUTRAL\"\n",
    "IRRELEVANT = LABELPREFIX+\"IRRELEVANT\"\n",
    "SUPPORTS = LABELPREFIX+\"SUPPORTS\"\n",
    "REJECTS = LABELPREFIX+\"REJECTS\"\n",
    "TEXT = \"text\"\n",
    "USER = \"user\"\n",
    "TOTAL = \"total\"\n",
    "DIM = BESTDIM\n",
    "EPOCH = BESTEPOCH\n",
    "LR = BESTLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    text = re.sub(r\"\\\\n\",\" \",text)\n",
    "    text = re.sub(r\"https://\\S+\",\"\",text)\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return(text)\n",
    "\n",
    "def tokenize(text):\n",
    "    return(\" \".join(TweetTokenizer().tokenize(text)))\n",
    "\n",
    "def preprocess(text):\n",
    "    return(tokenize(cleanup(text)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCREENNAME = \"screenname\"\n",
    "\n",
    "def makeGrepCommandFromQuery(query):\n",
    "    grepCommand = \"grep\"\n",
    "    for orPart in query.split(\"|\"):\n",
    "        grepCommand += ' -e \"'+orPart+'\"'\n",
    "    return(grepCommand)\n",
    "\n",
    "def readData(datePattern,query,dataDirectory=DATADIRECTORY):\n",
    "    fileList = sorted(os.listdir(dataDirectory))\n",
    "    testData = []\n",
    "    for inFileName in fileList:\n",
    "        if re.search(datePattern,inFileName) and os.path.exists(dataDirectory+inFileName):\n",
    "            try:\n",
    "                fileData = pd.read_csv(dataDirectory+inFileName).drop_duplicates()\n",
    "                matchedText = fileData[fileData[TEXT].str.contains(query,case=False)]\n",
    "                matchedTextPreprocessed = matchedText[TEXT].apply(lambda x:preprocess(x))\n",
    "                testData.extend(list(matchedTextPreprocessed))\n",
    "            except:\n",
    "                pass\n",
    "    return(testData)\n",
    "\n",
    "def readDataGeo(datePattern,query,dataDirectory=DATADIRECTORY):\n",
    "    fileList = sorted(os.listdir(dataDirectory))\n",
    "    testData = []\n",
    "    locations = []\n",
    "    for inFileName in fileList:\n",
    "        if re.search(datePattern,inFileName) and os.path.exists(dataDirectory+inFileName):\n",
    "            try:\n",
    "                fileData = pd.read_csv(dataDirectory+inFileName).drop_duplicates()\n",
    "                matchedText = fileData[fileData[TEXT].str.contains(query,case=False)]\n",
    "                matchedTextPreprocessed = matchedText[TEXT].apply(lambda x:preprocess(x))\n",
    "                testData.extend(matchedTextPreprocessed)\n",
    "            except:\n",
    "                pass\n",
    "    return(testData)\n",
    "\n",
    "def classify(datePattern,query,model,dataDirectory=DATADIRECTORY):\n",
    "    testData = readData(datePattern,query,dataDirectory)\n",
    "    predictedLabels = model.predict(testData)\n",
    "    predictedGroups = pd.DataFrame(predictedLabels[0]).groupby(0).groups\n",
    "    labelCountsPredicted = {label:len(predictedGroups[label]) for label in predictedGroups}\n",
    "    nbrOfLabels = sum([labelCountsPredicted[label] for label in labelCountsPredicted if label != IRRELEVANT])\n",
    "    labelPercentages = { label:round(100*labelCountsPredicted[label]/nbrOfLabels,1) for label in labelCountsPredicted}\n",
    "    labelPercentages[TOTAL] = nbrOfLabels\n",
    "    return(labelPercentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "RESULTSFILE = \"results-\"+TOPIC+\".csv\"\n",
    "\n",
    "\n",
    "def storeResults(results, results_file=RESULTSFILE):\n",
    "     pd.DataFrame(results).T.to_csv(results_file, index_label=SOURCE)\n",
    "\n",
    "        \n",
    "def readResults(results_file=RESULTSFILE):\n",
    "    try:\n",
    "        df = pd.read_csv(results_file, index_col=SOURCE)\n",
    "        for source in df.index:\n",
    "            for date in df.loc[source].index:\n",
    "                try:\n",
    "                    df.loc[source][date] = ast.literal_eval(df.loc[source][date])\n",
    "                except: pass\n",
    "        return(removeNan(df.T.to_dict()))\n",
    "    except:\n",
    "        return({})\n",
    "\n",
    "\n",
    "def combineResults(results_target, results_source):\n",
    "    for source in results_source:\n",
    "        if not source in results_target:\n",
    "            results_target[source] = {}\n",
    "        for date in results_source[source]:\n",
    "            if date in results_target[source]:\n",
    "                print(f\"duplicate date: {date}!\")\n",
    "            else:\n",
    "                results_target[source][date] = results_source[source][date]\n",
    "\n",
    "            \n",
    "def getTotals(results):\n",
    "    totals = {}\n",
    "    for source in results:\n",
    "        for date in results[source]:\n",
    "            try:\n",
    "                if not source in totals: totals[source] = 0\n",
    "                totals[source] += results[source][date][TOTAL]\n",
    "            except: pass\n",
    "    return(totals)\n",
    "\n",
    "\n",
    "def removeNan(results):\n",
    "    for source in results:\n",
    "        toBeDeleted = []\n",
    "        for date in results[source]:\n",
    "            if results[source][date] != results[source][date]:\n",
    "                toBeDeleted.append(date)\n",
    "        for date in toBeDeleted: del(results[source][date])\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results2021 = readResults()\n",
    "#results2020 = readResults(results_file=RESULTSFILE + \".20201231\")\n",
    "#results = {}\n",
    "#combineResults(results, results2020)\n",
    "#combineResults(results, results2021)\n",
    "#totals =  getTotals(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  20694\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread: 2131359 lr:  0.000000 avg.loss:  0.002630 ETA:   0h 0m 0s words/sec/thread: 2142272 lr:  0.079598 avg.loss:  0.003744 ETA:   0h 0m49s\n"
     ]
    }
   ],
   "source": [
    "MAXDAYSPERMONTH = 31\n",
    "MODELFILE = f\"model-{TOPIC}-{BESTDIM}-{BESTEPOCH}-{BESTLR}.bin\"\n",
    "\n",
    "if TOPIC == DISTANCE:\n",
    "    QUERY = \"1[.,]5[ -]*m|afstand.*hou|hou.*afstand|anderhalve[ -]*meter\"\n",
    "elif TOPIC == FACEMASK:\n",
    "    QUERY = TOPIC\n",
    "else:\n",
    "    print(\"unknown topic!\",file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "MAXDAYSPERMONTH = 31\n",
    "\n",
    "model = fasttext.train_supervised(FILEFASTTEXT, dim=BESTDIM, epoch=BESTEPOCH, lr=BESTLR, pretrainedVectors=\"twiqs-model-2020.vec\")\n",
    "model.save_model(MODELFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(MODELFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recompute results per data source (run once per month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twitter 20200101 6.8 93.2 272.7 44\n",
      "twitter 20200102 0 100.0 737.0 27\n",
      "twitter 20200103 25.0 75.0 445.0 20\n",
      "twitter 20200104 15.4 84.6 196.2 26\n",
      "twitter 20200105 9.3 90.7 496.3 54\n",
      "twitter 20200106 3.4 96.6 166.1 59\n",
      "twitter 20200107 6.5 93.5 256.5 46\n",
      "twitter 20200108 4.3 95.7 228.3 46\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "totals = {}\n",
    "for source in SOURCES:\n",
    "    dataDirectory = DATADIRECTORIES[source]\n",
    "    total = 0\n",
    "    results[source] = {}\n",
    "    for month in \"202001 202002 202003 202004 202005 202006 202007 202008 202009 202010 202011 202012 202101 202102 202103\".split():\n",
    "        for day in range(1,MAXDAYSPERMONTH+1):\n",
    "            date = month+str(day).zfill(2)\n",
    "            try:\n",
    "                labels = classify(date,QUERY,model,dataDirectory=dataDirectory)\n",
    "                if not REJECTS in labels: labels[REJECTS] = 0\n",
    "                if not SUPPORTS in labels: labels[SUPPORTS] = 0\n",
    "                if not IRRELEVANT in labels: labels[IRRELEVANT] = 0\n",
    "                if labels[REJECTS]+labels[SUPPORTS]+labels[IRRELEVANT] > 0:\n",
    "                    results[source][date] = labels\n",
    "                    print(source,date,labels[REJECTS],labels[SUPPORTS],labels[IRRELEVANT],labels[TOTAL])\n",
    "                    total += labels[TOTAL]\n",
    "            except:\n",
    "                print(source,date,\"error\")\n",
    "    totals[source] = total\n",
    "    print(\"total relevant found:\",total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeResults(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for platform in \"twitter reddit nunl\".split():\n",
    "    counts = {}\n",
    "    for date in results[platform]:\n",
    "        try:\n",
    "            if TOTAL in results[platform][date] and results[platform][date][TOTAL] > 0:\n",
    "                for label in results[platform][date].keys():\n",
    "                    if not label in counts: counts[label] = 0\n",
    "                    if label == TOTAL: counts[label] += results[platform][date][label]\n",
    "                    else: counts[label] += round(results[platform][date][TOTAL]*results[platform][date][label]/100.0)\n",
    "        except: pass\n",
    "    print(platform,counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Social distancing:\n",
    "```\n",
    "twitter {'__label__IRRELEVANT': 321240, '__label__REJECTS': 292168, '__label__SUPPORTS': 707633, 'total': 999800}\n",
    "reddit {'__label__SUPPORTS': 2967, 'total': 3857, '__label__REJECTS': 890, '__label__IRRELEVANT': 411}\n",
    "nunl {'__label__IRRELEVANT': 7760, '__label__SUPPORTS': 39671, 'total': 53243, '__label__REJECTS': 13572}\n",
    "```\n",
    "Face masks:\n",
    "```\n",
    "twitter {'__label__IRRELEVANT': 821039, '__label__REJECTS': 791113, 'total': 910603, '__label__SUPPORTS': 119490}\n",
    "reddit {'__label__IRRELEVANT': 2368, '__label__REJECTS': 1697, '__label__SUPPORTS': 496, 'total': 2193}\n",
    "nunl {'__label__IRRELEVANT': 27926, '__label__REJECTS': 32008, '__label__SUPPORTS': 11438, 'total': 43446}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(RESULTSFILE,index_col=\"source\").T\n",
    "totalsIncludingIrrelevant = {source:0 for source in [TWITTER,REDDIT,NUNL]}\n",
    "for i in range(0,len(df)):\n",
    "    for source in [TWITTER,REDDIT,NUNL]:\n",
    "        try:\n",
    "            dictionary = ast.literal_eval(df.iloc[i][source])\n",
    "        except:\n",
    "            dictionary = {TOTAL:0}\n",
    "        if dictionary[TOTAL] > 0:\n",
    "            totalsIncludingIrrelevant[source] += round((1+dictionary[IRRELEVANT]/100)*dictionary[TOTAL])\n",
    "totalsIncludingIrrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(numberList,size):\n",
    "    newList = []\n",
    "    start_offset = int(size/2)\n",
    "    for i in range(0,len(numberList)):\n",
    "        newList.append(numberList[i])\n",
    "        numbersUsed = 1\n",
    "        for j in range(-start_offset,size-start_offset):\n",
    "            if i+j >= 0 and i+j < len(numberList) and i+j != i:\n",
    "                newList[i] += numberList[i+j]\n",
    "                numbersUsed += 1\n",
    "        newList[i] /= numbersUsed\n",
    "    return(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_number(number):\n",
    "    digits = str(number)\n",
    "    pretty_digits = \"\"\n",
    "    for i in range(0,len(digits)):\n",
    "        if i > 0 and i % 3 == 0: pretty_digits = \",\"+pretty_digits\n",
    "        pretty_digits = digits[-1-i]+pretty_digits\n",
    "    return(pretty_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATEFORMAT = \"%Y%m%d\"\n",
    "DAYSCOMBINED = 30\n",
    "EXTRASMOOTHDAYS = 7\n",
    "STARTDATE = \"20200310\"\n",
    "LABELS = {TWITTER: f\"Twitter ({pretty_number(totals[TWITTER])})\",\\\n",
    "          NUNL: f\"Nu.nl ({pretty_number(totals[NUNL])})\",\\\n",
    "          REDDIT: f\"Reddit ({pretty_number(totals[REDDIT])})\"}\n",
    "if TOPIC == DISTANCE:\n",
    "    PLOTFILENAME = \"social-distancing-all.png\"\n",
    "elif TOPIC == FACEMASK:\n",
    "    PLOTFILENAME = \"mondkapje-all.png\"\n",
    "else:\n",
    "    print(\"unknown topic!\",file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "x = {}\n",
    "ySupports = {}\n",
    "plt.subplots(figsize=(12,6))\n",
    "font = {\"size\":16}\n",
    "matplotlib.rc(\"font\",**font)\n",
    "ax = plt.subplot(111)\n",
    "for source in [REDDIT,NUNL,TWITTER]:\n",
    "    x[source] = [datetime.datetime.strptime(k,DATEFORMAT) for k in results[source].keys()]\n",
    "    ySupports[source] = movingAverage(movingAverage([results[source][k][SUPPORTS] for k in results[source].keys()],DAYSCOMBINED),EXTRASMOOTHDAYS)\n",
    "    xPart = [x[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    ySupportsPart = [ySupports[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    plt.plot_date(xPart,ySupportsPart,fmt=\"-\",label=LABELS[source])\n",
    "plt.legend(framealpha=0.3,loc=\"lower right\")\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.xlabel(\"day/month\")\n",
    "if TOPIC == FACEMASK:\n",
    "    plt.title(\"Support for general public not wearing face masks per medium over time\")\n",
    "elif TOPIC == DISTANCE:\n",
    "    plt.title(\"Support for social distancing per medium over time (average over \"+str(DAYSCOMBINED)+\" days)\")\n",
    "else:\n",
    "    print(\"unknown topic!\",file=sys.stderr)\n",
    "    sys.exit(1)    \n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%-d/%-m\"))\n",
    "plt.savefig(PLOTFILENAME)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not run any further code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNOTATED = \"annotated\"\n",
    "ANNOTATEDFILE = \"manual-annotation.csv\"\n",
    "DATE = \"date\"\n",
    "EENS = \"EENS\"\n",
    "ONEENS = \"ONEENS\"\n",
    "ANDERS = \"ANDERS\"\n",
    "TOTAL = \"total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ANNOTATEDFILE,index_col=DATE,dtype={EENS:\"float\",ONEENS:\"float\",ANDERS:\"float\"})\n",
    "totals = []\n",
    "for i in range(0,len(df)):\n",
    "    total = df.iloc[i][EENS]+df.iloc[i][ONEENS]+df.iloc[i][ANDERS]\n",
    "    if df.iloc[i][EENS]+df.iloc[i][ONEENS] > 0:\n",
    "        df.iloc[i][EENS] = 100*df.iloc[i][EENS]/(total-df.iloc[i][ANDERS])\n",
    "        df.iloc[i][ONEENS] = 100*df.iloc[i][ONEENS]/(total-df.iloc[i][ANDERS])\n",
    "        df.iloc[i][ANDERS] = 100*df.iloc[i][ANDERS]/(total-df.iloc[i][ANDERS])\n",
    "    else: \n",
    "        df.iloc[i][ANDERS] = 100\n",
    "    totals.append(total)\n",
    "df[TOTAL] = totals\n",
    "annotatedDict = df.T.to_dict(orient=\"dict\")\n",
    "annotatedDict = {d:annotatedDict[d] for d in sorted(annotatedDict.keys())}\n",
    "results[ANNOTATED] = annotatedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTFILEANNOTATED = \"manual-annotation.png\"\n",
    "\n",
    "x = {}\n",
    "ySupports = {}\n",
    "plt.subplots(figsize=(12,6))\n",
    "font = {\"size\":12}\n",
    "matplotlib.rc(\"font\",**font)\n",
    "ax = plt.subplot(111)\n",
    "for source in [TWITTER]:\n",
    "    x[source] = [datetime.datetime.strptime(k,DATEFORMAT) for k in results[source].keys()]\n",
    "    ySupports[source] = movingAverage([results[source][k][SUPPORTS] for k in results[source].keys()],DAYSCOMBINED)\n",
    "    xPart = [x[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    ySupportsPart = [ySupports[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    plt.plot_date(xPart,ySupportsPart,fmt=\"-\",label=\"predicted\")\n",
    "for source in [ANNOTATED]:\n",
    "    x[source] = [datetime.datetime.strptime(str(k),DATEFORMAT) for k in results[source].keys()]\n",
    "    ySupports[source] = movingAverage([results[source][k][EENS] for k in results[source].keys()],DAYSCOMBINED)\n",
    "    xPart = [x[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    ySupportsPart = [ySupports[source][i] for i in range(0,len(x[source])) if x[source][i] >= datetime.datetime.strptime(STARTDATE,DATEFORMAT)] \n",
    "    plt.plot_date(xPart,ySupportsPart,fmt=\"-\",label=ANNOTATED)\n",
    "plt.title(\"Support for social distancing per medium over time (average over \"+str(DAYSCOMBINED)+\" days)\")\n",
    "plt.legend()\n",
    "plt.savefig(PLOTFILEANNOTATED)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotatedValues = []\n",
    "twitterValues = []\n",
    "\n",
    "for i in range(0,len(x[TWITTER])):\n",
    "    date = x[TWITTER][i]\n",
    "    if date >= datetime.datetime.strptime(\"20200310\",DATEFORMAT):\n",
    "        for j in range(0,len(x[ANNOTATED])):\n",
    "            if x[ANNOTATED][j] == date:\n",
    "                twitterValues.append(ySupports[TWITTER][i])\n",
    "                annotatedValues.append(ySupports[ANNOTATED][j])\n",
    "np.corrcoef(annotatedValues,twitterValues)[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTFILENAME = \"plot.png\"\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(annotatedValues,twitterValues)\n",
    "plt.ylabel(\"Predicted support percentages\")\n",
    "plt.xlabel(\"Annotated support percentages\")\n",
    "plt.title(\"Annotated and predicted support for social distancing\")\n",
    "plt.savefig(PLOTFILENAME)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allResults = results\n",
    "results = results[TWITTER]\n",
    "\n",
    "x = [datetime.datetime.strptime(k,DATEFORMAT) for k in results.keys()]\n",
    "ySupports = movingAverage([results[k][SUPPORTS] for k in results.keys()],DAYSCOMBINED)\n",
    "yRejects = movingAverage([results[k][REJECTS] for k in results.keys()],DAYSCOMBINED)\n",
    "yIrrelevant = movingAverage([results[k][IRRELEVANT] for k in results.keys()],DAYSCOMBINED)\n",
    "\n",
    "keyDates = []\n",
    "for i in range(0,len(yIrrelevant)):\n",
    "    if i < len(yIrrelevant)-1 and \\\n",
    "       (yIrrelevant[i] < 100 and yIrrelevant[i+1] >= 100 or yIrrelevant[i] >= 100 and yIrrelevant[i+1] < 100):\n",
    "        keyDates.append(list(results.keys())[i])\n",
    "        print(keyDates[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STARTDATE = \"20200310\" # keyDates[0]\n",
    "ENDDATE = \"20201231\"\n",
    "\n",
    "results = {k:results[k] for k in sorted(results.keys())}\n",
    "font = {\"size\":14}\n",
    "matplotlib.rc(\"font\",**font)\n",
    "plt.subplots(figsize=(14,7))\n",
    "\n",
    "ax1 = plt.subplot(111)\n",
    "plt.plot_date(x,ySupports,fmt=\"-\",label=\"Supports\")\n",
    "plt.plot_date(x,yRejects,fmt=\"-\",label=\"Rejects\")\n",
    "plt.plot_date(x,yIrrelevant,fmt=\"-\",label=\"Other\")\n",
    "plt.plot_date(x,[100 for i in x],fmt=\"-\",label=\"100%\",color=\"black\")\n",
    "for date in [STARTDATE]:\n",
    "    plt.plot_date([datetime.datetime.strptime(date,DATEFORMAT)],[100],color=\"black\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.xlabel(\"day/month\")\n",
    "plt.title(\"stance on the RIVM policy on \"+\"social\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTFILEOUT = \"social-distancing.png\"\n",
    "#PLOTFILEOUT = \"mondkapje.png\"\n",
    "\n",
    "plt.subplots(figsize=(14,7))\n",
    "\n",
    "dates = [d for d in results.keys()]\n",
    "xPart = [x[i] for i in range(0,len(dates)) if dates[i] >= STARTDATE and dates[i] <= ENDDATE]\n",
    "ySupportsPart = [ySupports[i] for i in range(0,len(dates)) if dates[i] >= STARTDATE and dates[i] <= ENDDATE] \n",
    "yRejectsPart = [yRejects[i] for i in range(0,len(dates)) if dates[i] >= STARTDATE and dates[i] <= ENDDATE]\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(\"%-d/%-m\"))\n",
    "\n",
    "ax2 = plt.subplot(111)\n",
    "plt.plot_date(xPart,ySupportsPart,fmt=\"-\",label=\"Supports\")\n",
    "plt.plot_date(xPart,yRejectsPart,fmt=\"-\",label=\"Rejects\")\n",
    "plt.legend()\n",
    "plt.ylabel(\"percentage\")\n",
    "plt.xlabel(\"day/month\")\n",
    "plt.title(\"Twitter stance on the RIVM policy on social distancing (average over \"+str(DAYSCOMBINED)+\" days)\")\n",
    "plt.xticks([datetime.datetime.strptime(d,DATEFORMAT) for d in \"20200301 2020401 20200501 20200601 20200701 20200801\".split()])\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter(\"%-d/%-m\"))\n",
    "\n",
    "plt.savefig(PLOTFILEOUT)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = {}\n",
    "for text in fasttextData:\n",
    "    label = text.split()[0]\n",
    "    if label in counts: counts[label] += 1\n",
    "    else: counts[label] = 1\n",
    "for label in counts: print(round(counts[label]/len(fasttextData),3),label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotator comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFTESTDATA = 100\n",
    "\n",
    "annotators = []\n",
    "annotator_data = {}\n",
    "kappa_data = [{},{}]\n",
    "blocked_tweet_ids = {}\n",
    "tweet_labels = {}\n",
    "main_annotator = \"\"\n",
    "for row in annotations.iterrows():\n",
    "    annotator = row[1][0]\n",
    "    tweet_id = row[1][2]\n",
    "    tweet_label = row[1][4]\n",
    "    if main_annotator == \"\":\n",
    "        main_annotator = annotator\n",
    "    if tweet_label == \"NEUTRAL\":\n",
    "        tweet_label = \"ANDERS\"\n",
    "    if not annotator in annotator_data: \n",
    "        annotator_data[annotator] = {}\n",
    "        annotators.append(annotator)\n",
    "    annotator_data[annotator][tweet_id] = tweet_label\n",
    "    if len(annotator_data[annotator]) <= NBROFTESTDATA: \n",
    "        blocked_tweet_ids[tweet_id] = True\n",
    "    if tweet_id not in blocked_tweet_ids:\n",
    "        if tweet_id not in tweet_labels:\n",
    "            tweet_labels[tweet_id] = {}\n",
    "        tweet_labels[tweet_id][annotator] = tweet_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for annotator in annotator_data:\n",
    "    label_count = {}\n",
    "    for tweet_id in annotator_data[annotator]:\n",
    "        label = annotator_data[annotator][tweet_id]\n",
    "        if not label in label_count: \n",
    "            label_count[label] = 0\n",
    "        label_count[label] += 1\n",
    "    results.append({\"total\": len(annotator_data[annotator])})\n",
    "    for label in sorted(label_count):\n",
    "        results[-1][label] = round(label_count[label] / len(annotator_data[annotator]),3)\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len({id_str: tweet_labels[id_str] for id_str in tweet_labels if len(tweet_labels[id_str]) > 1 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kappa_data_0 = [ tweet_labels[id_str][annotators[0]] for id_str in sorted(tweet_labels.keys()) if len(tweet_labels[id_str]) > 1 ]\n",
    "kappa_data_1 = [ tweet_labels[id_str][annotators[1]] for id_str in sorted(tweet_labels.keys()) if len(tweet_labels[id_str]) > 1 ]\n",
    "cohen_kappa_score(kappa_data_0, kappa_data_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX = 100\n",
    "\n",
    "for annotator in annotators:\n",
    "    if not annotator == main_annotator:\n",
    "        total = 0\n",
    "        identical = 0\n",
    "        matrix = {}\n",
    "        for tweet_id in annotator_data[main_annotator]:\n",
    "            if tweet_id in annotatorData[annotator]:\n",
    "                label_annotator = annotator_data[annotator][tweet_id]\n",
    "                label_main_annotator = annotator_data[main_annotator][tweet_id]\n",
    "                if label_main_annotator not in matrix:\n",
    "                    matrix[label_main_annotator] = {}\n",
    "                if label_annotator not in matrix[label_main_annotator]:\n",
    "                    matrix[label_main_annotator][label_annotator] = 0\n",
    "                matrix[label_main_annotator][label_annotator] += 1\n",
    "        matrix = { label:matrix[label] for label in sorted(matrix.keys()) }\n",
    "        for label in matrix:\n",
    "            matrix[label] = { l:matrix[label][l] for l in sorted(matrix[label].keys()) }\n",
    "        print(pd.DataFrame(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
