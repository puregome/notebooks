{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fasttext tweet classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data file creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"mondkapje\"\n",
    "FILETWEETS = TOPIC+\"-tweets.csv\"\n",
    "FILEANNOTATIONS = \"human-labels-\"+TOPIC+\"-tweets.txt\"\n",
    "FILEFASTTEXT = \"fasttext-\"+TOPIC+\".csv\"\n",
    "LABELPREFIX = \"__label__\"\n",
    "LABEL = \"label\"\n",
    "TEXT = \"text\"\n",
    "USER = \"user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(FILETWEETS,header=None,index_col=0)\n",
    "annotations = pd.read_csv(FILEANNOTATIONS,header=None,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttextData = {}\n",
    "for i in range(0,len(annotations)):\n",
    "    tweetId = annotations.iloc[i][2]\n",
    "    tweetLabel = annotations.iloc[i][4]\n",
    "    if tweetId in list(tweets.index):\n",
    "        tweetLabel = annotations.iloc[i][4]\n",
    "        tweetUser = tweets.loc[tweetId][1]\n",
    "        fasttextData[tweetId] = {LABEL:LABELPREFIX+tweetLabel,USER:tweetUser,TEXT:\" \".join(TweetTokenizer().tokenize(tweets.loc[tweetId][2]))}\n",
    "\n",
    "outFile = open(FILEFASTTEXT,\"w\")\n",
    "seenTexts = {}\n",
    "for tweetId in fasttextData:\n",
    "    if not fasttextData[tweetId][TEXT] in seenTexts:\n",
    "        print(fasttextData[tweetId][LABEL],fasttextData[tweetId][USER],fasttextData[tweetId][TEXT],file=outFile)\n",
    "        seenTexts[fasttextData[tweetId][TEXT]] = True\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINEDDIR = \"/home/erikt/projects/newsgac/fasttext-runs/\"\n",
    "WIKIFILENAME = \"wiki.nl.vec\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttextData = []\n",
    "inFile = open(FILEFASTTEXT,\"r\")\n",
    "for line in inFile: fasttextData.append(line.strip())\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "DIM = 300\n",
    "N = 10\n",
    "TRAIN = \"TRAIN\"\n",
    "TEST = \"TEST\"\n",
    "\n",
    "results = []\n",
    "labels = []\n",
    "for fold in range(0,N):\n",
    "    clear_output(wait=True)\n",
    "    print(\"starting fold\",fold)\n",
    "    testStart = round(fold*len(fasttextData)/N)\n",
    "    testEnd = round((fold+1)*len(fasttextData)/N)\n",
    "    trainFile = open(TRAIN,\"w\")\n",
    "    testFile = open(TEST,\"w\")\n",
    "    testData = []\n",
    "    for i in range(0,len(fasttextData)):\n",
    "        if i < testStart or i >= testEnd: print(fasttextData[i],file=trainFile)\n",
    "        else: \n",
    "            print(fasttextData[i],file=testFile)\n",
    "            testData.append(fasttextData[i])\n",
    "    testFile.close()\n",
    "    trainFile.close()\n",
    "    model = fasttext.train_supervised(TRAIN,dim=DIM,epoch=100,pretrainedVectors=PRETRAINEDDIR+WIKIFILENAME)\n",
    "    results.append([*model.test(TEST)])\n",
    "    labels.append(model.predict(testData))\n",
    "clear_output(wait=True)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cases: 593; precision: 0.663; recall: 0.663\n"
     ]
    }
   ],
   "source": [
    "caseTotal = 0\n",
    "pTotal = 0\n",
    "rTotal = 0\n",
    "for i in range(0,len(results)):\n",
    "    caseTotal += results[i][0]\n",
    "    pTotal += results[i][0]*results[i][1]\n",
    "    rTotal += results[i][0]*results[i][2]\n",
    "print(\"cases: {0}; precision: {1}; recall: {2}\".format(caseTotal,round(pTotal/caseTotal,3),round(rTotal/caseTotal,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCounts = {}\n",
    "for i in range(0,len(fasttextData)):\n",
    "    label = fasttextData[i].split()[0]\n",
    "    if label in labelCounts: labelCounts[label] += 1\n",
    "    else: labelCounts[label] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__NEUTRAL': 53,\n",
       " '__label__IRRELEVANT': 235,\n",
       " '__label__NEGATIVE': 273,\n",
       " '__label__POSITIVE': 32}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCounts = {}\n",
    "for i in range(0,len(labels)):\n",
    "    for label in labels[i][0]:\n",
    "        if label[0] in labelCounts: labelCounts[label[0]] += 1\n",
    "        else: labelCounts[label[0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__label__NEUTRAL': 30,\n",
       " '__label__NEGATIVE': 325,\n",
       " '__label__IRRELEVANT': 230,\n",
       " '__label__POSITIVE': 8}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fasttext predicts 64% of the labels correctly without external dictionary and 66% with a Wikipedia dictionary (baseline: 46%). It overestimates the presence of negative labels and underestimates the level of positive and neutral labels. The amount of irrelevant labels is about right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
