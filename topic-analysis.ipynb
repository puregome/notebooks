{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic analysis\n",
    "\n",
    "Compare the word frequencies in one group of tweets with the word frequencies in another group of tweets to find out the topics discussed in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "sys.path.append(\"/home/erikt/projects/newsgac/fasttext-runs\")\n",
    "import tscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIRTEXT = \"/home/erikt/projects/puregome/data/text/\"\n",
    "DATADIRTOKENS = \"../data/tokens/\"\n",
    "DATADIRTOKENSTOPIC = \"../data/tokens-topic/\"\n",
    "DATEFORMAT = \"%Y%m%d\"\n",
    "FREQ = \"freq\"\n",
    "IDSTR = \"id_str\"\n",
    "NBROFEXAMPLES = 20\n",
    "TEXT = \"text\"\n",
    "TOKEN = \"token\"\n",
    "TOKENFILE = \"tokens.csv\"\n",
    "USER = \"user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: print(text)\n",
    "        \n",
    "def tokenize(text):\n",
    "    return(TweetTokenizer().tokenize(text))\n",
    "\n",
    "def isInterestingToken(token):\n",
    "    return(re.search(r\"[a-z]\",token) and not re.search(\"http\",token))\n",
    "\n",
    "def getTokenCountsFromTweets(datePattern,query=\"\",baseDict={}):\n",
    "    tokenCounts = dict(baseDict)\n",
    "    fileList = sorted(os.listdir(DATADIRTEXT))\n",
    "    for inFileName in fileList:\n",
    "        if re.search(datePattern,inFileName):\n",
    "            squeal(inFileName)\n",
    "            df = pd.read_csv(DATADIRTEXT+inFileName,index_col=IDSTR)\n",
    "            for i in range(0,len(df)):\n",
    "                text = df.iloc[i][TEXT]\n",
    "                if re.search(query,text):\n",
    "                    for token in set(tokenize(text.lower())): \n",
    "                        if isInterestingToken(token):\n",
    "                            if not token in tokenCounts: tokenCounts[token] = 0\n",
    "                            tokenCounts[token] += 1\n",
    "    return(tokenCounts)\n",
    "\n",
    "def dictTopN(dictionary,N=NBROFEXAMPLES):\n",
    "    return([(x[1],x[0]) for x in dictionary.items()][0:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFTOKENS = \"totalFreq\"\n",
    "NBROFTYPES = \"nbrOfWords\"\n",
    "WORDFREQS = \"wordFreqs\"\n",
    "\n",
    "def makeTscoreData(tokenList):\n",
    "    data = { NBROFTOKENS:0, NBROFTYPES:0, WORDFREQS:{} }\n",
    "    for token in tokenList:\n",
    "        if not math.isnan(tokenList[token]):\n",
    "            data[WORDFREQS][token] = tokenList[token]\n",
    "            data[NBROFTYPES] += 1\n",
    "            data[NBROFTOKENS] += tokenList[token]\n",
    "    return(data)\n",
    "\n",
    "def sortTscores(tscores):\n",
    "    return({token:tscores[token] \\\n",
    "            for token in sorted(tscores.keys(),key=lambda t:tscores[t],reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDateFromFileName(fileName):\n",
    "    return(fileName[0:8])\n",
    "\n",
    "def sortDict(myDict):\n",
    "    return({m:myDict[m] for m in sorted(myDict.keys(),key=lambda m:myDict[m],reverse=True)})\n",
    "\n",
    "def writeTokenCounts(tokenCounts,outFileName):\n",
    "    pd.DataFrame.from_dict([tokenCounts]).T.to_csv(outFileName,index_label=TOKEN,header=[FREQ])\n",
    "    \n",
    "def readTokenCounts(date,dataDir=DATADIRTOKENS):\n",
    "    return(pd.read_csv(dataDir+date+FILESUFFIX,index_col=TOKEN).to_dict(orient=\"dict\")[FREQ])\n",
    "\n",
    "def combineTokenCounts(tokenCountsList):\n",
    "    if len(tokenCountsList) == 0: return({})\n",
    "    else:\n",
    "        tokenCountsOut = dict(tokenCountsList[0])\n",
    "        for tokenCounts in tokenCountsList[1:]:\n",
    "            for token in tokenCounts:\n",
    "                if token in tokenCountsOut: tokenCountsOut[token] += tokenCounts2[token]\n",
    "                else: tokenCountsOut[token] = tokenCounts[token]\n",
    "    return(tokenCountsOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store token counts for all tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATTERN = \"20200\"\n",
    "FILESUFFIX = \".csv.gz\"\n",
    "\n",
    "seen = {}\n",
    "inFileNames = sorted(os.listdir(DATADIRTEXT))\n",
    "for inFileName in inFileNames:\n",
    "    if re.search(FILEPATTERN,inFileName):\n",
    "        date = makeDateFromFileName(inFileName)\n",
    "        outFileName = DATADIRTOKENS+date+FILESUFFIX\n",
    "        if not date in seen and not os.path.exists(outFileName):\n",
    "            tokenCounts = sortDict(getTokenCountsFromTweets(date))\n",
    "            writeTokenCounts(tokenCounts,outFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store token counts for tweets with topic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATTERN = \"20200\"\n",
    "FILESUFFIX = \".csv.gz\"\n",
    "QUERYTOPIC = r\"corona|covid|mondkapje|rivm|blijfthuis|houvol|huisarts|flattenthecurve\"\n",
    "\n",
    "seen = {}\n",
    "inFileNames = sorted(os.listdir(DATADIRTEXT))\n",
    "for inFileName in inFileNames:\n",
    "    if re.search(FILEPATTERN,inFileName):\n",
    "        date = makeDateFromFileName(inFileName)\n",
    "        outFileName = DATADIRTOKENSTOPIC+date+FILESUFFIX\n",
    "        if not date in seen and not os.path.exists(outFileName):\n",
    "            writeTokenCounts({},outFileName)\n",
    "            tokenCounts = sortDict(getTokenCountsFromTweets(date,query=QUERYTOPIC))\n",
    "            writeTokenCounts(tokenCounts,outFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIRSENT = \"../data/sentiment/pattern/\"\n",
    "DEFAULTFILEPATTERN = \"\"\n",
    "SENTIMENT = \"sentiment\"\n",
    "COUNT = \"count\"\n",
    "\n",
    "def getSentimentQuery(query,filePattern=DEFAULTFILEPATTERN):\n",
    "    fileList = sorted(os.listdir(DATADIRSENT))\n",
    "    sentimentPerHour = {}\n",
    "    for inFileName in fileList:\n",
    "        if re.search(filePattern,inFileName):\n",
    "            squeal(inFileName)\n",
    "            try:\n",
    "                dfSent = pd.read_csv(DATADIRSENT+inFileName,header=None)\n",
    "                dfText = pd.read_csv(DATADIRTEXT+inFileName)\n",
    "            except: continue\n",
    "            dictSent = {dfSent.iloc[i][0]:dfSent.iloc[i][1] for i in range(0,len(dfSent))}\n",
    "            sentScores = {}\n",
    "            for i in range(0,len(dfText)):\n",
    "                if re.search(query,dfText.iloc[i][TEXT],flags=re.IGNORECASE):\n",
    "                    try:\n",
    "                        idStr = dfText.iloc[i][IDSTR]\n",
    "                        sentScore = dictSent[idStr] \n",
    "                        sentScores[idStr] = sentScore\n",
    "                    except: pass\n",
    "            if len(sentScores) > 0:\n",
    "                sentiment = sum(sentScores.values())/len(sentScores)\n",
    "                hour = inFileName[0:11]\n",
    "                sentimentPerHour[hour] = { SENTIMENT:sentiment, COUNT:len(sentScores) }\n",
    "    totalSentiment = sum([sentimentPerHour[hour][SENTIMENT]*sentimentPerHour[hour][COUNT] for hour in sentimentPerHour])\n",
    "    totalCount = sum([sentimentPerHour[hour][COUNT] for hour in sentimentPerHour])\n",
    "    if totalCount == 0: return(None)\n",
    "    else: return(totalSentiment/totalCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateStringToDate(dateString):\n",
    "    return(datetime.datetime.strptime(dateString,DATEFORMAT))\n",
    "\n",
    "def dateToDateString(date):\n",
    "    return(datetime.datetime.strftime(date,DATEFORMAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGETDATE = \"20200316\"\n",
    "PREVIOUSDATE = dateToDateString(dateStringToDate(TARGETDATE)+datetime.timedelta(days=-1))\n",
    "\n",
    "tokenCounts1 = readTokenCounts(PREVIOUSDATE,dataDir=DATADIRTOKENSTOPIC)\n",
    "tokenCounts2 = readTokenCounts(TARGETDATE,dataDir=DATADIRTOKENSTOPIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(88.80423155811405, '#coronanederland'),\n",
       " (61.75866525996601, '#coronavirusnl'),\n",
       " (57.03928319351592, 'toespraak'),\n",
       " (45.81650686351358, 'mondmaskers'),\n",
       " (42.89354718484161, 'lockdown'),\n",
       " (40.26474919586335, '#coronacrisis'),\n",
       " (39.534911354114726, '#lockdownnl'),\n",
       " (39.04831174086576, 'stand'),\n",
       " (38.88487436314697, 'tekst'),\n",
       " (38.39747500072537, '#coronapocolypse'),\n",
       " (36.65463396509332, '#rutte'),\n",
       " (36.63270044050758, 'immunity'),\n",
       " (36.49595768230355, 'herd'),\n",
       " (34.202150710629, 'groepsimmuniteit'),\n",
       " (34.112145489260165, 'coronacrisis'),\n",
       " (32.74197005731711, 'speech'),\n",
       " (32.32950958711355, 'zaken'),\n",
       " (31.007227730050985, '#covidー19'),\n",
       " (30.76991036460662, '#toespraak'),\n",
       " (29.435913937965974, 'zorgverleners')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tscores1 = makeTscoreData(tokenCounts1)\n",
    "tscores2 = makeTscoreData(tokenCounts2)\n",
    "dictTopN(sortTscores(tscore.computeTscore(tscores2,tscores1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200316-23.out.gz\n",
      "{'#coronapocalypse': 0.09766978228924934, 'intensive': 0.030272982893127423, 'familie': 0.03667318441136217, 'malieveld': 0.01722157962261319, 'corona-app': 0.10772500307276305, 'staat': 0.06519107142857139, 'wetenschappelijke': 0.04429563492063492, 'plessen': 0.0767361111111111, 'hazes': -0.025, 'noodfonds': 0.019901315789473687, 'klachten': -0.0741732804232804, 'gegevens': 0.30252083333333324, '@rivm': 0.05990722078327415, 'zorgveleners': None, 'kwijt': -0.007661616161616162, 'zorgverleners': -0.3113715277777778, 'stand': 0.05283312040805441, 'speech': -0.05129367559523808, 'zaken': 0.019641927083333333}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for token in \"stand speech zaken\".split():\n",
    "    scores[token] = getSentimentQuery(token,filePattern=TARGETDATE+\"-23\")\n",
    "    print(scores)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATEPATTERN = \"2020031[1245]\"\n",
    "QUERY = \"corona|covid|flattenthecurve|blijfthuis|rivm|mondkapje|huisarts|houvol|zorg\"\n",
    "NBROFEXAMPLES = 10\n",
    "\n",
    "def getExamples(datePattern,query1=QUERY,query2=\"\"):\n",
    "    fileList = sorted(os.listdir(DATADIR))\n",
    "    tweets = {}\n",
    "    for inFileName in fileList:\n",
    "        if re.search(datePattern,inFileName):\n",
    "            clear_output(wait=True)\n",
    "            print(inFileName)\n",
    "            df = pd.read_csv(DATADIR+inFileName,compression=\"gzip\",index_col=ID)\n",
    "            for i in range(0,len(df)):\n",
    "                text = df.iloc[i][TEXT]\n",
    "                if re.search(query1,text) and re.search(query2,text):\n",
    "                    if text in tweets: tweets[text] += 1\n",
    "                    else: tweets[text] = 1\n",
    "    return({tweet:tweets[tweet] for tweet in sorted(tweets.keys(),key=lambda t:tweets[t],reverse=True)})\n",
    "\n",
    "def getTokenCountsFromTweets(datePattern,query=\"\"):\n",
    "    tokenCounts = {}\n",
    "    fileList = sorted(os.listdir(DATADIR))\n",
    "    for inFileName in fileList:\n",
    "        if re.search(datePattern,inFileName):\n",
    "            squeal(inFileName)\n",
    "            df = pd.read_csv(DATADIR+inFileName,index_col=IDSTR)\n",
    "            for i in range(0,len(df)):\n",
    "                text = df.iloc[i][TEXT]\n",
    "                if re.search(query,text):\n",
    "                    for token in set(tokenize(text.lower())): \n",
    "                        if re.search(r\"[a-z]\",token):\n",
    "                            if not token in tokenCounts: tokenCounts[token] = 0\n",
    "                            tokenCounts[token] += 1\n",
    "    return(tokenCounts)\n",
    "\n",
    "def writeData(data,fileName):\n",
    "    pd.DataFrame(data).to_csv(fileName,index_label=TOKEN)\n",
    "    \n",
    "def readData(fileName):\n",
    "    return(pd.read_csv(fileName,index_col=TOKEN).to_dict())\n",
    "\n",
    "def dictTopN(dictionary,N=NBROFEXAMPLES):\n",
    "    return([(x[1],x[0]) for x in dictionary.items()][0:N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = readData(TOKENFILE)\n",
    "for month in \"202003 202004 202005\".split():\n",
    "    print(month)\n",
    "    tokens = makeData(month)\n",
    "    writeData(tokens,\"tokens\"+month+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscoreData = {}\n",
    "for date in tokens:\n",
    "    tscoreData[date] = makeTscoreData(tokens[date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTopN(sortTscores(tscore.computeTscore(tscoreData[\"20200312\"],tscoreData[\"20200311\"])),N=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTopN(sortTscores(tscore.computeTscore(tscoreData[\"20200315\"],tscoreData[\"20200314\"])),N=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "There is a clear impact of the national press conferences on the topic tweets of the two volume peak dates. On 20200312, both the event (*persconferentie*) and the speakers (*rutte* and *kabinet*) are present in the top 20 words selected by the tscore measure. The most important topic in the tweets was the dicussion about school closures (*scholen*, *kinderen*, *onderwijs*, *sluiten*, *ouders* and *onderwijspersoneel*). On 20200315 the main topic is the closure of bars and restaurants (*horeca*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTopN(getExamples(\"20200312\",query1=QUERY,query2=\"scholen\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictTopN(getExamples(\"20200313\",query1=QUERY,query2=\"scholen\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
