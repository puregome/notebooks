{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE eScience paper\n",
    "\n",
    "Compare stances over time derived from tweets with stances from other sources, for example RIVM behavioral studies.\n",
    "\n",
    "This notebook reuses code from the notebooks:\n",
    "* domain-adaptation.ipynb\n",
    "* fasttext.ipynb\n",
    "* tweet-counts.ipynb\n",
    "\n",
    "RIVM measures that were evaluated:\n",
    "* Naleving gedragsregels / Houden aan gedragsregels / Voldoende afstand houden van anderen\n",
    "* Naleving gedragsregels / Houden aan gedragsregels / Mondkapje in publieke binnenruimten\n",
    "* Naleving gedragsregels / Testen / Klachten totaal\n",
    "* Vaccinatiebereidheid / Totaal\n",
    "* (Naleving gedragsregels / Avondklok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "ANNOTATION_DIR = DATA_DIR + \"annotation/\"\n",
    "DISTANCE = \"distance\"\n",
    "FACEMASK = \"mondkapje\"\n",
    "TESTING = \"testing\"\n",
    "VACCINATION = \"vaccin\"\n",
    "IDSTR = \"id_str\"\n",
    "IRRELEVANT = \"ANDERS\"\n",
    "TOTAL = \"total\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    text = re.sub(r\"\\\\n\",\" \",text)\n",
    "    text = re.sub(r\"https://\\S+\",\"\",text)\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return(\" \".join(TweetTokenizer().tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return(tokenize(cleanup(text)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_annotations(annotations_in, tweets, main_annotator):\n",
    "    annotations_out = {}\n",
    "    for i, row in annotations_in.iterrows():\n",
    "        if row[0]== main_annotator:\n",
    "            id_str = row[2]\n",
    "            label = row[4]\n",
    "            if id_str in tweets.index:\n",
    "                annotations_out[id_str] = { \"label\": label, \"text\": preprocess(tweets.loc[id_str][\"text\"]) }\n",
    "    return annotations_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_topic(topic):\n",
    "    if topic == TESTING:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202003-202012.csv\"\n",
    "    elif topic == FACEMASK:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202003-202103-1000.csv\"\n",
    "    elif topic == VACCINATION:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202001-202101-1000.csv\"\n",
    "    elif topic == DISTANCE:\n",
    "        tweet_file = DATA_DIR + topic + \"-tweets.csv\"\n",
    "    else:\n",
    "        print(f\"read_fasttext_data: unknown topic: {topic}\")\n",
    "    tweets = pd.read_csv(tweet_file, index_col=IDSTR)\n",
    "    annotations = pd.read_csv(tweet_file + \".human-labels.txt\", header=None, sep=\" \")\n",
    "    main_annotator = annotations.iloc[0][0]\n",
    "    annotations = select_annotations(annotations, tweets, main_annotator)\n",
    "    return annotations, main_annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_annotations(annotations, topic):\n",
    "    pd.DataFrame([[key, annotations[key][\"label\"]] for key in annotations], columns=[\"id_str\", \"label\"]).to_csv(f\"data-{topic}.csv\", \n",
    "                                                                                                                index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_data(topics):\n",
    "    annotations = {}\n",
    "    for topic in topics:\n",
    "        annotations[topic], main_annotator = read_data_topic(topic)\n",
    "        print(f\"read topic {topic}: {len(annotations[topic])} annotations from {main_annotator}\")\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read topic distance: 5977 annotations from erikt@xs4all.nl\n",
      "read topic mondkapje: 1011 annotations from erikt@xs4all.nl\n",
      "read topic testing: 1181 annotations from erikt@xs4all.nl\n",
      "read topic vaccin: 1007 annotations from erikt@xs4all.nl\n"
     ]
    }
   ],
   "source": [
    "annotations = read_all_data([DISTANCE, FACEMASK, TESTING, VACCINATION])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import re\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"text\"\n",
    "TEXT_DIR = DATA_DIR + \"text/\"\n",
    "DISTANCE_QUERY = \"1[.,]5[ -]*m|afstand.*hou|hou.*afstand|anderhalve[ -]*meter\"\n",
    "FACEMASK_QUERY = \"mondkapje\"\n",
    "TEST_QUERY = r'\\btest|getest|sneltest|pcr'\n",
    "VACCIN_QUERY = \"vaccin|ingeÃ«nt|ingeent|inent|prik|spuit|bijwerking|-->|ðŸ’‰|pfizer|moderna|astrazeneca|astra|zeneca|novavax|biontech\"\n",
    "RIVM_ROUNDS = [ \"^2020041[7-9]|^2020042[0-4]\", \"^2020050[7-9]|^2020051[0-2]\", \"^2020052[7-9]|^2020053|^20200601\", \"^2020061[7-9]|^2020062[0-1]\",\n",
    "                \"^2020070[8-9]|^2020071[0-2]\", \"^20200819|^2020082[0-3]\",     \"^20200930|^2020100[1-4]\",          \"^2020111[1-5]\",\n",
    "                \"^2020123|^2021010[1-3]\",      \"^2021021[0-4]\",               \"^2021032[4-8]\",                    \"^2021050[5-9]\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_fasttext(date_pattern, query, data_directory=TEXT_DIR):\n",
    "    file_list = sorted(os.listdir(data_directory))\n",
    "    test_data = []\n",
    "    hours = 0\n",
    "    for in_file_name in file_list:\n",
    "        if re.search(date_pattern, in_file_name) and os.path.exists(data_directory + in_file_name):\n",
    "            try:\n",
    "                file_data = pd.read_csv(data_directory + in_file_name).drop_duplicates()\n",
    "                matched_text = file_data[file_data[TEXT].str.contains(query, case=False)]\n",
    "                matched_text_preprocessed = matched_text[TEXT].apply(lambda x: preprocess(x))\n",
    "                test_data.extend(list(matched_text_preprocessed))\n",
    "                hours += 1\n",
    "            except:\n",
    "                pass\n",
    "    return test_data, hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(date_pattern, query, model, data_directory=TEXT_DIR):\n",
    "    test_data, hours = read_data_fasttext(date_pattern, query, data_directory)\n",
    "    predicted_labels = model.predict(test_data)\n",
    "    predicted_groups = pd.DataFrame(predicted_labels[0]).groupby(0).groups\n",
    "    label_counts_predicted = { label: len(predicted_groups[label]) for label in predicted_groups}\n",
    "    nbr_of_labels = sum([ label_counts_predicted[label] for label in label_counts_predicted if label != IRRELEVANT ])\n",
    "    label_percentages = { label: round(100*label_counts_predicted[label] / nbr_of_labels, 1) for label in label_counts_predicted }\n",
    "    label_percentages[TOTAL] = nbr_of_labels\n",
    "    label_percentages[\"hours\"] = hours\n",
    "    return label_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fasttext_model(annotations):\n",
    "    _, path = tempfile.mkstemp()\n",
    "    data_file = open(path, \"w\")\n",
    "    for id_str in annotations:\n",
    "        print(f'__label__{annotations[id_str][\"label\"]} {preprocess(annotations[id_str][\"text\"])}', \n",
    "              file=data_file)\n",
    "    data_file.close()\n",
    "    model = fasttext.train_supervised(path, dim=300, pretrainedVectors=\"twiqs-model-2020.vec\")\n",
    "    os.remove(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"testing\"\n",
    "model = make_fasttext_model(annotations[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {}\n",
    "classifications[topic] = {}\n",
    "for date_pattern in RIVM_ROUNDS:\n",
    "    classifications[topic][date_pattern] = classify(date_pattern, TEST_QUERY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'testing': {'^2020041[7-9]|^2020042[0-4]': {'__label__ANDERS': 65.8,\n",
       "   '__label__EENS': 26.9,\n",
       "   '__label__ONEENS': 7.2,\n",
       "   'total': 54013,\n",
       "   'hours': 192},\n",
       "  '^2020050[7-9]|^2020051[0-2]': {'__label__ANDERS': 65.0,\n",
       "   '__label__EENS': 29.2,\n",
       "   '__label__ONEENS': 5.8,\n",
       "   'total': 35286,\n",
       "   'hours': 144},\n",
       "  '^2020052[7-9]|^2020053|^20200601': {'__label__ANDERS': 69.3,\n",
       "   '__label__EENS': 24.0,\n",
       "   '__label__ONEENS': 6.7,\n",
       "   'total': 24869,\n",
       "   'hours': 144},\n",
       "  '^2020061[7-9]|^2020062[0-1]': {'__label__ANDERS': 79.0,\n",
       "   '__label__EENS': 14.9,\n",
       "   '__label__ONEENS': 6.1,\n",
       "   'total': 13506,\n",
       "   'hours': 120},\n",
       "  '^2020070[8-9]|^2020071[0-2]': {'__label__ANDERS': 75.6,\n",
       "   '__label__EENS': 16.5,\n",
       "   '__label__ONEENS': 7.8,\n",
       "   'total': 12938,\n",
       "   'hours': 120},\n",
       "  '^20200819|^2020082[0-3]': {'__label__ANDERS': 68.9,\n",
       "   '__label__EENS': 15.3,\n",
       "   '__label__ONEENS': 15.8,\n",
       "   'total': 25829,\n",
       "   'hours': 120},\n",
       "  '^20200930|^2020100[1-4]': {'__label__ANDERS': 69.0,\n",
       "   '__label__EENS': 12.8,\n",
       "   '__label__ONEENS': 18.2,\n",
       "   'total': 43549,\n",
       "   'hours': 120},\n",
       "  '^2020111[1-5]': {'__label__ANDERS': 71.0,\n",
       "   '__label__EENS': 10.9,\n",
       "   '__label__ONEENS': 18.2,\n",
       "   'total': 23403,\n",
       "   'hours': 120},\n",
       "  '^2020123|^2021010[1-3]': {'__label__ANDERS': 71.5,\n",
       "   '__label__EENS': 11.4,\n",
       "   '__label__ONEENS': 17.1,\n",
       "   'total': 36295,\n",
       "   'hours': 120},\n",
       "  '^2021021[0-4]': {'__label__ANDERS': 74.3,\n",
       "   '__label__EENS': 11.7,\n",
       "   '__label__ONEENS': 14.0,\n",
       "   'total': 25673,\n",
       "   'hours': 120},\n",
       "  '^2021032[4-8]': {'__label__ANDERS': 69.5,\n",
       "   '__label__EENS': 16.8,\n",
       "   '__label__ONEENS': 13.8,\n",
       "   'total': 60705,\n",
       "   'hours': 119},\n",
       "  '^2021050[5-9]': {'__label__ANDERS': 80.1,\n",
       "   '__label__EENS': 9.5,\n",
       "   '__label__ONEENS': 10.4,\n",
       "   'total': 80368,\n",
       "   'hours': 120}}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = range(1, 1+len(classifications[topic]))\n",
    "y = [ classifications[topic][date_pattern][\"__label__EENS\"] / (classifications[topic][date_pattern][\"__label__EENS\"] + \n",
    "                                                               classifications[topic][date_pattern][\"__label__ONEENS\"] ) \n",
    "      for date_pattern in classifications[topic]]\n",
    "x_rivm = [6, 7, 8, 9, 10, 11, 12]\n",
    "y_rivm = [0.319, 0.318, 0.450, 0.527, 0.370, 0.454, 0.493]\n",
    "plt.plot(x, y)\n",
    "plt.plot(x_rivm, y_rivm)\n",
    "plt.title(topic)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code from domain-adaptation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import fasttext\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import scipy.stats\n",
    "import uuid\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"../data/\"\n",
    "ANNOTATION_DIR = DATADIR + \"annotation/\"\n",
    "DISTANCE = \"distance\"\n",
    "FACEMASK = \"mondkapje\"\n",
    "TESTING = \"testing\"\n",
    "VACCINATION = \"vaccin\"\n",
    "LONELY = \"eenzaam\"\n",
    "FILEFASTTEXT = \"fasttext.csv\"\n",
    "IDSTR = \"id_str\"\n",
    "IRRELEVANT = \"IRRELEVANT\"\n",
    "NEUTRAL = \"NEUTRAL\"\n",
    "ANDERS = \"ANDERS\"\n",
    "EENS = \"EENS\"\n",
    "ONEENS = \"ONEENS\"\n",
    "SUPPORTS = \"SUPPORTS\"\n",
    "REJECTS = \"REJECTS\"\n",
    "LABEL = \"label\"\n",
    "TEXT = \"text\"\n",
    "USER = \"user\"\n",
    "LABELPREFIX = \"__label__\"\n",
    "LARGEINT = 9999999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 300\n",
    "EPOCH = 200\n",
    "LR = 0.2\n",
    "N = 10\n",
    "TRAIN = \"TRAIN\"+str(int(random.random()*LARGEINT))\n",
    "TEST = \"TEST\"+str(int(random.random()*LARGEINT))\n",
    "VALIDATION = \"VALIDATION\"+str(int(random.random()*LARGEINT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(text):\n",
    "    text = re.sub(r\"\\\\n\",\" \",text)\n",
    "    text = re.sub(r\"https://\\S+\",\"\",text)\n",
    "    text = re.sub(r\"\\s+\",\" \",text)\n",
    "    text = text.strip()\n",
    "    return(text)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    return(\" \".join(TweetTokenizer().tokenize(text)))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    return(tokenize(cleanup(text)).lower())\n",
    "\n",
    "\n",
    "def select_annotations(tweets, annotations, target_annotator):\n",
    "    fasttext_data = {}\n",
    "    for i in range(0,len(annotations)):\n",
    "        annotator = annotations.iloc[i][0]\n",
    "        tweet_id = annotations.iloc[i][2]\n",
    "        if annotator == target_annotator and tweet_id in list(tweets.index):\n",
    "            tweet_user = tweets.loc[tweet_id][1]\n",
    "            tweet_label = annotations.iloc[i][4]\n",
    "            if tweet_label == NEUTRAL: tweet_label = IRRELEVANT\n",
    "            if tweet_label == ANDERS: tweet_label = IRRELEVANT\n",
    "            if tweet_label == EENS: tweet_label = SUPPORTS\n",
    "            if tweet_label == ONEENS: tweet_label = REJECTS\n",
    "            fasttext_data[tweet_id] = { LABEL:LABELPREFIX+tweet_label,\n",
    "                                        USER:tweet_user,\n",
    "                                        TEXT:preprocess(tweets.loc[tweet_id][TEXT]) }\n",
    "    return({tweet_id: fasttext_data[tweet_id] for tweet_id in sorted(fasttext_data.keys())})\n",
    "\n",
    "\n",
    "def store_fasttext_data_fixed(fasttext_data):\n",
    "    outfile = open(FILEFASTTEXT, \"w\")\n",
    "    seen_texts = {}\n",
    "    fasttext_data_list = []\n",
    "    tweet_ids = []\n",
    "    for tweet_id in fasttext_data:\n",
    "        text = cleanup(fasttext_data[tweet_id][TEXT])\n",
    "        if not text in seen_texts:\n",
    "            print(fasttext_data[tweet_id][LABEL], text, file=outfile)\n",
    "            fasttext_data_list.append(\" \".join([fasttext_data[tweet_id][LABEL], text]))\n",
    "            tweet_ids.append(tweet_id)\n",
    "            seen_texts[text] = True\n",
    "    outfile.close()\n",
    "    return(fasttext_data_list, tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fasttext_data(topic):\n",
    "    if topic == TESTING:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202003-202012.csv\"\n",
    "    elif topic == FACEMASK:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202003-202103-1000.csv\"\n",
    "    elif topic == VACCINATION:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"-202001-202101-1000.csv\"\n",
    "    elif topic == DISTANCE:\n",
    "        tweet_file = DATADIR + topic + \"-tweets.csv\"\n",
    "    elif topic == LONELY:\n",
    "        tweet_file = ANNOTATION_DIR + topic + \"_2020.csv\"\n",
    "    else:\n",
    "        print(f\"make_fasttext_data: cannot happen! ({topic})\")\n",
    "    tweets = pd.read_csv(tweet_file, index_col=IDSTR)\n",
    "    annotations = pd.read_csv(tweet_file + \".human-labels.txt\", header=None, sep=\" \")        \n",
    "    main_annotator = annotations.iloc[0][0]\n",
    "    fasttext_data = select_annotations(tweets, annotations, main_annotator)\n",
    "    fasttext_data_list, tweet_ids = store_fasttext_data_fixed(fasttext_data)\n",
    "    return(fasttext_data_list, tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrix(fasttextData,predictionLabels):\n",
    "    goldLabels = pd.Series([fasttextData[i].split()[0] for i in range(0,len(fasttextData))])\n",
    "    predictedLabels = pd.Series([x[0] for row in predictionLabels for x in row[0]])\n",
    "    return(pd.crosstab(goldLabels,predictedLabels))\n",
    "\n",
    "def evaluate(predictionCounts,predictionLabels,fasttextData,printResults=True,printMatrix=False):\n",
    "    caseTotal = 0\n",
    "    pTotal = 0\n",
    "    rTotal = 0\n",
    "    for i in range(0,len(predictionCounts)):\n",
    "        caseTotal += predictionCounts[i][0]\n",
    "        pTotal += predictionCounts[i][0]*predictionCounts[i][1]\n",
    "        rTotal += predictionCounts[i][0]*predictionCounts[i][2]\n",
    "    precision = round(pTotal/caseTotal,3)\n",
    "    recall = round(rTotal/caseTotal,3)\n",
    "    try:\n",
    "        cf = makeConfusionMatrix(fasttextData,predictionLabels)\n",
    "        if printMatrix:\n",
    "            print(cf)\n",
    "        for label in ['__label__REJECTS','__label__SUPPORTS','__label__IRRELEVANT']:\n",
    "            if not label in cf: cf[label] = [0 for i in range(0,len(cf))]\n",
    "        numberOfPredictedSupportLabels = sum([cf.iloc[i]['__label__SUPPORTS'] for i in range(0,len(cf))])\n",
    "        numberOfGoldSupportLabels = sum(cf.loc['__label__SUPPORTS'])\n",
    "        numberOfPredictedRejectLabels = sum([cf.iloc[i]['__label__REJECTS'] for i in range(0,len(cf))])\n",
    "        numberOfGoldRejectLabels = sum(cf.loc['__label__REJECTS'])\n",
    "        if numberOfPredictedSupportLabels > 0 and numberOfGoldRejectLabels > 0 and numberOfGoldSupportLabels > 0:\n",
    "            fraction = (numberOfPredictedRejectLabels/numberOfPredictedSupportLabels)/(numberOfGoldRejectLabels/numberOfGoldSupportLabels)\n",
    "        else:\n",
    "            fraction = LARGEINT\n",
    "    except:\n",
    "        fraction = 0\n",
    "    if printResults: \n",
    "        print(\"cases: {0}; precision: {1:0.3f}; recall: {2:0.3f}; fraction: {3:0.3f}\".format(caseTotal,precision,recall,fraction))\n",
    "    return((precision,fraction))\n",
    "\n",
    "def runFasttext(fasttextData1, fasttextData2, dim=DIM, epoch=EPOCH, lr=LR, n=N, squealFlag=True, maxTrain=None, pretrainedVectors=None):\n",
    "    print(f\"data size 1={len(fasttextData1)}, data size 2={len(fasttextData2)}, dim={dim}, epoch={epoch}, lr={lr}, pretrainedVectors={pretrainedVectors}\")\n",
    "    predictionCountsTest = []\n",
    "    predictionLabelsTest = []\n",
    "    predictionCountsValidation = []\n",
    "    predictionLabelsValidation = []\n",
    "    for fold in range(0,n):\n",
    "        if squealFlag:\n",
    "            clear_output(wait=True)\n",
    "            print(\"starting fold\",fold)\n",
    "        testStart1 = round(fold*len(fasttextData1)/n)\n",
    "        testEnd1 = round((fold+1)*len(fasttextData1)/n)\n",
    "        testStart2 = round(fold*len(fasttextData2)/n)\n",
    "        testEnd2 = round((fold+1)*len(fasttextData2)/n)\n",
    "        if fold < n-1: nextFold = fold+1\n",
    "        else: nextFold = 0\n",
    "        validationStart1 = round(nextFold*len(fasttextData1)/n)\n",
    "        validationEnd1 = round((nextFold+1)*len(fasttextData1)/n)\n",
    "        validationStart2 = round(nextFold*len(fasttextData2)/n)\n",
    "        validationEnd2 = round((nextFold+1)*len(fasttextData2)/n)\n",
    "        trainFile = open(TRAIN,\"w\")\n",
    "        testFile = open(TEST,\"w\")\n",
    "        validationFile = open(VALIDATION,\"w\")\n",
    "        trainData = []\n",
    "        validationData = []\n",
    "        testData = []\n",
    "        for i in range(0,len(fasttextData1)):\n",
    "            data = fasttextData1[i]\n",
    "            if i >= testStart1 and i < testEnd1: \n",
    "                pass\n",
    "            elif i >= validationStart1 and i < validationEnd1: \n",
    "                pass\n",
    "            else: \n",
    "                print(data,file=trainFile)\n",
    "                trainData.append(data)\n",
    "        for i in range(0,len(fasttextData2)):\n",
    "            data = fasttextData2[i]\n",
    "            if i >= testStart2 and i < testEnd2: \n",
    "                print(data,file=testFile)\n",
    "                testData.append(data)\n",
    "            elif i >= validationStart2 and i < validationEnd2: \n",
    "                print(data,file=validationFile)\n",
    "                validationData.append(data)\n",
    "        testFile.close()\n",
    "        trainFile.close()\n",
    "        validationFile.close()\n",
    "        if not maxTrain == None and maxTrain < len(trainData):\n",
    "            trainData = ranSelect(trainData,maxTrain)\n",
    "            trainFile = open(TRAIN,\"w\")\n",
    "            for i in range(0,len(trainData)):\n",
    "                print(trainData[i],file=trainFile)\n",
    "            trainFile.close()\n",
    "        if pretrainedVectors == None:\n",
    "            model = fasttext.train_supervised(TRAIN, dim=dim, epoch=epoch, lr=lr)\n",
    "        else:\n",
    "            model = fasttext.train_supervised(TRAIN, dim=dim, epoch=epoch, lr=lr, pretrainedVectors=pretrainedVectors)\n",
    "        predictionCountsValidation.append([*model.test(VALIDATION)])\n",
    "        predictionLabelsValidation.append(model.predict(validationData, k=3))\n",
    "        predictionCountsTest.append([*model.test(TEST)])\n",
    "        predictionLabelsTest.append(model.predict(testData, k=3))\n",
    "        print(fold, end=\" \")\n",
    "        evaluate([[*model.test(TEST)]], [model.predict(testData)], fasttextData2[testStart2:testEnd2])\n",
    "        os.unlink(TRAIN)\n",
    "        os.unlink(TEST)\n",
    "        os.unlink(VALIDATION)\n",
    "    print(\"\", end=\" \")\n",
    "    precision, fraction = evaluate(predictionCountsTest, predictionLabelsTest, fasttextData2, printMatrix=True)\n",
    "    if squealFlag:\n",
    "        clear_output(wait=True)\n",
    "        print(\"finished\")\n",
    "    return(predictionCountsValidation, predictionLabelsValidation, predictionCountsTest, predictionLabelsTest, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_distance, tweet_ids_distance = make_fasttext_data(DISTANCE)\n",
    "fasttext_data_list_facemask, tweet_ids_facemask = make_fasttext_data(FACEMASK)\n",
    "fasttext_data_list_testing, tweet_ids_testing = make_fasttext_data(TESTING)\n",
    "fasttext_data_list_vaccination, tweet_ids_vaccination = make_fasttext_data(VACCINATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_lonely, tweet_ids_lonely = make_fasttext_data(LONELY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_ids_distance), len(tweet_ids_facemask), len(tweet_ids_testing), len(tweet_ids_vaccination), len(tweet_ids_lonely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_fasttext_data(data, file_name):\n",
    "    with open(file_name, \"wb\") as outfile:\n",
    "        pickle.dump(data, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "def read_fasttext_data(file_name):\n",
    "    with open(file_name, \"rb\") as infile:\n",
    "        data = pickle.load(infile)\n",
    "        infile.close()\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation within domains (TGTONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_labels_list(prediction_labels_10cv):\n",
    "    prediction_labels = []\n",
    "    for i in range(0, len(prediction_labels_10cv)):\n",
    "        prediction_labels.extend([row[0] for row in prediction_labels_10cv[i][0]])\n",
    "    return prediction_labels\n",
    "\n",
    "\n",
    "def compute_accuracy(prediction_labels, fasttext_data_list):\n",
    "    correct = 0\n",
    "    for i in range(0, len(fasttext_data_list)):\n",
    "        gold_label = fasttext_data_list[i].split(\" \")[0]\n",
    "        if prediction_labels[i] == gold_label:\n",
    "            correct += 1\n",
    "    return(round(correct / len(fasttext_data_list), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationLLT,predictionLabelsValidationLLT,predictionCountsTestLLT,predictionLabelsTestLLT, precision = \\\n",
    "    runFasttext(fasttext_data_list_lonely, fasttext_data_list_lonely, squealFlag=False, pretrainedVectors=\"twiqs-model-2020-exf.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestLLT, f\"predictionLabelsTestLLT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationDDT,predictionLabelsValidationDDT,predictionCountsTestDDT,predictionLabelsTestDDT = \\\n",
    "    runFasttext(fasttext_data_list_distance, fasttext_data_list_distance, squealFlag=False, pretrainedVectors=\"twiqs-model-2020-exf.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestDDT, f\"predictionLabelsTestDDT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationFFT,predictionLabelsValidationFFT,predictionCountsTestFFT,predictionLabelsTestFFT = \\\n",
    "    runFasttext(fasttext_data_list_facemask, fasttext_data_list_facemask, squealFlag=False, pretrainedVectors=\"twiqs-model-2020-exf.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestFFT, f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "for i in range(0, 10):\n",
    "    predictionCountsValidationTTT, predictionLabelsValidationTTT, predictionCountsTestTTT, predictionLabelsTestTTT, accuracy = \\\n",
    "        runFasttext(fasttext_data_list_testing, fasttext_data_list_testing, squealFlag=False, pretrainedVectors=\"twiqs-model-2020-exf.vec\")\n",
    "    accuracies.append(accuracy)\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestTTT, f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationVVT,predictionLabelsValidationVVT,predictionCountsTestVVT,predictionLabelsTestVVT = \\\n",
    "    runFasttext(fasttext_data_list_vaccination, fasttext_data_list_vaccination, squealFlag=False, pretrainedVectors=\"twiqs-model-2020-exf.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestVVT, f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\")), fasttext_data_list_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation of other domains with social distancing data (SRCONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distance = fasttext.train_supervised(\"fasttext-distance.csv\", dim=DIM, epoch=EPOCH, lr=LR, pretrainedVectors=\"twiqs-model-2020-exf.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(get_prediction_labels_list([model_distance.predict(fasttext_data_list_distance)]), fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(get_prediction_labels_list([model_distance.predict(fasttext_data_list_facemask)]), fasttext_data_list_facemask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(get_prediction_labels_list([model_distance.predict(fasttext_data_list_testing)]), fasttext_data_list_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(get_prediction_labels_list([model_distance.predict(fasttext_data_list_vaccination)]), fasttext_data_list_vaccination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting social distancing with facemask data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_labels(fasttext_data_list):\n",
    "    counts = {}\n",
    "    for i in range(0, len(fasttext_data_list)):\n",
    "        token_list = fasttext_data_list[i].split()\n",
    "        if token_list[0] in counts:\n",
    "            counts[token_list[0]] += 1\n",
    "        else:\n",
    "            counts[token_list[0]] = 1\n",
    "    return(counts)\n",
    "\n",
    "\n",
    "def swap_labels(fasttext_data_list_in):\n",
    "    fasttext_data_list_out = []\n",
    "    for i in range(0, len(fasttext_data_list_in)):\n",
    "        token_list = fasttext_data_list_in[i].split()\n",
    "        if token_list[0] == \"__label__REJECTS\":\n",
    "            token_list[0] = \"__label__SUPPORTS\"\n",
    "        elif token_list[0] == \"__label__SUPPORTS\":\n",
    "            token_list[0] = \"__label__REJECTS\"\n",
    "        fasttext_data_list_out.append(\" \".join(token_list))\n",
    "    return(fasttext_data_list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationFD,predictionLabelsValidationFD,predictionCountsTestFD,predictionLabelsTestFD = \\\n",
    "        runFasttext(fasttext_data_list_facemask, fasttext_data_list_distance, squealFlag=False, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationDFT, predictionLabelsValidationDFT, predictionCountsTestDFT, predictionLabelsTestDFT = \\\n",
    "    runFasttext(fasttext_data_list_distance, fasttext_data_list_facemask, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestDFT, f\"predictionLabelsTestDFT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationDTT, predictionLabelsValidationDTT, predictionCountsTestDTT, predictionLabelsTestDTT = \\\n",
    "    runFasttext(fasttext_data_list_distance, fasttext_data_list_testing, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestDTT, f\"predictionLabelsTestDTT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationDVT, predictionLabelsValidationDVT, predictionCountsTestDVT, predictionLabelsTestDVT = \\\n",
    "    runFasttext(fasttext_data_list_distance, fasttext_data_list_vaccination, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestDVT, f\"predictionLabelsTestDVT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestFD = read_fasttext_data(\"predictionLabelsTestFD.pickle\")\n",
    "predictionLabelsTestDD = read_fasttext_data(\"predictionLabelsTestDD.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    plot_data =[]\n",
    "    for key in data:\n",
    "        if re.search(\"^[0-9]+$\", str(key)):\n",
    "            support_count = 0\n",
    "            reject_count = 0\n",
    "            for label in data[key]:\n",
    "                if label == \"__label__SUPPORTS\":\n",
    "                    support_count += 1\n",
    "                if label == \"__label__REJECTS\":\n",
    "                    reject_count += 1\n",
    "            if support_count+reject_count > 0:\n",
    "                plot_data.append((int(key), support_count/(support_count+reject_count)))\n",
    "    plot_data = [x for x in sorted(plot_data, key=lambda x:x[0])]\n",
    "    return(plot_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_predicted(predictionLabelsTest, tweet_ids, time_factor=4):\n",
    "    predicted = []\n",
    "    for i in range(0, len(predictionLabelsTest)):\n",
    "        for j in range(0, len(predictionLabelsTest[i][0])):\n",
    "            predicted.append(predictionLabelsTest[i][0][j])\n",
    "    data_predicted = {}\n",
    "    for i in range(0, len(tweet_ids)):\n",
    "        summary = str(tweet_ids[i])[0:time_factor]\n",
    "        if summary not in data_predicted:\n",
    "            data_predicted[summary] = []\n",
    "        data_predicted[summary].append(predicted[i][0])\n",
    "    return(data_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_factor = 4\n",
    "data_predicted_fd = make_data_predicted(predictionLabelsTestFD, tweet_ids_distance, time_factor=time_factor)\n",
    "data_predicted_dd = make_data_predicted(predictionLabelsTestDD, tweet_ids_distance, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for tweet in fasttext_data_list_distance:\n",
    "    gold.append(tweet.split()[0])\n",
    "\n",
    "data_gold = {}\n",
    "for i in range(0, len(tweet_ids_distance)):\n",
    "    summary = str(tweet_ids_distance[i])[0:time_factor]\n",
    "    if summary not in data_gold:\n",
    "        data_gold[summary] = []\n",
    "    data_gold[summary].append(gold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_fd = convert(data_predicted_fd)\n",
    "plot_data_predicted_dd = convert(data_predicted_dd)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_fd = [x[0] for x in plot_data_predicted_fd]\n",
    "y_fd = [x[1] for x in plot_data_predicted_fd]\n",
    "x_dd = [x[0] for x in plot_data_predicted_dd]\n",
    "y_dd = [x[1] for x in plot_data_predicted_dd]\n",
    "plt.plot(x_fd, y_fd, label=\"predicted (out-of-domain)\")\n",
    "plt.plot(x_dd, y_dd, label=\"predicted (in-domain)\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting social distancing with facemask data\")\n",
    "plt.savefig(\"facemask-to-distancing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflate predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freqs(label_list):\n",
    "    freqs = {}\n",
    "    for label in label_list:\n",
    "        if label in freqs:\n",
    "            freqs[label] += 1\n",
    "        else:\n",
    "            freqs[label] = 1\n",
    "    for label in freqs:\n",
    "        freqs[label] /= len(label_list)\n",
    "    return(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate(predictionLabelsTestIn, source_label, target_label, factor):\n",
    "    predictionLabelsTestOut = list(predictionLabelsTestIn)\n",
    "    counter = 0\n",
    "    for fold in predictionLabelsTestOut:\n",
    "        for i in range(0, len(fold[0])):\n",
    "            if fold[0][i][0] == target_label:\n",
    "                for j in range(0, len(fold[0][i])):\n",
    "                    if fold[0][i][j] == source_label:\n",
    "                        break\n",
    "                if fold[0][i][j] != source_label:\n",
    "                    sys.exit(\"cannot happen\")\n",
    "                if fold[1][i][j]*factor > fold[1][i][0]:\n",
    "                    fold[0][i][0], fold[0][i][j] = fold[0][i][j], fold[0][i][0]\n",
    "    return(predictionLabelsTestOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inflate_plus(predictionLabelsTestIn, source_label, target_label, factor):\n",
    "    predictionLabelsTestOut = list(predictionLabelsTestIn)\n",
    "    counter = 0\n",
    "    for fold in predictionLabelsTestOut:\n",
    "        for i in range(0, len(fold[0])):\n",
    "            if fold[0][i][0] == target_label:\n",
    "                for j in range(0, len(fold[0][i])):\n",
    "                    if fold[0][i][j] == source_label:\n",
    "                        break\n",
    "                if fold[0][i][j] != source_label:\n",
    "                    sys.exit(\"cannot happen\")\n",
    "                if fold[1][i][j]+factor > fold[1][i][0]:\n",
    "                    fold[0][i][0], fold[0][i][j] = fold[0][i][j], fold[0][i][0]\n",
    "    return(predictionLabelsTestOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = get_freqs(gold)\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 0.9967\n",
    "\n",
    "predictionLabelsTestFDinflated = list(predictionLabelsTestFD)\n",
    "freqs = get_freqs([y[0] for x in inflate_plus(predictionLabelsTestFDinflated, '__label__SUPPORTS', '__label__REJECTS', FACTOR) for y in x[0]])\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted_fd_inflated = make_data_predicted(predictionLabelsTestFDinflated, tweet_ids_distance)\n",
    "plot_data_predicted_fd_inflated = convert(data_predicted_fd_inflated)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_fd = [x[0] for x in plot_data_predicted_fd]\n",
    "y_fd = [x[1] for x in plot_data_predicted_fd]\n",
    "x_dd = [x[0] for x in plot_data_predicted_dd]\n",
    "y_dd = [x[1] for x in plot_data_predicted_dd]\n",
    "x_fd_inflated = [x[0] for x in plot_data_predicted_fd_inflated]\n",
    "y_fd_inflated = [x[1] for x in plot_data_predicted_fd_inflated]\n",
    "plt.plot(x_fd, y_fd, label=\"predicted (out-of-domain)\")\n",
    "plt.plot(x_dd, y_dd, label=\"predicted (in-domain)\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\")\n",
    "plt.plot(x_fd_inflated, y_fd_inflated, label=\"predicted (out-of-domain, inflated\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting social distancing with facemask data\")\n",
    "plt.savefig(\"facemask-to-distancing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical analysis of predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_fd_dict = { x[0]:x[1] for x in plot_data_predicted_fd }\n",
    "plot_data_predicted_dd_dict = { x[0]:x[1] for x in plot_data_predicted_dd }\n",
    "plot_data_gold_dict = { x[0]:x[1] for x in plot_data_gold }\n",
    "plot_data_predicted_fd_inflated_dict = { x[0]:x[1] for x in plot_data_predicted_fd_inflated }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_key_values(dict1, dict2):\n",
    "    values1 = []\n",
    "    values2 = []\n",
    "    for key in sorted(dict1.keys()):\n",
    "        if key in dict2:\n",
    "            values1.append(dict1[key])\n",
    "            values2.append(dict2[key])\n",
    "    return(values1, values2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_dd, values_gold_dd = get_common_key_values(plot_data_predicted_dd_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs in-domain:\", round(scipy.stats.linregress(np.array(values_gold_dd), np.array(values_dd)).rvalue, 2))\n",
    "values_fd, values_gold_fd = get_common_key_values(plot_data_predicted_fd_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain:\", round(scipy.stats.linregress(np.array(values_gold_fd), np.array(values_fd)).rvalue, 2))\n",
    "values_fd_inflated, values_gold_fd_inflated = get_common_key_values(plot_data_predicted_fd_inflated_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (inflated):\", round(scipy.stats.linregress(np.array(values_gold_fd), np.array(values_fd_inflated)).rvalue, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absolute_difference(list1, list2):\n",
    "    absolute_difference = 0\n",
    "    for i in range(0, len(list1)):\n",
    "        absolute_difference += abs(list1[i]-list2[i])\n",
    "    return(absolute_difference/len(list1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"absolute difference gold vs in-domain:\", round(absolute_difference(values_gold_dd, values_dd), 2))\n",
    "print(\"absolute difference gold vs out-of-domain:\", round(absolute_difference(values_gold_fd, values_fd),2))\n",
    "print(\"absolute difference gold vs out-of-domain (inflated):\", round(absolute_difference(values_gold_fd_inflated, values_fd_inflated),2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting facemasks with social distancing data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDF,predictionLabelsValidationDF,predictionCountsTestDF,predictionLabelsTestDF = \\\n",
    "        runFasttext(fasttext_data_list_distance, fasttext_data_list_facemask, squealFlag=False, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDFT,predictionLabelsValidationDFT,predictionCountsTestDFT,predictionLabelsTestDFT = \\\n",
    "        runFasttext(fasttext_data_list_distance, fasttext_data_list_facemask, squealFlag=False, dim=300, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    store_fasttext_data(predictionLabelsTestDFT, \"predictionLabelsTestDFT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDF = read_fasttext_data(\"predictionLabelsTestDF.pickle\")\n",
    "predictionLabelsTestFF = read_fasttext_data(\"predictionLabelsTestFF.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_factor = 5\n",
    "data_predicted_df = make_data_predicted(predictionLabelsTestDF, tweet_ids_facemask, time_factor=time_factor)\n",
    "data_predicted_ff = make_data_predicted(predictionLabelsTestFF, tweet_ids_facemask, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for tweet in fasttext_data_list_facemask:\n",
    "    gold.append(tweet.split()[0])\n",
    "\n",
    "data_gold = {}\n",
    "for i in range(0, len(tweet_ids_facemask)):\n",
    "    summary = str(tweet_ids_facemask[i])[0:time_factor]\n",
    "    if summary not in data_gold:\n",
    "        data_gold[summary] = []\n",
    "    data_gold[summary].append(gold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_df = convert(data_predicted_df)\n",
    "plot_data_predicted_ff = convert(data_predicted_ff)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_df = [x[0] for x in plot_data_predicted_df]\n",
    "y_df = [x[1] for x in plot_data_predicted_df]\n",
    "x_ff = [x[0] for x in plot_data_predicted_ff]\n",
    "y_ff = [x[1] for x in plot_data_predicted_ff]\n",
    "plt.plot(x_df, y_df, label=\"predicted (out-of-domain)\")\n",
    "plt.plot(x_ff, y_ff, label=\"predicted (in-domain)\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting facemask with social distancing data\")\n",
    "plt.savefig(\"distancing-to-facemask.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflate predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = get_freqs(gold)\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 0.9893\n",
    "\n",
    "predictionLabelsTestDFinflated = list(predictionLabelsTestDF)\n",
    "freqs = get_freqs([y[0] for x in inflate_plus(predictionLabelsTestDFinflated, '__label__REJECTS', '__label__SUPPORTS', FACTOR) for y in x[0]])\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted_df_inflated = make_data_predicted(inflate(predictionLabelsTestDFinflated, '__label__REJECTS', '__label__SUPPORTS', FACTOR), tweet_ids_facemask, time_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_df = convert(data_predicted_df)\n",
    "plot_data_predicted_ff = convert(data_predicted_ff)\n",
    "plot_data_predicted_df_inflated = convert(data_predicted_df_inflated)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_df = [x[0] for x in plot_data_predicted_df]\n",
    "y_df = [x[1] for x in plot_data_predicted_df]\n",
    "x_ff = [x[0] for x in plot_data_predicted_ff]\n",
    "y_ff = [x[1] for x in plot_data_predicted_ff]\n",
    "x_df_inflated = [x[0] for x in plot_data_predicted_df_inflated]\n",
    "y_df_inflated = [x[1] for x in plot_data_predicted_df_inflated]\n",
    "plt.plot(x_df, y_df, label=\"predicted (out-of-domain)\")\n",
    "plt.plot(x_ff, y_ff, label=\"predicted (in-domain)\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\")\n",
    "plt.plot(x_df_inflated, y_df_inflated, label=\"predicted (out-of-domain, inflated)\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting facemask with social distancing data\")\n",
    "plt.savefig(\"distancing-to-facemask.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical analysis of predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_df_dict = { x[0]:x[1] for x in plot_data_predicted_df }\n",
    "plot_data_predicted_ff_dict = { x[0]:x[1] for x in plot_data_predicted_ff }\n",
    "plot_data_predicted_df_inflated_dict = { x[0]:x[1] for x in plot_data_predicted_df_inflated }\n",
    "plot_data_gold_dict = { x[0]:x[1] for x in plot_data_gold }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_ff, values_gold_ff = get_common_key_values(plot_data_predicted_ff_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs in-domain:\", round(scipy.stats.linregress(np.array(values_gold_ff), np.array(values_ff)).rvalue, 2))\n",
    "values_df, values_gold_df = get_common_key_values(plot_data_predicted_df_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain:\", round(scipy.stats.linregress(np.array(values_gold_df), np.array(values_df)).rvalue, 2))\n",
    "values_df_inflated, values_gold_df_inflated = get_common_key_values(plot_data_predicted_df_inflated_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (inflated):\", round(scipy.stats.linregress(np.array(values_gold_df_inflated), np.array(values_df_inflated)).rvalue, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"absolute difference gold vs in-domain:\", round(absolute_difference(values_gold_ff, values_ff), 2))\n",
    "print(\"absolute difference gold vs out-of-domain:\", round(absolute_difference(values_gold_df, values_df),2)) \n",
    "print(\"absolute difference gold vs out-of-domain (inflated):\", round(absolute_difference(values_gold_df_inflated, values_df_inflated),2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting testing data with distance data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDT, predictionLabelsValidationDT, predictionCountsTestDT, predictionLabelsTestDT = \\\n",
    "        runFasttext(fasttext_data_list_distance, fasttext_data_list_testing, squealFlag=False, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDT, predictionLabelsValidationDT, predictionCountsTestDT, predictionLabelsTestDT = \\\n",
    "        runFasttext(fasttext_data_list_distance, \n",
    "                    fasttext_data_list_testing, \n",
    "                    squealFlag=False, \n",
    "                    dim=300, \n",
    "                    pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    store_fasttext_data(predictionLabelsTestDT, \"predictionLabelsTestDT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDT = read_fasttext_data(\"predictionLabelsTestDT.pickle\")\n",
    "predictionLabelsTestTT = read_fasttext_data(\"predictionLabelsTestTT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_factor = 3\n",
    "data_predicted_dt = make_data_predicted(predictionLabelsTestDT, tweet_ids_testing, time_factor=time_factor)\n",
    "data_predicted_tt = make_data_predicted(predictionLabelsTestTT, tweet_ids_testing, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for tweet in fasttext_data_list_testing:\n",
    "    gold.append(tweet.split()[0])\n",
    "\n",
    "data_gold = {}\n",
    "for i in range(0, len(tweet_ids_testing)):\n",
    "    summary = str(tweet_ids_testing[i])[0:time_factor]\n",
    "    if summary not in data_gold:\n",
    "        data_gold[summary] = []\n",
    "    data_gold[summary].append(gold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt = convert(data_predicted_dt)\n",
    "plot_data_predicted_tt = convert(data_predicted_tt)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_df = [x[0] for x in plot_data_predicted_dt]\n",
    "y_df = [x[1] for x in plot_data_predicted_dt]\n",
    "x_ff = [x[0] for x in plot_data_predicted_tt]\n",
    "y_ff = [x[1] for x in plot_data_predicted_tt]\n",
    "plt.plot(x_df, y_df, label=\"predicted (out-of-domain)\")\n",
    "plt.plot(x_ff, y_ff, label=\"predicted (in-domain)\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting testing with social distancing data\")\n",
    "plt.savefig(\"distancing-to-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use extra data in training (ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runFasttextPlus(fasttextData1,\n",
    "                    fasttextData2,\n",
    "                    dim=DIM,\n",
    "                    epoch=EPOCH,\n",
    "                    lr=LR,\n",
    "                    n=N,\n",
    "                    squealFlag=True,\n",
    "                    maxTrain=None,\n",
    "                    pretrainedVectors=None,\n",
    "                    extraFasttextData=None):\n",
    "    print(f\"data size 1={len(fasttextData1)}, data size 2={len(fasttextData2)}, dim={dim}, epoch={epoch}, lr={lr}, pretrainedVectors={pretrainedVectors}\")\n",
    "    predictionCountsTest = []\n",
    "    predictionLabelsTest = []\n",
    "    predictionCountsValidation = []\n",
    "    predictionLabelsValidation = []\n",
    "    for fold in range(0,n):\n",
    "        if squealFlag:\n",
    "            clear_output(wait=True)\n",
    "            print(\"starting fold\",fold)\n",
    "        testStart1 = round(fold*len(fasttextData1)/n)\n",
    "        testEnd1 = round((fold+1)*len(fasttextData1)/n)\n",
    "        testStart2 = round(fold*len(fasttextData2)/n)\n",
    "        testEnd2 = round((fold+1)*len(fasttextData2)/n)\n",
    "        if fold < n-1: nextFold = fold+1\n",
    "        else: nextFold = 0\n",
    "        validationStart1 = round(nextFold*len(fasttextData1)/n)\n",
    "        validationEnd1 = round((nextFold+1)*len(fasttextData1)/n)\n",
    "        validationStart2 = round(nextFold*len(fasttextData2)/n)\n",
    "        validationEnd2 = round((nextFold+1)*len(fasttextData2)/n)\n",
    "        trainFile = open(TRAIN,\"w\")\n",
    "        testFile = open(TEST,\"w\")\n",
    "        validationFile = open(VALIDATION,\"w\")\n",
    "        trainData = []\n",
    "        validationData = []\n",
    "        testData = []\n",
    "        for i in range(0,len(fasttextData1)):\n",
    "            data = fasttextData1[i]\n",
    "            if i >= testStart1 and i < testEnd1: \n",
    "                pass\n",
    "            elif i >= validationStart1 and i < validationEnd1: \n",
    "                pass\n",
    "            else: \n",
    "                print(data,file=trainFile)\n",
    "                trainData.append(data)\n",
    "        for i in range(0,len(fasttextData2)):\n",
    "            data = fasttextData2[i]\n",
    "            if i >= testStart2 and i < testEnd2: \n",
    "                print(data,file=testFile)\n",
    "                testData.append(data)\n",
    "            elif i >= validationStart2 and i < validationEnd2: \n",
    "                print(data,file=validationFile)\n",
    "                validationData.append(data)\n",
    "        if extraFasttextData != None:\n",
    "            for i in range(0,len(extraFasttextData)):\n",
    "                data = extraFasttextData[i]\n",
    "                print(data,file=trainFile)\n",
    "                trainData.append(data)\n",
    "        testFile.close()\n",
    "        trainFile.close()\n",
    "        validationFile.close()\n",
    "        if not maxTrain == None and maxTrain < len(trainData):\n",
    "            trainData = ranSelect(trainData,maxTrain)\n",
    "            trainFile = open(TRAIN,\"w\")\n",
    "            for i in range(0,len(trainData)):\n",
    "                print(trainData[i],file=trainFile)\n",
    "            trainFile.close()\n",
    "        if pretrainedVectors == None:\n",
    "            model = fasttext.train_supervised(TRAIN, dim=dim, epoch=epoch, lr=lr)\n",
    "        else:\n",
    "            model = fasttext.train_supervised(TRAIN, dim=dim, epoch=epoch, lr=lr, pretrainedVectors=pretrainedVectors)\n",
    "        predictionCountsValidation.append([*model.test(VALIDATION)])\n",
    "        predictionLabelsValidation.append(model.predict(validationData, k=3))\n",
    "        predictionCountsTest.append([*model.test(TEST)])\n",
    "        predictionLabelsTest.append(model.predict(testData, k=3))\n",
    "        print(fold, end=\" \")\n",
    "        evaluate([[*model.test(TEST)]], [model.predict(testData)], fasttextData2[testStart2:testEnd2])\n",
    "        os.unlink(TRAIN)\n",
    "        os.unlink(TEST)\n",
    "        os.unlink(VALIDATION)\n",
    "    print(\"\", end=\" \")\n",
    "    evaluate(predictionCountsTest, predictionLabelsTest, fasttextData2, printMatrix=True)\n",
    "    if squealFlag:\n",
    "        clear_output(wait=True)\n",
    "        print(\"finished\")\n",
    "    return(predictionCountsValidation,predictionLabelsValidation,predictionCountsTest,predictionLabelsTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationETT, predictionLabelsValidationETT, predictionCountsTestETT, predictionLabelsTestETT = \\\n",
    "    runFasttextPlus(fasttext_data_list_testing, \n",
    "                    fasttext_data_list_testing, \n",
    "                    squealFlag=False, \n",
    "                    pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                    extraFasttextData=fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestETT, f\"predictionLabelsTestETT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationEFT, predictionLabelsValidationEFT, predictionCountsTestEFT, predictionLabelsTestEFT = \\\n",
    "    runFasttextPlus(fasttext_data_list_facemask, \n",
    "                    fasttext_data_list_facemask, \n",
    "                    squealFlag=False, \n",
    "                    pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                    extraFasttextData=fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestEFT, f\"predictionLabelsTestEFT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationEVT, predictionLabelsValidationEVT, predictionCountsTestEVT, predictionLabelsTestEVT = \\\n",
    "    runFasttextPlus(fasttext_data_list_vaccination, \n",
    "                    fasttext_data_list_vaccination, \n",
    "                    squealFlag=False, \n",
    "                    pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                    extraFasttextData=fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestEVT, f\"predictionLabelsTestEVT-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relevance experiments\n",
    "\n",
    "For the Relevance Only (5. RLVONLY) experiment, we need relevance preprocessing. This is a easier (binary) task which is handled here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_fasttext_data(fasttext_data_list_in):\n",
    "    fasttext_data_list_out = []\n",
    "    for text in fasttext_data_list_in:\n",
    "        text = re.sub(r'__label__SUPPORTS ', r'__label__RELEVANT ', text)\n",
    "        text = re.sub(r'__label__REJECTS ', r'__label__RELEVANT ', text)\n",
    "        fasttext_data_list_out.append(text)\n",
    "    return fasttext_data_list_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_facemask_binary = make_binary_fasttext_data(fasttext_data_list_facemask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationFFB, predictionLabelsValidationFFB, predictionCountsTestFFB, predictionLabelsTestFFB = \\\n",
    "    runFasttext(fasttext_data_list_facemask_binary, fasttext_data_list_facemask_binary, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestFFB, f\"predictionLabelsTestFFB-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_testing_binary = make_binary_fasttext_data(fasttext_data_list_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationTTB, predictionLabelsValidationTTB, predictionCountsTestTTB, predictionLabelsTestTTB = \\\n",
    "    runFasttext(fasttext_data_list_testing_binary, fasttext_data_list_testing_binary, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestTTB, f\"predictionLabelsTestTTB-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_vaccination_binary = make_binary_fasttext_data(fasttext_data_list_vaccination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationVVB, predictionLabelsValidationVVB, predictionCountsTestVVB, predictionLabelsTestVVB = \\\n",
    "    runFasttext(fasttext_data_list_vaccination_binary, fasttext_data_list_vaccination_binary, squealFlag=False, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_fasttext_data(predictionLabelsTestVVB, f\"predictionLabelsTestVVB-{DIM}-{EPOCH}-{LR}.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use relevant data only (RLVONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_irrelevant_annotated_data(fasttext_data_list):\n",
    "    return [annotated_text for annotated_text in fasttext_data_list if not re.search(r'^__label__IRRELEVANT ', annotated_text)]\n",
    "\n",
    "\n",
    "def remove_irrelevant_predicted_data(predictionLabelsTest, fasttext_data_list):\n",
    "    prediction_labels = get_prediction_labels_list(predictionLabelsTest)\n",
    "    assert len(prediction_labels) == len(fasttext_data_list), \"remove_irrelevant_data: lists have different lengths\"\n",
    "    fasttext_data_list_selected = []\n",
    "    for i in range(0, len(prediction_labels)):\n",
    "        if prediction_labels[i] != \"__label__IRRELEVANT\":\n",
    "            fasttext_data_list_selected.append(fasttext_data_list[i])\n",
    "    return fasttext_data_list_selected\n",
    "\n",
    "\n",
    "def restore_irrelevant_predicted_data(prediction_labels_in, prediction_labels_relevant):\n",
    "    prediction_labels = list(prediction_labels_in)\n",
    "    prediction_labels_relevant_index = 0\n",
    "    for i in range(0, len(prediction_labels)):\n",
    "        if prediction_labels[i] != \"__label__IRRELEVANT\":\n",
    "            prediction_labels[i] = prediction_labels_relevant[prediction_labels_relevant_index]\n",
    "            prediction_labels_relevant_index += 1\n",
    "    assert prediction_labels_relevant_index == len(prediction_labels_relevant), \\\n",
    "           f\"restore_irrelevant_predicted_data: length mismatch: {prediction_labels_relevant_index} {len(prediction_labels_relevant)}\"\n",
    "    return prediction_labels\n",
    "\n",
    "\n",
    "def save_fasttext_data(fasttext_data_list, file_name):\n",
    "    out_file = open(file_name, \"w\")\n",
    "    for line in fasttext_data_list:\n",
    "        print(line, file=out_file)\n",
    "    out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_relevant_model(fasttext_data_list):\n",
    "    fasttext_data_list_selected = remove_irrelevant_annotated_data(fasttext_data_list)\n",
    "    print(f\"building relevant model from: data_size={len(fasttext_data_list_selected)}, dim={DIM}, epoch={EPOCH}, lr={LR}\")\n",
    "    tmp_file_name = str(uuid.uuid1())\n",
    "    save_fasttext_data(fasttext_data_list_distance_selected, tmp_file_name)\n",
    "    model_relevant = fasttext.train_supervised(tmp_file_name, dim=DIM, epoch=EPOCH, lr=LR, pretrainedVectors=\"twiqs-model-2020.vec\")\n",
    "    os.remove(tmp_file_name)\n",
    "    return model_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relevant = make_relevant_model(fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_relevant_experiment(model_relevant, result_file_name, fasttext_data_list):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = remove_irrelevant_data(predictionLabelsTest, fasttext_data_list)\n",
    "    prediction_labels_relevant = model_relevant.predict(fasttext_data_list_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list([prediction_labels_relevant]))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), fasttext_data_list)\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")\n",
    "    \n",
    "    \n",
    "def run_relevant_experiment_binary(model_relevant, result_file_name, fasttext_data_list):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = remove_irrelevant_data(predictionLabelsTest, fasttext_data_list)\n",
    "    prediction_labels_relevant = model_relevant.predict(fasttext_data_list_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list([prediction_labels_relevant]))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), make_binary_fasttext_data(fasttext_data_list))\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")\n",
    "    \n",
    "    \n",
    "def run_relevant_experiment_all(result_file_name, fasttext_data_list, fasttext_data_list_extra):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = remove_irrelevant_data(predictionLabelsTest, fasttext_data_list)\n",
    "    fasttext_data_list_extra_selected = remove_irrelevant_annotated_data(fasttext_data_list_extra)\n",
    "    predictionCountsValidationRun, predictionLabelsValidationRun, predictionCountsTestRun, predictionLabelsTestRun = \\\n",
    "        runFasttextPlus(fasttext_data_list_selected, \n",
    "                        fasttext_data_list_selected, \n",
    "                        squealFlag=False, \n",
    "                        pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                        extraFasttextData=fasttext_data_list_extra_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list(predictionLabelsTestRun))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    store_fasttext_data(prediction_labels_relevant_complete, f\"{result_file_name}.RLV-{DIM}-{EPOCH}-{LR}.pickle\")\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), fasttext_data_list)\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_facemask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestFFB-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_facemask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_all(f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_facemask, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestTTB-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_all(f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_testing, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_vaccination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment(model_relevant, f\"predictionLabelsTestVVB-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_vaccination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_all(f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_vaccination, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Daume Feature Space Separation\n",
    "\n",
    "Schemes:\n",
    "1. [x] without setting initial word vectors for new vector spaces\n",
    "2. [ ] while also changing classes\n",
    "3. [x] while setting initial word vectors for new vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_features(fasttext_data_list_in, suffix):\n",
    "    fasttext_data_list_out = []\n",
    "    for text in fasttext_data_list_in:\n",
    "        tokens_in = text.split()\n",
    "        tokens_out = []\n",
    "        for token in tokens_in[1:]:\n",
    "            tokens_out.append(token + suffix)\n",
    "        fasttext_data_list_out.append(\" \".join(tokens_in + tokens_out))\n",
    "    return fasttext_data_list_out\n",
    "\n",
    "\n",
    "def run_relevant_experiment_expand_features(result_file_name, fasttext_data_list, fasttext_data_list_extra):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = expand_features(remove_irrelevant_data(predictionLabelsTest, fasttext_data_list), \"@1\")\n",
    "    fasttext_data_list_extra_selected = expand_features(remove_irrelevant_annotated_data(fasttext_data_list_extra), \"@2\")\n",
    "    predictionCountsValidationRun, predictionLabelsValidationRun, predictionCountsTestRun, predictionLabelsTestRun = \\\n",
    "        runFasttextPlus(fasttext_data_list_selected, \n",
    "                        fasttext_data_list_selected, \n",
    "                        squealFlag=False, \n",
    "                        pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                        extraFasttextData=fasttext_data_list_extra_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list(predictionLabelsTestRun))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), fasttext_data_list)\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_features(fasttext_data_list_in, suffix):\n",
    "    fasttext_data_list_out = []\n",
    "    for text in fasttext_data_list_in:\n",
    "        tokens_in = text.split()\n",
    "        tokens_out = []\n",
    "        for token in tokens_in[1:]:\n",
    "            tokens_out.append(token + suffix)\n",
    "        fasttext_data_list_out.append(\" \".join(tokens_in + tokens_out))\n",
    "    return fasttext_data_list_out\n",
    "\n",
    "\n",
    "def run_relevant_experiment_expand_features(result_file_name, fasttext_data_list, fasttext_data_list_extra):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = expand_features(remove_irrelevant_data(predictionLabelsTest, fasttext_data_list), \"@1\")\n",
    "    fasttext_data_list_extra_selected = expand_features(remove_irrelevant_annotated_data(fasttext_data_list_extra), \"@2\")\n",
    "    predictionCountsValidationRun, predictionLabelsValidationRun, predictionCountsTestRun, predictionLabelsTestRun = \\\n",
    "        runFasttextPlus(fasttext_data_list_selected, \n",
    "                        fasttext_data_list_selected, \n",
    "                        squealFlag=False, \n",
    "                        pretrainedVectors=\"twiqs-model-2020.vec\",\n",
    "                        extraFasttextData=fasttext_data_list_extra_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list(predictionLabelsTestRun))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    store_fasttext_data(prediction_labels_relevant_complete, f\"{result_file_name}.EXF-{DIM}-{EPOCH}-{LR}.pickle\")\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), fasttext_data_list)\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")\n",
    "    \n",
    "    \n",
    "def run_relevant_experiment_expand_features_new(result_file_name, fasttext_data_list, fasttext_data_list_extra, data_name):\n",
    "    predictionLabelsTest = read_fasttext_data(result_file_name)\n",
    "    fasttext_data_list_selected = expand_features(remove_irrelevant_data(predictionLabelsTest, fasttext_data_list), f\"@{data_name}\")\n",
    "    fasttext_data_list_extra_selected = expand_features(remove_irrelevant_annotated_data(fasttext_data_list_extra), \"@distance\")\n",
    "    predictionCountsValidationRun, predictionLabelsValidationRun, predictionCountsTestRun, predictionLabelsTestRun = \\\n",
    "        runFasttextPlus(fasttext_data_list_selected, \n",
    "                        fasttext_data_list_selected, \n",
    "                        squealFlag=False, \n",
    "                        pretrainedVectors=\"twiqs-model-2020-exf.vec\",\n",
    "                        extraFasttextData=fasttext_data_list_extra_selected)\n",
    "    prediction_labels_relevant_complete = restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), get_prediction_labels_list(predictionLabelsTestRun))\n",
    "    accuracy = compute_accuracy(prediction_labels_relevant_complete, fasttext_data_list)\n",
    "    store_fasttext_data(prediction_labels_relevant_complete, f\"{result_file_name}.EXG-{DIM}-{EPOCH}-{LR}.pickle\")\n",
    "    accuracy_phase_1 = compute_accuracy(get_prediction_labels_list(predictionLabelsTest), fasttext_data_list)\n",
    "    gold_labels_selected = [annotated_text.split(\" \")[0] for annotated_text in fasttext_data_list_selected]\n",
    "    accuracy_phase_1_plus_gold = compute_accuracy(restore_irrelevant_predicted_data(get_prediction_labels_list(predictionLabelsTest), gold_labels_selected), fasttext_data_list) \n",
    "    print(f\"accuracy (phase 1): {accuracy_phase_1}, accuracy (phase 1 plus gold): {accuracy_phase_1_plus_gold}, accuracy (phase 2): {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features(f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_facemask, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features(f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_testing, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features(f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_vaccination, fasttext_data_list_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make domain word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(fasttext_data_list):\n",
    "    tokens = {}\n",
    "    for text in fasttext_data_list:\n",
    "        for token in text.split()[1:]:\n",
    "            tokens[token] = True\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_distance = get_tokens(fasttext_data_list_distance)\n",
    "tokens_facemask = get_tokens(fasttext_data_list_facemask)\n",
    "tokens_testing = get_tokens(fasttext_data_list_testing)\n",
    "tokens_vaccination = get_tokens(fasttext_data_list_vaccination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_all = dict(tokens_distance)\n",
    "tokens_all.update(tokens_facemask)\n",
    "tokens_all.update(tokens_testing)\n",
    "tokens_all.update(tokens_vaccination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to fix first line: number_of_tokens vector_length\n",
    "file_name_in = \"twiqs-model-2020.vec\"\n",
    "file_name_out = \"twiqs-model-2020-exf.vec\"\n",
    "infile = open(file_name_in, \"r\")\n",
    "outfile = open(file_name_out, \"w\")\n",
    "for line in infile:\n",
    "    token = line.strip().split()[0]\n",
    "    rest = \" \".join(line.strip().split()[1:])\n",
    "    if token in tokens_all:\n",
    "        print(f\"{token} {rest}\", file=outfile)\n",
    "        if token in tokens_distance:\n",
    "            print(f\"{token}@distance {rest}\", file=outfile)\n",
    "        if token in tokens_facemask:\n",
    "            print(f\"{token}@facemask {rest}\", file=outfile)\n",
    "        if token in tokens_testing:\n",
    "            print(f\"{token}@testing {rest}\", file=outfile)\n",
    "        if token in tokens_vaccination:\n",
    "            print(f\"{token}@vaccination {rest}\", file=outfile)\n",
    "outfile.close()\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features_new(f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_facemask, fasttext_data_list_distance, \"facemask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features_new(f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_testing, fasttext_data_list_distance, \"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_relevant_experiment_expand_features_new(f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\", fasttext_data_list_vaccination, fasttext_data_list_distance, \"vaccination\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-task learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Numeric analysis\n",
    "\n",
    "This also needs the first code block of *1. Evaluation within domains (TGTONLY)* (`get_prediction_labels_list`) and and the first code block of *2. Evaluation of other domains with social distancing data (SRCONLY)* (`model_predict`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 100\n",
    "\n",
    "\n",
    "def get_labels_from_fasttext_data(fasttext_data_list):\n",
    "    return([text.split()[0] for text in fasttext_data_list])\n",
    "\n",
    "\n",
    "def percentage(window):\n",
    "    total_count = 0\n",
    "    support_count = 0\n",
    "    for label in window:\n",
    "        total_count += 1\n",
    "        if label == \"__label__SUPPORTS\":\n",
    "            support_count += 1\n",
    "        else:\n",
    "            assert label == '__label__REJECTS', f\"percentage: unexpected label value: {label}\"\n",
    "    return(100 * support_count / total_count)\n",
    "\n",
    "\n",
    "def make_graph_data_make_relevant(label_list):\n",
    "    label_list_relevant = [ label for label in label_list if label != '__label__IRRELEVANT' ]\n",
    "    window = []\n",
    "    graph_data_relevant = len(label_list_relevant) * [None]\n",
    "    for i in range(0, len(label_list_relevant)):\n",
    "        window.append(label_list_relevant[i])\n",
    "        if len(window) >= WINDOW_SIZE:\n",
    "            index = round((i - WINDOW_SIZE + 1) * (len(label_list_relevant)-1)/(len(label_list_relevant)-WINDOW_SIZE))\n",
    "            assert index >= 0, f\"make_graph_data_make_relevant: index ({index}) cannot be negative!\"\n",
    "            graph_data_relevant[index] = percentage(window)        \n",
    "        while len(window) >= WINDOW_SIZE:\n",
    "            window.pop(0)\n",
    "    return graph_data_relevant\n",
    "            \n",
    "    \n",
    "def make_graph_data_fill_data(label_list, graph_data_relevant):\n",
    "    graph_data = len(label_list) * [None]\n",
    "    index_relevant = 0\n",
    "    for i in range(0, len(label_list)):\n",
    "        if label_list[i] != '__label__IRRELEVANT':\n",
    "            graph_data[i] = graph_data_relevant[index_relevant]\n",
    "            index_relevant += 1\n",
    "    assert index_relevant == len(graph_data_relevant), f\"make_graph_data: unexpected length: {index_relevant} {len(graph_data_relevant)}\"\n",
    "    return graph_data\n",
    "   \n",
    "    \n",
    "def make_graph_data_interpolate_center(graph_data):\n",
    "    for i in range(0, len(graph_data)):\n",
    "        if graph_data[i] != None and i+1 < len(graph_data) and graph_data[i+1] == None:\n",
    "            for j in range(i+1, len(graph_data)):\n",
    "                if graph_data[j] != None:\n",
    "                    break\n",
    "            if j < len(graph_data) and graph_data[j] != None:\n",
    "                for k in range(i+1, j):\n",
    "                    graph_data[k] = graph_data[i] + (k-i) * (graph_data[j] - graph_data[i])/(j - i)\n",
    "\n",
    "    return graph_data\n",
    "    \n",
    "    \n",
    "def make_graph_data_interpolate_start(graph_data):\n",
    "    for i in range(0, len(graph_data)):\n",
    "        if graph_data[i] != None:\n",
    "            break\n",
    "    if i > 0 and i+1 < len(graph_data):\n",
    "        for j in range(i-1, -1, -1):\n",
    "            graph_data[j] = graph_data[i] + (i - j) * (graph_data[i] - graph_data[i+1]) \n",
    "    return graph_data\n",
    "\n",
    "\n",
    "def make_graph_data_interpolate_end(graph_data):\n",
    "    for i in range(len(graph_data)-1, -1, -1):\n",
    "        if graph_data[i] != None:\n",
    "            break\n",
    "    if i > 0 and i < len(graph_data)-1:\n",
    "        for j in range(i+1, len(graph_data)):\n",
    "            graph_data[j] = graph_data[i] + (j - i) * (graph_data[i] - graph_data[i-1]) \n",
    "    return graph_data\n",
    "\n",
    "\n",
    "def make_graph_data_interpolate(graph_data):    \n",
    "    graph_data = make_graph_data_interpolate_center(graph_data)\n",
    "    graph_data = make_graph_data_interpolate_start(graph_data)\n",
    "    graph_data = make_graph_data_interpolate_end(graph_data)\n",
    "    return graph_data\n",
    "\n",
    "\n",
    "def make_graph_data(label_list):\n",
    "    graph_data_relevant = make_graph_data_make_relevant(label_list)\n",
    "    graph_data = make_graph_data_fill_data(label_list, graph_data_relevant)\n",
    "    graph_data = make_graph_data_interpolate(graph_data)\n",
    "    return graph_data\n",
    "\n",
    "\n",
    "def get_r_value(list1, list2):\n",
    "    return round(scipy.stats.linregress(np.array(list1), np.array(list2)).rvalue, 3)\n",
    "\n",
    "\n",
    "def absolute_difference(list1, list2):\n",
    "    absolute_difference = 0\n",
    "    for i in range(0, len(list1)):\n",
    "        absolute_difference += abs(list1[i]-list2[i])\n",
    "    return(round(0.01*absolute_difference/len(list1), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Face masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_facemask_gold = make_graph_data(get_labels_from_fasttext_data(fasttext_data_list_facemask))\n",
    "graph_data_facemask_FFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestFFT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_facemask_DFT = make_graph_data(get_prediction_labels_list([model_distance.predict(fasttext_data_list_facemask)]))\n",
    "graph_data_facemask_EFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestEFT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_facemask_RLV = make_graph_data(read_fasttext_data(\"predictionLabelsTestFFT-300-200-0.2.pickle.RLV-300-200-0.2.pickle\"))\n",
    "graph_data_facemask_EXG = make_graph_data(read_fasttext_data(\"predictionLabelsTestFFT-300-200-0.2.pickle.EXG-300-200-0.2.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_r_value(graph_data_facemask_gold, graph_data_facemask_FFT), \n",
    " get_r_value(graph_data_facemask_gold, graph_data_facemask_DFT),\n",
    " get_r_value(graph_data_facemask_gold, graph_data_facemask_EFT),\n",
    " get_r_value(graph_data_facemask_gold, graph_data_facemask_RLV),\n",
    " get_r_value(graph_data_facemask_gold, graph_data_facemask_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(absolute_difference(graph_data_facemask_gold, graph_data_facemask_FFT), \n",
    " absolute_difference(graph_data_facemask_gold, graph_data_facemask_DFT),\n",
    " absolute_difference(graph_data_facemask_gold, graph_data_facemask_EFT),\n",
    " absolute_difference(graph_data_facemask_gold, graph_data_facemask_RLV),\n",
    " absolute_difference(graph_data_facemask_gold, graph_data_facemask_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_FILE = \"csv/time_data.mondkapje-202003-202103-1000.csv\"\n",
    "\n",
    "time_data_facemask_all = pd.read_csv(TIME_FILE, index_col=\"id_str\").to_dict(orient=\"series\")['created_at']\n",
    "time_data_facemask = []\n",
    "for tweet_id in tweet_ids_facemask:\n",
    "    try:\n",
    "        time_data_facemask.append(datetime.datetime.strptime(time_data_facemask_all[tweet_id], \"%a %b %d %H:%M:%S %z %Y\"))\n",
    "    except:\n",
    "        print(f\"key not found {tweet_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m\"))\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_gold, fmt=\"-\", label=\"gold\")\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_FFT, fmt=\"-\", label=\"TGTONLY\")\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_DFT, fmt=\"-\", label=\"SRCONLY\")\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_EFT, fmt=\"-\", label=\"ALL\")\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_RLV, fmt=\"-\", label=\"RLVONLY\")\n",
    "ax.plot_date(time_data_facemask, graph_data_facemask_EXG, fmt=\"-\", label=\"FEATAUG\")\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"face masks\")\n",
    "plt.xlabel(\"months in the years 2020 and 2021\")\n",
    "plt.ylabel(\"percentage support\")\n",
    "plt.legend()\n",
    "plt.savefig(\"clin-facemask.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_testing_gold = make_graph_data(get_labels_from_fasttext_data(fasttext_data_list_testing))\n",
    "graph_data_testing_FFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestTTT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_testing_DFT = make_graph_data(get_prediction_labels_list([model_distance.predict(fasttext_data_list_testing)]))\n",
    "graph_data_testing_EFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestETT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_testing_RLV = make_graph_data(read_fasttext_data(\"predictionLabelsTestTTT-300-200-0.2.pickle.RLV-300-200-0.2.pickle\"))\n",
    "graph_data_testing_EXG = make_graph_data(read_fasttext_data(\"predictionLabelsTestTTT-300-200-0.2.pickle.EXG-300-200-0.2.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_r_value(graph_data_testing_gold, graph_data_testing_FFT), \n",
    " get_r_value(graph_data_testing_gold, graph_data_testing_DFT),\n",
    " get_r_value(graph_data_testing_gold, graph_data_testing_EFT),\n",
    " get_r_value(graph_data_testing_gold, graph_data_testing_RLV),\n",
    " get_r_value(graph_data_testing_gold, graph_data_testing_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(absolute_difference(graph_data_testing_gold, graph_data_testing_FFT), \n",
    " absolute_difference(graph_data_testing_gold, graph_data_testing_DFT),\n",
    " absolute_difference(graph_data_testing_gold, graph_data_testing_EFT),\n",
    " absolute_difference(graph_data_testing_gold, graph_data_testing_RLV),\n",
    " absolute_difference(graph_data_testing_gold, graph_data_testing_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_FILE = \"csv/time_data.testing-202003-202012.csv\"\n",
    "\n",
    "time_data_testing_all = pd.read_csv(TIME_FILE, index_col=\"id_str\").to_dict(orient=\"series\")['created_at']\n",
    "time_data_testing = []\n",
    "for tweet_id in tweet_ids_testing:\n",
    "    try:\n",
    "        time_data_testing.append(datetime.datetime.strptime(time_data_testing_all[tweet_id], \"%a %b %d %H:%M:%S %z %Y\"))\n",
    "    except:\n",
    "        print(f\"key not found {tweet_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m\"))\n",
    "ax.plot_date(time_data_testing, graph_data_testing_gold, fmt=\"-\", label=\"gold\")\n",
    "ax.plot_date(time_data_testing, graph_data_testing_FFT, fmt=\"-\", label=\"TGTONLY\")\n",
    "ax.plot_date(time_data_testing, graph_data_testing_DFT, fmt=\"-\", label=\"SRCONLY\")\n",
    "ax.plot_date(time_data_testing, graph_data_testing_EFT, fmt=\"-\", label=\"ALL\")\n",
    "ax.plot_date(time_data_testing, graph_data_testing_RLV, fmt=\"-\", label=\"RLVONLY\")\n",
    "ax.plot_date(time_data_testing, graph_data_testing_EXG, fmt=\"-\", label=\"FEATAUG\")\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"testing\")\n",
    "plt.xlabel(\"months in the years 2020 and 2021\")\n",
    "plt.ylabel(\"percentage support\")\n",
    "plt.legend()\n",
    "plt.savefig(\"clin-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Vaccination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_vaccination_gold = make_graph_data(get_labels_from_fasttext_data(fasttext_data_list_vaccination))\n",
    "graph_data_vaccination_FFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestVVT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_vaccination_DFT = make_graph_data(get_prediction_labels_list([model_distance.predict(fasttext_data_list_vaccination)]))\n",
    "graph_data_vaccination_EFT = make_graph_data(get_prediction_labels_list(read_fasttext_data(f\"predictionLabelsTestEVT-{DIM}-{EPOCH}-{LR}.pickle\")))\n",
    "graph_data_vaccination_RLV = make_graph_data(read_fasttext_data(\"predictionLabelsTestVVT-300-200-0.2.pickle.RLV-300-200-0.2.pickle\"))\n",
    "graph_data_vaccination_EXG = make_graph_data(read_fasttext_data(\"predictionLabelsTestVVT-300-200-0.2.pickle.EXG-300-200-0.2.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_r_value(graph_data_vaccination_gold, graph_data_vaccination_FFT), \n",
    " get_r_value(graph_data_vaccination_gold, graph_data_vaccination_DFT),\n",
    " get_r_value(graph_data_vaccination_gold, graph_data_vaccination_EFT),\n",
    " get_r_value(graph_data_vaccination_gold, graph_data_vaccination_RLV),\n",
    " get_r_value(graph_data_vaccination_gold, graph_data_vaccination_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(absolute_difference(graph_data_vaccination_gold, graph_data_vaccination_FFT), \n",
    " absolute_difference(graph_data_vaccination_gold, graph_data_vaccination_DFT),\n",
    " absolute_difference(graph_data_vaccination_gold, graph_data_vaccination_EFT),\n",
    " absolute_difference(graph_data_vaccination_gold, graph_data_vaccination_RLV),\n",
    " absolute_difference(graph_data_vaccination_gold, graph_data_vaccination_EXG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_FILE = \"csv/time_data.vaccin-202001-202101-1000.csv\"\n",
    "\n",
    "time_data_vaccination_all = pd.read_csv(TIME_FILE, index_col=\"id_str\").to_dict(orient=\"series\")['created_at']\n",
    "time_data_vaccination = []\n",
    "for tweet_id in tweet_ids_vaccination:\n",
    "    try:\n",
    "        time_data_vaccination.append(datetime.datetime.strptime(time_data_vaccination_all[tweet_id], \"%a %b %d %H:%M:%S %z %Y\"))\n",
    "    except:\n",
    "        print(f\"key not found {tweet_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m\"))\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_gold, fmt=\"-\", label=\"gold\")\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_FFT, fmt=\"-\", label=\"TGTONLY\")\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_DFT, fmt=\"-\", label=\"SRCONLY\")\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_EFT, fmt=\"-\", label=\"ALL\")\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_RLV, fmt=\"-\", label=\"RLVONLY\")\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_EXG, fmt=\"-\", label=\"FEATAUG\")\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"vaccination\")\n",
    "plt.xlabel(\"months in the years 2020 and 2021\")\n",
    "plt.ylabel(\"percentage support\")\n",
    "plt.legend()\n",
    "plt.savefig(\"clin-vaccination.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 vaccination with RIVM data\n",
    "\n",
    "Needs *8.3 vaccination*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rivm_month_string = \"2020-06-01 2020-07-12 2020-11-15 2021-01-03 2021-02-14 2021-03-28\"\n",
    "rivm_month_list = []\n",
    "for month in rivm_month_string.split():\n",
    "    rivm_month_list.append(datetime.datetime.strptime(month, \"%Y-%m-%d\"))\n",
    "rivm_support = [74.2, 65.0, 58.2, 80.3, 85.5, 85.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m\"))\n",
    "ax.plot_date(time_data_vaccination, graph_data_vaccination_gold, fmt=\"-\", label=\"annotated tweets\")\n",
    "ax.plot_date(rivm_month_list, rivm_support, fmt=\"-\", label=\"RIVM surveys\")\n",
    "ax.plot_date\n",
    "plt.ylim(0, 100)\n",
    "plt.title(\"vaccination\")\n",
    "plt.xlabel(\"months in the years 2020 and 2021\")\n",
    "plt.ylabel(\"percentage support\")\n",
    "plt.legend()\n",
    "plt.savefig(\"clin-vaccination-rivm.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDT = read_fasttext_data(\"predictionLabelsTestDT.pickle\")\n",
    "predictionLabelsTestTT = read_fasttext_data(\"predictionLabelsTestTT.pickle\")\n",
    "predictionLabelsTestET = read_fasttext_data(\"predictionLabelsTestET.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_factor = 3\n",
    "data_predicted_dt = make_data_predicted(predictionLabelsTestDT, tweet_ids_testing, time_factor=time_factor)\n",
    "data_predicted_tt = make_data_predicted(predictionLabelsTestTT, tweet_ids_testing, time_factor=time_factor)\n",
    "data_predicted_et = make_data_predicted(predictionLabelsTestET, tweet_ids_testing, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = []\n",
    "for tweet in fasttext_data_list_testing:\n",
    "    gold.append(tweet.split()[0])\n",
    "\n",
    "data_gold = {}\n",
    "for i in range(0, len(tweet_ids_testing)):\n",
    "    summary = str(tweet_ids_testing[i])[0:time_factor]\n",
    "    if summary not in data_gold:\n",
    "        data_gold[summary] = []\n",
    "    data_gold[summary].append(gold[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt = convert(data_predicted_dt)\n",
    "plot_data_predicted_tt = convert(data_predicted_tt)\n",
    "plot_data_predicted_et = convert(data_predicted_et)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_dt = [x[0] for x in plot_data_predicted_dt]\n",
    "y_dt = [x[1] for x in plot_data_predicted_dt]\n",
    "x_tt = [x[0] for x in plot_data_predicted_tt]\n",
    "y_tt = [x[1] for x in plot_data_predicted_tt]\n",
    "x_et = [x[0] for x in plot_data_predicted_et]\n",
    "y_et = [x[1] for x in plot_data_predicted_et]\n",
    "plt.plot(x_dt, y_dt, label=\"predicted (out-of-domain)\", c=\"C0\")\n",
    "plt.plot(x_tt, y_tt, label=\"predicted (in-domain)\", c=\"C1\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\", c=\"C2\")\n",
    "#plt.plot(x_et, y_et, label=\"predicted (added data)\", c=\"C3\")\n",
    "plt.legend()\n",
    "plt.title(\"predicting testing with social distancing data\")\n",
    "plt.savefig(\"distancing-to-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inflate predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = get_freqs(gold)\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDT = read_fasttext_data(\"predictionLabelsTestDT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACTOR = 0.7957\n",
    "\n",
    "predictionLabelsTestDTinflated = list(predictionLabelsTestDT)\n",
    "freqs = get_freqs([y[0] for x in inflate_plus(predictionLabelsTestDTinflated, '__label__REJECTS', '__label__SUPPORTS', FACTOR) for y in x[0]])\n",
    "print(freqs)\n",
    "print(freqs['__label__REJECTS']/freqs['__label__SUPPORTS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted_dt_inflated = make_data_predicted(inflate(predictionLabelsTestDTinflated, \n",
    "                                                         '__label__REJECTS', \n",
    "                                                         '__label__SUPPORTS', \n",
    "                                                         FACTOR), \n",
    "                                                         tweet_ids_testing, \n",
    "                                                         time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt = convert(data_predicted_dt)\n",
    "plot_data_predicted_tt = convert(data_predicted_tt)\n",
    "plot_data_predicted_et = convert(data_predicted_et)\n",
    "plot_data_predicted_dt_inflated = convert(data_predicted_dt_inflated)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_dt = [x[0] for x in plot_data_predicted_dt]\n",
    "y_dt = [x[1] for x in plot_data_predicted_dt]\n",
    "x_tt = [x[0] for x in plot_data_predicted_tt]\n",
    "y_tt = [x[1] for x in plot_data_predicted_tt]\n",
    "x_et = [x[0] for x in plot_data_predicted_et]\n",
    "y_et = [x[1] for x in plot_data_predicted_et]\n",
    "x_dt_inflated = [x[0] for x in plot_data_predicted_dt_inflated]\n",
    "y_dt_inflated = [x[1] for x in plot_data_predicted_dt_inflated]\n",
    "\n",
    "#plt.plot(x_dt, y_dt, label=\"predicted (out-of-domain)\", c=\"C0\")\n",
    "#plt.plot(x_tt, y_tt, label=\"predicted (in-domain)\", c=\"C1\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\", c=\"C2\")\n",
    "plt.plot(x_et, y_et, label=\"predicted (added data)\", c=\"C3\")\n",
    "plt.plot(x_dt_inflated, y_dt_inflated, label=\"predicted (out-of-domain, inflated)\", c=\"C4\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"predicting COVID-19 testing with social distancing data\")\n",
    "plt.savefig(\"distancing-to-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing domain-dependent words (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionCountsValidationDTD, predictionLabelsValidationDTD, predictionCountsTestDTD, predictionLabelsTestDTD = \\\n",
    "        runFasttext(fasttext_data_list_distance, fasttext_data_list_testing, squealFlag=False, dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tweet_list):\n",
    "    counts = {}\n",
    "    for tweet in tweet_list:\n",
    "        for token in tweet.split()[1:]:\n",
    "            if not token in counts:\n",
    "                counts[token] = 0\n",
    "            counts[token] += 1\n",
    "    return(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_train = count_tokens(fasttext_data_list_distance)\n",
    "counts_test = count_tokens(fasttext_data_list_testing)\n",
    "counts_train_total = sum(counts_train.values())\n",
    "counts_test_total = sum(counts_test.values())\n",
    "print(counts_train_total, counts_test_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_infrequent_tokens(tweet_list_in, counts_train, counts_test, counts_train_total, counts_test_total, minimum_frequency=0):\n",
    "    tweet_list_out = []\n",
    "    token_count = 0\n",
    "    for tweet in tweet_list_in:\n",
    "        accepted_tokens = []\n",
    "        tweet_tokens = tweet.split()\n",
    "        for token in tweet_tokens[1:]:\n",
    "            if token in counts_train and token in counts_test and \\\n",
    "               counts_train[token] / counts_train_total > minimum_frequency and \\\n",
    "               counts_test[token] / counts_test_total > minimum_frequency:\n",
    "                accepted_tokens.append(token)\n",
    "        token_count += 1 + len(accepted_tokens)\n",
    "        if len(accepted_tokens) == 0:\n",
    "            accepted_tokens = [ \"DUMMY\" ]\n",
    "        tweet_list_out.append(\" \".join(tweet_tokens[:1]+accepted_tokens))\n",
    "    print(token_count)\n",
    "    return(tweet_list_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINFREQ = 0.0002\n",
    "fasttext_data_list_distance_filtered = remove_infrequent_tokens(fasttext_data_list_distance, counts_train, counts_test, counts_train_total, counts_test_total, minimum_frequency=MINFREQ)\n",
    "fasttext_data_list_testing_filtered = remove_infrequent_tokens(fasttext_data_list_testing, counts_train, counts_test, counts_train_total, counts_test_total, minimum_frequency=MINFREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDTD, predictionLabelsValidationDTD, predictionCountsTestDTD, predictionLabelsTestDTD = \\\n",
    "        runFasttext(fasttext_data_list_distance_filtered, fasttext_data_list_testing, squealFlag=False, dim=300, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    store_fasttext_data(predictionLabelsTestDTD, \"predictionLabelsTestDTD0.0002A.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDTD = read_fasttext_data(\"predictionLabelsTestDTD0.0002A.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  method | train | test | accuracy | fraction | comment |\n",
    "| ------- | ----- | ---- | -------- | -------- | ------- |\n",
    "| none        | 196888 | 52387 | 0.385 | 0.478 | |\n",
    "| M = 0       | 171520 | 49477 | 0.379 | 0.650 | |\n",
    "| M = 0.00001 | 170682 | 48139 | 0.375 | 0.650 | |\n",
    "| M = 0.0001  | 145771 | 39891 | 0.379 | 0.688 | |\n",
    "| M = 0.0002  | 137571 | 36390 | 0.390 | 0.833 | |\n",
    "| M = 0.0003  | 127440 | 34457 | 0.359 | 0.776 | |\n",
    "| M = 0.0004  | 119718 | 32913 | 0.346 | 0.764 | |\n",
    "| M = 0.001   |  99965 | 28123 | 0.327 | 0.186 | |\n",
    "| M = 0.01    |  46634 | 12909 | 0.289 | 0.006 | |\n",
    "| M = 0.1     |   5731 |  1181 | 0.288 | 0.000 | |\n",
    "| M = 0       | 171520 | 52387 | 0.381 | 0.635 | |\n",
    "| M = 0.00001 | 170682 | 52387 | 0.374 | 0.637 | |\n",
    "| M = 0.0001  | 145771 | 52387 | 0.378 | 0.665 | |\n",
    "| M = 0.0002  | 137571 | 52387 | 0.395 | 0.860 | |\n",
    "| M = 0.0003  | 127440 | 52387 | 0.358 | 0.764 | |\n",
    "| M = 0.0004  | 119718 | 52387 | 0.345 | 0.769 | |\n",
    "| M = 0.001   |  99965 | 52387 | 0.324 | 0.193 | |\n",
    "| M = 0.01    |  46634 | 52387 | 0.290 | 0.007 | |\n",
    "| M = 0.1     |   5731 | 52387 | 0.288 | 0.000 | |\n",
    "| M = 0       | 171520 | 49477 | 0.393 | 0.736 | +model |\n",
    "| M = 0.0002  | 137571 | 36390 | 0.385 | 0.841 | +model |\n",
    "| M = 0.0002  | 137571 | 52387 | 0.380 | 0.734 | +model |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted_dtd = make_data_predicted(predictionLabelsTestDTD, tweet_ids_testing, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt = convert(data_predicted_dt)\n",
    "plot_data_predicted_tt = convert(data_predicted_tt)\n",
    "plot_data_predicted_et = convert(data_predicted_et)\n",
    "plot_data_predicted_dt_inflated = convert(data_predicted_dt_inflated)\n",
    "plot_data_predicted_dtd = convert(data_predicted_dtd)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_dt = [x[0] for x in plot_data_predicted_dt]\n",
    "y_dt = [x[1] for x in plot_data_predicted_dt]\n",
    "x_tt = [x[0] for x in plot_data_predicted_tt]\n",
    "y_tt = [x[1] for x in plot_data_predicted_tt]\n",
    "x_et = [x[0] for x in plot_data_predicted_et]\n",
    "y_et = [x[1] for x in plot_data_predicted_et]\n",
    "x_dt_inflated = [x[0] for x in plot_data_predicted_dt_inflated]\n",
    "y_dt_inflated = [x[1] for x in plot_data_predicted_dt_inflated]\n",
    "x_dtd = [x[0] for x in plot_data_predicted_dtd]\n",
    "y_dtd = [x[1] for x in plot_data_predicted_dtd]\n",
    "\n",
    "plt.plot(x_dt, y_dt, label=\"predicted (out-of-domain)\", c=\"C0\")\n",
    "plt.plot(x_tt, y_tt, label=\"predicted (in-domain)\", c=\"C1\")\n",
    "plt.plot(x_gold, y_gold, label=\"gold\", c=\"C2\")\n",
    "#plt.plot(x_et, y_et, label=\"predicted (added data)\", c=\"C3\")\n",
    "#plt.plot(x_dt_inflated, y_dt_inflated, label=\"predicted (out-of-domain, inflated)\", c=\"C4\")\n",
    "plt.plot(x_dtd, y_dtd, label=\"predicted (out-of-domain, no domain words)\", c=\"C6\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"predicting COVID-19 testing with social distancing data\")\n",
    "plt.savefig(\"distancing-to-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing on only relevant data (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_distance_relevant = [ line for line in fasttext_data_list_distance if not re.search(\"^__label__IRRELEVANT\", line) ]\n",
    "len(fasttext_data_list_distance_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationT, predictionLabelsValidationT, predictionCountsTestT, predictionLabelsTestT = \\\n",
    "        runFasttext(fasttext_data_list_testing, fasttext_data_list_testing, squealFlag=False, dim=300, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    store_fasttext_data(predictionLabelsTestT, \"predictionLabelsTestT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestT = read_fasttext_data(\"predictionLabelsTestT.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_data_list_testing_relevant = []\n",
    "i = 0\n",
    "for fold in predictionLabelsTestT:\n",
    "    for prediction_order_list in fold[0]:\n",
    "        if prediction_order_list[0] != \"__label__IRRELEVANT\":\n",
    "            tweet = re.sub(\"^__label__IRRELEVANT\", \"__label__SUPPORTS\", fasttext_data_list_testing[i])\n",
    "            fasttext_data_list_testing_relevant.append(tweet)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fasttext_data_list_distance_relevant), len(fasttext_data_list_testing_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    predictionCountsValidationDTR, predictionLabelsValidationDTR, predictionCountsTestDTR, predictionLabelsTestDTR = \\\n",
    "        runFasttext(fasttext_data_list_distance_relevant, fasttext_data_list_testing_relevant, squealFlag=False, dim=300, pretrainedVectors=\"twiqs-model-2020.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    store_fasttext_data(predictionLabelsTestDTR, \"predictionLabelsTestDTR.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDTR = read_fasttext_data(\"predictionLabelsTestDTR.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionLabelsTestDTRrestored = []\n",
    "fold_count = 0\n",
    "for fold in predictionLabelsTestT:\n",
    "    predictionLabelsTestDTRrestored.append(([[]]))\n",
    "    tweet_count = 0\n",
    "    for prediction_order_list in fold[0]:\n",
    "        if prediction_order_list[0] == \"__label__IRRELEVANT\":\n",
    "            predictionLabelsTestDTRrestored[-1][0].append(prediction_order_list)\n",
    "        else:\n",
    "            if tweet_count >= len(predictionLabelsTestDTR[fold_count][0]):\n",
    "                break\n",
    "            predictionLabelsTestDTRrestored[-1][0].append(predictionLabelsTestDTR[fold_count][0][tweet_count])\n",
    "            tweet_count += 1\n",
    "    for i in range(len(predictionLabelsTestDTRrestored[-1][0]), len(fold[0])):\n",
    "        predictionLabelsTestDTRrestored[-1][0].append(fold[0][i])\n",
    "    fold_count += 1\n",
    "sum([len(fold[0]) for fold in predictionLabelsTestDTRrestored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_predicted_dtr = make_data_predicted(predictionLabelsTestDTRrestored, tweet_ids_testing, time_factor=time_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt = convert(data_predicted_dt)\n",
    "plot_data_predicted_tt = convert(data_predicted_tt)\n",
    "plot_data_predicted_et = convert(data_predicted_et)\n",
    "plot_data_predicted_dt_inflated = convert(data_predicted_dt_inflated)\n",
    "plot_data_predicted_dtd = convert(data_predicted_dtd)\n",
    "plot_data_predicted_dtr = convert(data_predicted_dtr)\n",
    "plot_data_gold = convert(data_gold)\n",
    "\n",
    "x_gold = [x[0] for x in plot_data_gold]\n",
    "y_gold = [x[1] for x in plot_data_gold]\n",
    "x_dt = [x[0] for x in plot_data_predicted_dt]\n",
    "y_dt = [x[1] for x in plot_data_predicted_dt]\n",
    "x_tt = [x[0] for x in plot_data_predicted_tt]\n",
    "y_tt = [x[1] for x in plot_data_predicted_tt]\n",
    "x_et = [x[0] for x in plot_data_predicted_et]\n",
    "y_et = [x[1] for x in plot_data_predicted_et]\n",
    "x_dt_inflated = [x[0] for x in plot_data_predicted_dt_inflated]\n",
    "y_dt_inflated = [x[1] for x in plot_data_predicted_dt_inflated]\n",
    "x_dtd = [x[0] for x in plot_data_predicted_dtd]\n",
    "y_dtd = [x[1] for x in plot_data_predicted_dtd]\n",
    "x_dtr = [x[0] for x in plot_data_predicted_dtr]\n",
    "y_dtr = [x[1] for x in plot_data_predicted_dtr]\n",
    "\n",
    "plt.plot(x_gold, y_gold, label=\"gold\", c=\"C2\")\n",
    "#plt.plot(x_dt, y_dt, label=\"predicted (out-of-domain)\", c=\"C0\")\n",
    "#plt.plot(x_tt, y_tt, label=\"predicted (in-domain)\", c=\"C1\")\n",
    "#lt.plot(x_et, y_et, label=\"predicted (added data)\", c=\"C3\")\n",
    "#plt.plot(x_dt_inflated, y_dt_inflated, label=\"predicted (out-of-domain, inflated)\", c=\"C4\")\n",
    "plt.plot(x_dtd, y_dtd, label=\"predicted (no domain words)\", c=\"C6\")\n",
    "plt.plot(x_dtr, y_dtr, label=\"predicted (relevant data)\", c=\"C7\")\n",
    "\n",
    "plt.ylim(0.3, 1.0)\n",
    "plt.legend()\n",
    "plt.title(\"predicting COVID-19 testing with social distancing data\")\n",
    "plt.savefig(\"distancing-to-testing.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical analysis of predictions (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data_predicted_dt_dict = { x[0]:x[1] for x in plot_data_predicted_dt }\n",
    "plot_data_predicted_tt_dict = { x[0]:x[1] for x in plot_data_predicted_tt }\n",
    "plot_data_predicted_dt_inflated_dict = { x[0]:x[1] for x in plot_data_predicted_dt_inflated }\n",
    "plot_data_predicted_dtd_dict = { x[0]:x[1] for x in plot_data_predicted_dtd }\n",
    "plot_data_predicted_dtr_dict = { x[0]:x[1] for x in plot_data_predicted_dtr }\n",
    "plot_data_predicted_et_dict = { x[0]:x[1] for x in plot_data_predicted_et }\n",
    "plot_data_gold_dict = { x[0]:x[1] for x in plot_data_gold }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values_tt, values_gold_tt = get_common_key_values(plot_data_predicted_tt_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs in-domain:\", round(scipy.stats.linregress(np.array(values_gold_tt), np.array(values_tt)).rvalue, 2))\n",
    "values_dt, values_gold_dt = get_common_key_values(plot_data_predicted_dt_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain:\", round(scipy.stats.linregress(np.array(values_gold_dt), np.array(values_dt)).rvalue, 2))\n",
    "values_dt_inflated, values_gold_dt_inflated = get_common_key_values(plot_data_predicted_dt_inflated_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (inflated):\", round(scipy.stats.linregress(np.array(values_gold_dt_inflated), np.array(values_dt_inflated)).rvalue, 2))\n",
    "values_dtd, values_gold_dtd = get_common_key_values(plot_data_predicted_dtd_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (no domain data):\", round(scipy.stats.linregress(np.array(values_gold_dtd), np.array(values_dtd)).rvalue, 2))\n",
    "values_dtr, values_gold_dtr = get_common_key_values(plot_data_predicted_dtr_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (relevant data):\", round(scipy.stats.linregress(np.array(values_gold_dtr), np.array(values_dtr)).rvalue, 2))\n",
    "values_et, values_gold_et = get_common_key_values(plot_data_predicted_et_dict, plot_data_gold_dict)\n",
    "print(\"Pearson r correlation gold vs out-of-domain (extra data):\", round(scipy.stats.linregress(np.array(values_gold_et), np.array(values_et)).rvalue, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"absolute difference gold vs in-domain:\", round(absolute_difference(values_gold_tt, values_tt), 2))\n",
    "print(\"absolute difference gold vs out-of-domain:\", round(absolute_difference(values_gold_dt, values_dt),2)) \n",
    "print(\"absolute difference gold vs out-of-domain (inflated):\", round(absolute_difference(values_gold_dt_inflated, values_dt_inflated),2))\n",
    "print(\"absolute difference gold vs out-of-domain (no domain data):\", round(absolute_difference(values_gold_dtd, values_dtd),2))\n",
    "print(\"absolute difference gold vs out-of-domain (relevant data):\", round(absolute_difference(values_gold_dtr, values_dtr),2))\n",
    "print(\"absolute difference gold vs out-of-domain (extra data):\", round(absolute_difference(values_gold_et, values_et),2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
