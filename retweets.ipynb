{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze users by inspecting retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/home/erikt/projects/puregome/data/retweets/\"\n",
    "DATAFILE = \"get-retweet-users.py.out.gz\"\n",
    "RIGHT = \"geertwilderspvv thierrybaudet fvdemocratie\".split(\" \")\n",
    "DUTCHPRESS = \"telegraaf NOS RTLnieuws NUnl ADnl nrc volkskrant trouw parool AT5 NPORadio1 Nieuwsuur op1npo EenVandaag\".split(\" \")\n",
    "FLEMISHPRESS = \"HLN_BE demorgen destandaard vrtnws Nieuwsblad_be deafspraaktv VTMNIEUWS\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the retweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATADIR+DATAFILE,sep=\" \",header=None,compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a lists of retweeted users per retweeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby([0,1]).groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how often each retweeter retweets users of the three main groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3450000/3458767\n"
     ]
    }
   ],
   "source": [
    "profiles = {}\n",
    "\n",
    "def add(user,group,count):\n",
    "    if not user in profiles: profiles[user] = {RIGHT[0]:0,DUTCHPRESS[0]:0,FLEMISHPRESS[0]:0}\n",
    "    if not group[0] in profiles[user]: profiles[user][group[0]] = count\n",
    "    else: profiles[user][group[0]] += count\n",
    "\n",
    "counter = 0\n",
    "for key in groups.keys():\n",
    "    counter += 1\n",
    "    if counter % 10000 == 0: squeal(str(counter)+\"/\"+str(len(groups)))\n",
    "    retweetedUser = key[1]\n",
    "    retweeter = key[0]\n",
    "    if retweetedUser in RIGHT: add(retweeter,RIGHT,len(groups[key]))\n",
    "    elif retweetedUser in DUTCHPRESS: add(retweeter,DUTCHPRESS,len(groups[key]))\n",
    "    elif retweetedUser in FLEMISHPRESS: add(retweeter,FLEMISHPRESS,len(groups[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get users with retweets of only one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN = 5\n",
    "usersPerGroup = {RIGHT[0]:[],DUTCHPRESS[0]:[],FLEMISHPRESS[0]:[]}\n",
    "for user in profiles:\n",
    "    if profiles[user][RIGHT[0]] > MIN and profiles[user][DUTCHPRESS[0]] == 0 and profiles[user][FLEMISHPRESS[0]] == 0:\n",
    "        usersPerGroup[RIGHT[0]].append(user)\n",
    "    if profiles[user][RIGHT[0]] == 0 and profiles[user][DUTCHPRESS[0]] > MIN and profiles[user][FLEMISHPRESS[0]] == 0:\n",
    "        usersPerGroup[DUTCHPRESS[0]].append(user)\n",
    "    if profiles[user][RIGHT[0]] == 0 and profiles[user][DUTCHPRESS[0]] == 0 and profiles[user][FLEMISHPRESS[0]] > MIN:\n",
    "        usersPerGroup[FLEMISHPRESS[0]].append(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(usersPerGroup[RIGHT[0]]),len(usersPerGroup[DUTCHPRESS[0]]),len(usersPerGroup[FLEMISHPRESS[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for other interesting retweeted users per group (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweetedUsers = []\n",
    "counter = 0\n",
    "for user in usersPerGroup[RIGHT[0]]:\n",
    "    counter += 1\n",
    "    squeal(str(counter)+\"/\"+str(len(usersPerGroup[RIGHT[0]])))\n",
    "    retweetedUsers.extend([ x[1] for x in groups if x[0] == user ])\n",
    "retweetedUsersDict  = pd.DataFrame(retweetedUsers).groupby(0).groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({key:len(retweetedUsersDict[key]) for key in sorted(retweetedUsersDict.keys(),\n",
    "                                                         key=lambda k:len(retweetedUsersDict[k]),reverse=True)}.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({key:len(retweetedUsersDict[key]) for key in sorted(retweetedUsersDict.keys(),\n",
    "                                                         key=lambda k:len(retweetedUsersDict[k]),reverse=True)}.items())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({key:len(retweetedUsersDict[key]) for key in sorted(retweetedUsersDict.keys(),\n",
    "                                                         key=lambda k:len(retweetedUsersDict[k]),reverse=True)}.items())[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get tweet texts of selected users (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIRTEXT = \"/home/erikt/projects/puregome/data/text/\"\n",
    "FILEPATTERN = \"2020060[1-7]\"\n",
    "USER = \"user\"\n",
    "TEXT = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedUsers = usersPerGroup[RIGHT[0]]+usersPerGroup[DUTCHPRESS[0]]+usersPerGroup[FLEMISHPRESS[0]]\n",
    "files = sorted(os.listdir(DATADIRTEXT))\n",
    "tweetTexts = {}\n",
    "for inFileName in files:\n",
    "    if re.search(FILEPATTERN,inFileName):\n",
    "        squeal(inFileName)\n",
    "        df = pd.read_csv(DATADIRTEXT+inFileName)\n",
    "        for i in range(0,len(df)):\n",
    "            user = df.iloc[i][USER]\n",
    "            if df.iloc[i][USER] in selectedUsers:\n",
    "                if not user in tweetTexts: tweetTexts[user] = df.iloc[i][TEXT]\n",
    "                else: tweetTexts[user] += \" \"+df.iloc[i][TEXT]\n",
    "tweetTexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize tweet texts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeKeywords(text):\n",
    "    for keyword in RIGHT+DUTCHPRESS+FLEMISHPRESS:\n",
    "        text = re.sub(r\"\\b\"+keyword.lower()+r\"\\b\",\" \",text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizedTweetTexts = {}\n",
    "tokenizer = TweetTokenizer()\n",
    "for user in tweetTexts.keys():\n",
    "    tokenizedTweetTexts[user] = \" \".join(tokenizer.tokenize(re.sub(r\"\\\\n\",\" \",removeKeywords(tweetTexts[user].lower()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tweet texts (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTFILENAME = \"groups-train.txt\"\n",
    "LABELPREFIX = \"__label__\"\n",
    "RIGHTTEXT = \"RIGHT\"\n",
    "DUTCHPRESSTEXT = \"DUTCHPRESS\"\n",
    "FLEMISHPRESSTEXT = \"FLEMISHPRESS\"\n",
    "\n",
    "outFile = open(OUTFILENAME,\"w\")\n",
    "for user in tokenizedTweetTexts:\n",
    "    if user in usersPerGroup[RIGHT[0]]: label = RIGHTTEXT\n",
    "    elif user in usersPerGroup[DUTCHPRESS[0]]: label = DUTCHPRESSTEXT\n",
    "    elif user in usersPerGroup[FLEMISHPRESS[0]]: label = FLEMISHPRESSTEXT\n",
    "    else: sys.exit(\"cannot happen!\")\n",
    "    print(LABELPREFIX+label,tokenizedTweetTexts[user],file=outFile)\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate fasttext on data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = \"groups-train.txt\"\n",
    "LARGEINT = 99999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttextData = []\n",
    "inFile = open(TRAINFILE,\"r\")\n",
    "for line in inFile: fasttextData.append(line.strip())\n",
    "inFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 300\n",
    "EPOCH = 100\n",
    "LR = 0.1\n",
    "N = 10\n",
    "TRAIN = \"TRAIN\"+str(int(random.random()*LARGEINT))\n",
    "TEST = \"TEST\"+str(int(random.random()*LARGEINT))\n",
    "\n",
    "predictionCounts = []\n",
    "predictionLabels = []\n",
    "for fold in range(0,N):\n",
    "    squeal(\"starting fold \"+str(fold))\n",
    "    testStart = round(fold*len(fasttextData)/N)\n",
    "    testEnd = round((fold+1)*len(fasttextData)/N)\n",
    "    trainFile = open(TRAIN,\"w\")\n",
    "    testFile = open(TEST,\"w\")\n",
    "    testData = []\n",
    "    for i in range(0,len(fasttextData)):\n",
    "        if i < testStart or i >= testEnd: print(fasttextData[i],file=trainFile)\n",
    "        else: \n",
    "            print(fasttextData[i],file=testFile)\n",
    "            testData.append(fasttextData[i])\n",
    "    testFile.close()\n",
    "    trainFile.close()\n",
    "    model = fasttext.train_supervised(TRAIN,dim=DIM,epoch=EPOCH,lr=LR) # ,pretrainedVectors=PRETRAINEDDIR+WIKIFILENAME)\n",
    "    predictionCounts.append([*model.test(TEST)])\n",
    "    predictionLabels.append(model.predict(testData))\n",
    "    os.unlink(TRAIN)\n",
    "    os.unlink(TEST)\n",
    "clear_output(wait=True)\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caseTotal = 0\n",
    "pTotal = 0\n",
    "rTotal = 0\n",
    "for i in range(0,len(predictionCounts)):\n",
    "    caseTotal += predictionCounts[i][0]\n",
    "    pTotal += predictionCounts[i][0]*predictionCounts[i][1]\n",
    "    rTotal += predictionCounts[i][0]*predictionCounts[i][2]\n",
    "print(\"cases: {0}; precision: {1}; recall: {2}\".format(caseTotal,round(pTotal/caseTotal,3),round(rTotal/caseTotal,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores: epoch=10 P=52; epoch=20 P=86; epoch=30 P=89; epoch=50 P=91; epoch=100 P=94.7; epoch=200 P=94.8; epoch=130 P=95.1; epoch=150 P=95.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCountsGold = {}\n",
    "for i in range(0,len(fasttextData)):\n",
    "    label = fasttextData[i].split()[0]\n",
    "    if label in labelCountsGold: labelCountsGold[label] += 1\n",
    "    else: labelCountsGold[label] = 1\n",
    "labelCountsGold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelCountsPredicted = {}\n",
    "for i in range(0,len(predictionLabels)):\n",
    "    for label in predictionLabels[i][0]:\n",
    "        if label[0] in labelCountsPredicted: labelCountsPredicted[label[0]] += 1\n",
    "        else: labelCountsPredicted[label[0]] = 1\n",
    "labelCountsPredicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(fasttextData)):\n",
    "    labelGold = fasttextData[i].split()[0]\n",
    "    labelPredicted = predictionLabels[i][0]\n",
    "    print(labelGold,labelPredicted)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply fasttext to other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import fasttext\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINFILE = \"groups-train.txt\"\n",
    "DIM = 300\n",
    "EPOCH = 100\n",
    "LR = 0.1\n",
    "RIGHTTEXT = \"RIGHT\"\n",
    "DUTCHPRESSTEXT = \"DUTCHPRESS\"\n",
    "FLEMISHPRESSTEXT = \"FLEMISHPRESS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(TRAINFILE,dim=DIM,epoch=EPOCH,lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200622-23.out.gz\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"text\"\n",
    "USER = \"user\"\n",
    "TESTFILEPATTERN = \"2020061[6-9]|2020062[0-2]\"\n",
    "DATADIRTEXT = \"/home/erikt/projects/puregome/data/text/\"\n",
    "\n",
    "files = sorted(os.listdir(DATADIRTEXT))\n",
    "tokenizer = TweetTokenizer()\n",
    "userTexts = {}\n",
    "for inFileName in files:\n",
    "    if re.search(TESTFILEPATTERN,inFileName):\n",
    "        squeal(inFileName)\n",
    "        df = pd.read_csv(DATADIRTEXT+inFileName)\n",
    "        for i in range(0,len(df)): \n",
    "            user = df.iloc[i][USER]\n",
    "            text = df.iloc[i][TEXT]\n",
    "            if user in userTexts: userTexts[user] += \" \"+text\n",
    "            else: userTexts[user] = text\n",
    "tokenizedTexts = {}\n",
    "for user in userTexts:\n",
    "    tokenizedTexts[user] = \" \".join(tokenizer.tokenize(re.sub(r\"\\\\n\",\" \",userTexts[user].lower())))\n",
    "predictedLabels = model.predict([tokenizedTexts[user] for user in tokenizedTexts.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"label\"\n",
    "CONFIDENCE = \"confidence\"\n",
    "\n",
    "predictedLabelsDict = {}\n",
    "counter = 0\n",
    "for user in tokenizedTexts:\n",
    "    predictedLabelsDict[user] = { LABEL:predictedLabels[0][counter][0], CONFIDENCE:predictedLabels[1][counter][0] }\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__DUTCHPRESS 0.8073545 ??? lees \" hard rock hotel amsterdam american swingt \" op nieuwsblik - https://t.co/uf3iz7hhpu - lees verder bij de bron van het artikel lees \" luuk de jong in slotfase beroofd van heldenrol \" op nieuwsblik - https://t.co/gyonwxvjmg - lees verder bij de bron van het artikel lees \" champions league start op 8 augustus \" op nieuwsblik - https://t.co/cwkkvjbfjm - lees verder bij de bron van het artikel lees \" ‘ pompeo naar hawaii voor overleg met china ’ \" op nieuwsblik - https://t.co/rge8fufq4w - de a\n"
     ]
    }
   ],
   "source": [
    "MAXCOUNTER = 1\n",
    "\n",
    "counter = 0\n",
    "for user in sorted(tokenizedTexts.keys(),key=lambda k:len(tokenizedTexts[k]),reverse=True):\n",
    "    print(predictedLabelsDict[user][LABEL],end=\" \")\n",
    "    print(predictedLabelsDict[user][CONFIDENCE],end=\" \")\n",
    "    if not user in profiles: print(\"???\",end=\" \")\n",
    "    elif profiles[user][RIGHT[0]] > 0: print(RIGHTTEXT,end=\" \")\n",
    "    elif profiles[user][FLEMISHPRESS[0]] > 0: print(FLEMISHPRESSTEXT,end=\" \")\n",
    "    elif profiles[user][DUTCHPRESS[0]] > 0: print(DUTCHPRESSTEXT,end=\" \")\n",
    "    else: print(\"?!?\",end=\" \")\n",
    "    print(tokenizedTexts[user][0:500])\n",
    "    counter += 1\n",
    "    if counter >= MAXCOUNTER: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXCOUNTER = 5000\n",
    "DATAFILEOUT = \"user-groups.csv\"\n",
    "ID = \"ID\"\n",
    "\n",
    "counter = 0\n",
    "outFile = open(DATAFILEOUT,\"w\")\n",
    "csvwriter = csv.DictWriter(outFile,[ID,USER,TEXT])\n",
    "for user in sorted(tokenizedTexts.keys(),key=lambda k:len(tokenizedTexts[k]),reverse=True):\n",
    "    csvwriter.writerow({ID:user,USER:user,TEXT:tokenizedTexts[user][0:1000]})\n",
    "    counter += 1\n",
    "    if counter >= MAXCOUNTER: break\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
