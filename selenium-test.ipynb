{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NU.nl comment harvester\n",
    "\n",
    "You need the Python library selenium (pip install selenium) and the geckodriver: https://github.com/mozilla/geckodriver/releases\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. collect the list of relevant urls in a text file (see below)\n",
    "2. run the first code block to load library imports and read the url file\n",
    "3. set the start id in the for loop in the fifth code block (first id is 0)\n",
    "4. run the fifth code block and wait\n",
    "5. the comments are stored in a set of files `[category]-[article_id].csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect list of relevant URLs:\n",
    "\n",
    "1. run the second code block to open https://www.nu.nl/tag/Coronavirus\n",
    "2. run the third code block to get the desired time period on the page\n",
    "3. copy-paste from browser window to LibreOffice Writer\n",
    "4. save as Flat ODT xml file (.fodt)\n",
    "5. `grep -Eo \"https://www.nu.nl/[A-Za-z\\-]+/[0-9]+/\" [copied file].fodt|sort|uniq > [url file].txt`\n",
    "\n",
    "wrt 3 and 4: this can also be done with Microsoft Word, open the `docx` file in a file extraction program (e.g., WinZip) and locate the relevant `.xml` file. Probably a selenium/xpath solution is also possible for step 3-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "def id_from_article_url(article_url):\n",
    "    res = re.search(r\"([0-9]+)/$\", article_url)\n",
    "    return res.group(1)\n",
    "\n",
    "def category_from_article_url(article_url):\n",
    "    res = re.search(r\"^https://www.nu.nl/([^\\/]+)/\", article_url)\n",
    "    return res.group(1)\n",
    "\n",
    "def talk_url_from_id(article_id):\n",
    "    return \"https://talk.nu.nl/embed/stream?asset_url=https%3A%2F%2Fwww.nu.nl%2Fartikel%2F\"+str(article_id)+\"%2Fredirect.html&initialWidth=601&childId=coral_talk_wrapper\"\n",
    "\n",
    "DRIVER = \"/usr/local/bin/geckodriver\"\n",
    "\n",
    "URLFILE = \"nunl_Feb_01-May_14.txt\"\n",
    "urls_from_file = open(URLFILE, encoding=\"utf8\")\n",
    "urls_all = urls_from_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download metadata: date, title, abstract, content\n",
    "import json\n",
    "import csv\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "\n",
    "start_id = 1106\n",
    "write_mode = 'w'\n",
    "if start_id > 0:\n",
    "    write_mode = 'a'\n",
    "\n",
    "options = Options()\n",
    "options.headless = True\n",
    "\n",
    "file_out = open('nunl_meta.csv', write_mode, encoding = 'utf8', newline = '')\n",
    "meta_out = csv.writer(file_out,\n",
    "                      delimiter = ',',\n",
    "                      quotechar = '\"',\n",
    "                      doublequote = True,\n",
    "                      quoting = csv.QUOTE_NONNUMERIC)\n",
    "if start_id == 0:\n",
    "    meta_out.writerow(['url','date','title','abstract','body'])\n",
    "\n",
    "na_out = open('nunl_nometa.txt', 'a')\n",
    "    #<meta name=\"og:type\" content=\"video.other\" />\n",
    "    \n",
    "for i in range(start_id,len(urls_all)):\n",
    "    clear_output(wait=True)\n",
    "    print(i, \"/\", len(urls_all)-1, urls_all[i])\n",
    "    driver = webdriver.Firefox(options=options, executable_path=DRIVER)\n",
    "    driver.get(urls_all[i])\n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "    node_type = driver.find_element_by_xpath(\"//head/meta[@name='og:type']\")\n",
    "    txt_type = node_type.get_attribute('content')\n",
    "    if \"video\" in txt_type:\n",
    "        na_out.write(urls_all[i]+\"\\n\")\n",
    "    else:\n",
    "        node_title = driver.find_element_by_xpath(\"//head/meta[@name='title']\")\n",
    "        txt_title = node_title.get_attribute('content')\n",
    "        node_abstract = driver.find_element_by_xpath(\"//head/meta[@name='description']\")\n",
    "        txt_abstract = node_abstract.get_attribute('content')\n",
    "        node_date = driver.find_element_by_xpath(\"//head/meta[@name='article:published_time']\")\n",
    "        txt_date = node_date.get_attribute('content')\n",
    "        print(txt_date,txt_title)\n",
    "        node_content = driver.find_element_by_xpath(\"//script[@type='application/ld+json']\")\n",
    "        json_content = node_content.get_attribute('innerHTML')\n",
    "        #print(node_content,json_content)\n",
    "        json_dict = json.loads(json_content)\n",
    "        #print(json_dict[\"articleBody\"])\n",
    "        meta_out.writerow([urls_all[i], txt_date, txt_title, txt_abstract, json_dict[\"articleBody\"]])\n",
    "    driver.quit()\n",
    "    \n",
    "file_out.close()\n",
    "na_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all articles from Corona tag page\n",
    "WEBPAGE = \"https://www.nu.nl/tag/Coronavirus\"\n",
    "driver = webdriver.Firefox(executable_path=DRIVER)\n",
    "driver.get(WEBPAGE)\n",
    "driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click 'show more articles' a few hundred times\n",
    "# afterwards copy and paste to extract urls\n",
    "for i in range(320):\n",
    "    clear_output(wait=True)\n",
    "    print(i)\n",
    "    morearticles_link = driver.find_element_by_xpath(\"//a[starts-with(@class,'block-more__link')]\")\n",
    "    try:\n",
    "        morearticles_link.click()\n",
    "    except StaleElementReferenceException as e:\n",
    "        # new xpath executed before DOM update from previous click finished\n",
    "        print(\"stale element\")\n",
    "        pass\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all reactions for single page\n",
    "morereactions = True\n",
    "while morereactions:\n",
    "    reaction_buttons = driver.find_elements_by_xpath(\"//button[contains(text(),'meer reacties')]\")\n",
    "    print(\"number of buttons:\", len(reaction_buttons))\n",
    "    if len(reaction_buttons) > 0:\n",
    "        python_button = reaction_buttons[0]\n",
    "        try:\n",
    "            python_button.click()\n",
    "        except StaleElementReferenceException as e:\n",
    "            # new xpath executed before DOM update from previous click finished\n",
    "            pass\n",
    "    else:\n",
    "        morereactions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT = r\"^Comment__commentContainer\"\n",
    "AUTHORNAME = r\"^AuthorName__name\"\n",
    "TIMESTAMP = r\"^CommentTimestamp__timestamp\"\n",
    "TEXT = r\"^Comment__content\"\n",
    "CLASS = \"class\"\n",
    "TITLE = \"title\"\n",
    "XPATHID = \"../../..\"\n",
    "XPATHPARENTID = \"../../../../..\"\n",
    "\n",
    "for current_id in range(0,len(urls_all)):\n",
    "    current_url = urls_all[current_id]\n",
    "    \n",
    "    print(current_id, \"/\", len(urls_all)-1, current_url, \"id:\", id_from_article_url(current_url), \"category:\", category_from_article_url(current_url))\n",
    "    \n",
    "    article_id = id_from_article_url(current_url)\n",
    "    article_cat = category_from_article_url(current_url)\n",
    "    \n",
    "    OUTPUTFILE = f\"{article_cat}-{article_id}.csv\"\n",
    "    \n",
    "    WEBPAGE = talk_url_from_id(article_id)\n",
    "    \n",
    "    driver = webdriver.Firefox(executable_path=DRIVER)\n",
    "    driver.get(WEBPAGE)\n",
    "    driver.execute_script(\"window.scrollBy(0,document.body.scrollHeight)\")\n",
    "    \n",
    "    # wait for spinner to finish\n",
    "    while len(driver.find_elements_by_xpath(\"//div[starts-with(@class, 'Spinner__container')]\")) != 0:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # click more reactions button until all reactions are shown\n",
    "    print(\"click more reaction buttons\") \n",
    "    morereactions = True\n",
    "    while morereactions:\n",
    "        reaction_buttons = driver.find_elements_by_xpath(\"//button[contains(text(),'meer reacties')]\")\n",
    "        print(\"number of buttons:\", len(reaction_buttons))\n",
    "        if len(reaction_buttons) > 0:\n",
    "            python_button = reaction_buttons[0]\n",
    "            try:\n",
    "                python_button.click()\n",
    "            except StaleElementReferenceException as e:\n",
    "                # new xpath executed before DOM update from previous click finished\n",
    "                pass\n",
    "        else:\n",
    "            morereactions = False\n",
    "    \n",
    "    # wait for spinner to finish\n",
    "    while len(driver.find_elements_by_xpath(\"//div[starts-with(@class, 'Spinner__container')]\")) != 0:\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # locate all reactions and store in csv file\n",
    "    print(\"locate all reactions\")  \n",
    "    data = []\n",
    "    alldivs = driver.find_elements_by_xpath(\"//div\")\n",
    "    divnr = 0\n",
    "    lenall = len(alldivs)\n",
    "    for e in alldivs:\n",
    "        divnr += 1\n",
    "        eClass = e.get_attribute(CLASS)\n",
    "        if re.search(COMMENT,eClass):\n",
    "            eId = e.find_elements_by_xpath(XPATHID)[0].id\n",
    "            parent = e.find_elements_by_xpath(XPATHPARENTID)[0].id\n",
    "            authorName = \"\"\n",
    "            timeStamp = \"\"\n",
    "            text = \"\"\n",
    "            for f in e.find_elements_by_xpath(\".//*\"):\n",
    "                fClass = f.get_attribute(CLASS)\n",
    "                if fClass != eClass:\n",
    "                    if re.search(AUTHORNAME,fClass): \n",
    "                        authorName = f.text\n",
    "                    elif re.search(TIMESTAMP,fClass): \n",
    "                        timeStamp = f.get_attribute(TITLE)\n",
    "                    elif re.search(TEXT,fClass): \n",
    "                        text = re.sub(\"\\n\",\" \",f.text)\n",
    "                        break\n",
    "            data.append({\"id\":eId,\"name\":authorName,\"date\":timeStamp,\"text\":text,\"parent\":parent})\n",
    "            clear_output(wait=True)\n",
    "            print(current_id, \"/\", len(urls_all), current_url, \"id:\", id_from_article_url(current_url), \"category:\", category_from_article_url(current_url))\n",
    "            print(f\"processed: {len(data)} ({divnr} / {lenall} divs)\")\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(OUTPUTFILE,index=False)\n",
    "    driver.quit()\n",
    "    print(\"finished processing\", current_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
