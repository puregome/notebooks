{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Echo chambers\n",
    "\n",
    "Find groups of users which often send out the same tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functions for retrieving user groups from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"../data/text/\"\n",
    "TEXT = \"text\"\n",
    "USER = \"user\"\n",
    "MIN_TWEET_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: print(text)\n",
    "\n",
    "        \n",
    "def remove_url_suffix(tweet):\n",
    "    return(re.sub(\"\\s*https?://.*$\",\"\",tweet,flags=re.IGNORECASE))\n",
    "\n",
    "\n",
    "def remove_rt_prefix(tweet):\n",
    "    return(re.sub(\"^RT[^:]*:\\s*\",\"\",tweet))\n",
    "\n",
    "\n",
    "def read_tweets(file_pattern):\n",
    "    file_names = sorted(os.listdir(DATADIR))\n",
    "    users_per_tweet = {}\n",
    "    tweets_per_user = {}\n",
    "    nbr_of_tweets = 0\n",
    "    for file_name in file_names:\n",
    "        if re.search(file_pattern,file_name):\n",
    "            squeal(file_name)\n",
    "            df = pd.read_csv(DATADIR+file_name)\n",
    "            nbr_of_tweets += len(df)\n",
    "            for i in range(0, len(df)):\n",
    "                user = df.iloc[i][\"user\"]\n",
    "                text = remove_rt_prefix(remove_url_suffix(df.iloc[i][\"text\"])).strip()\n",
    "                if len(text) >= MIN_TWEET_LENGTH:\n",
    "                    if text not in users_per_tweet: \n",
    "                        users_per_tweet[text] = [user]\n",
    "                    elif user not in users_per_tweet[text]: \n",
    "                        users_per_tweet[text].append(user)\n",
    "                    if user not in tweets_per_user: \n",
    "                        tweets_per_user[user] = 1\n",
    "                    else: \n",
    "                        tweets_per_user[user] += 1\n",
    "    return users_per_tweet,tweets_per_user,nbr_of_tweets\n",
    "\n",
    "\n",
    "def read_tweets_of_user_group(file_pattern, user_group, reverse=False):\n",
    "    file_names = sorted(os.listdir(DATADIR))\n",
    "    texts = []\n",
    "    for file_name in file_names:\n",
    "        if re.search(file_pattern,file_name):\n",
    "            df = pd.read_csv(DATADIR+file_name)\n",
    "            if not reverse:\n",
    "                df = df[df[USER].isin(user_group)]\n",
    "            else:\n",
    "                df = df[~df[USER].isin(user_group)]\n",
    "            texts.extend(list(df[TEXT]))\n",
    "            squeal(file_name)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_pairs(users_per_tweet):\n",
    "    user_pairs = {}\n",
    "    for user_group in users_per_tweet.values():\n",
    "        for i in range(1,len(user_group)):\n",
    "            for j in range(i+1,len(user_group)):\n",
    "                pair = \" \".join([user_group[i],user_group[j]])\n",
    "                if not pair in user_pairs: user_pairs[pair] = 1\n",
    "                else: user_pairs[pair] += 1\n",
    "    return(user_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_user_pairs(user_pairs):\n",
    "    for pair in sorted(user_pairs.keys(),key=lambda p:user_pairs[p],reverse=True)[:20]:\n",
    "        print(f\"{user_pairs[pair]} {pair}\")\n",
    "    return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 10\n",
    "\n",
    "def make_user_groups(user_pairs):\n",
    "    user_groups = {}\n",
    "    for pair in user_pairs:\n",
    "        if user_pairs[pair] >= THRESHOLD:\n",
    "            user1,user2 = pair.split()\n",
    "            if not user1 in user_groups and not user2 in user_groups:\n",
    "                user_groups[user1] = (user1,user2)\n",
    "                user_groups[user2] = (user1,user2)\n",
    "            elif user1 in user_groups and not user2 in user_groups:\n",
    "                user_groups[user1] = tuple(set(user_groups[user1]+(user2,)))\n",
    "                for e in user_groups[user1]: user_groups[e] = user_groups[user1]\n",
    "                user_groups[user2] = user_groups[user1]\n",
    "            elif not user1 in user_groups and user2 in user_groups:\n",
    "                user_groups[user2] = tuple(set(user_groups[user2]+(user1,)))\n",
    "                for e in user_groups[user2]: user_groups[e] = user_groups[user2]\n",
    "                user_groups[user1] = user_groups[user2]\n",
    "            else:\n",
    "                user_groups[user1] = tuple(set(user_groups[user1]+user_groups[user2]))\n",
    "                for e in user_groups[user1]: user_groups[e] = user_groups[user1]\n",
    "                for e in user_groups[user2]: user_groups[e] = user_groups[user1]\n",
    "    return(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_USER_GROUP_SIZE = 5\n",
    "\n",
    "def show_user_groups(user_groups):\n",
    "    seen = {}\n",
    "    first_user = \"\"\n",
    "    for user in sorted(user_groups.keys(), key=lambda u: len(user_groups[u]), reverse=True):\n",
    "        if not user in seen and len(user_groups[user]) >= MIN_USER_GROUP_SIZE:\n",
    "            if first_user == \"\":\n",
    "                first_user = user\n",
    "            print(len(user_groups[user]), [u for u in sorted(user_groups[user],key=lambda u:u.lower())][:10])\n",
    "            for u in user_groups[user]: \n",
    "                seen[u] = True\n",
    "    return first_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_group(user_groups):\n",
    "    largest_group = []\n",
    "    for user in user_groups:\n",
    "        if len(user_groups[user]) > len(largest_group): \n",
    "            largest_group = user_groups[user]\n",
    "    return(largest_group)\n",
    "\n",
    "def compute_overlap(group1,group2):\n",
    "    return(len(set(group1) & set(group2)))\n",
    "\n",
    "def get_nbr_of_tweets(user_group,tweets_per_user):\n",
    "    nbr_of_tweets = 0\n",
    "    for user in user_group: nbr_of_tweets += tweets_per_user[user]\n",
    "    return(nbr_of_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGFILE = \"echo-chambers.csv\"\n",
    "\n",
    "def write_log(line):\n",
    "    logfile = open(LOGFILE, \"a\")\n",
    "    print(line, file=logfile)\n",
    "    logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_day(date, previous_largest_user_group=[], last_seen={}):\n",
    "    users_per_tweet, tweets_per_user, nbr_of_tweets = read_tweets(date)\n",
    "    if nbr_of_tweets == 0: \n",
    "        return ...\n",
    "    user_pairs = get_user_pairs(users_per_tweet)\n",
    "    user_groups = make_user_groups(user_pairs)\n",
    "    largest_user_group = get_largest_group(user_groups)\n",
    "    largest_user_group_tweet_count = get_nbr_of_tweets(largest_user_group,tweets_per_user)\n",
    "    if len(previous_largest_user_group) == 0:\n",
    "        overlap_count = 0\n",
    "    else:\n",
    "        overlap_count = compute_overlap(previous_largest_user_group,largest_user_group)\n",
    "    print(f\"date: {date}; largest group size: {len(largest_user_group)}; overlap with previous day: {overlap_count};\",\n",
    "          f\"tweets group: {largest_user_group_tweet_count}; all tweets: {nbr_of_tweets}; member: {largest_user_group[0]}\")\n",
    "    write_log(f\"{date},{len(largest_user_group)},{overlap_count},{largest_user_group_tweet_count},{nbr_of_tweets},{largest_user_group[0]}\")\n",
    "    for user in largest_user_group: \n",
    "        last_seen[user] = date\n",
    "    return user_groups, largest_user_group, last_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Functions for drawing wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(term_frequencies):\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(term_frequencies, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"wordcloud.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = (\"aan aangezien absoluut achter af afgelopen al algemeen alle alleen allemaal allen alles als alsnog alsof altijd ander andere anders \"\n",
    "\"antwoord april art augustus basis beeld beetje begin beginnen begint begrijp behalve beide bekend belangrijk ben benieuwd bent bepaalde \"\n",
    "\"beperkt best beste betekent beter betreft bezig bij bijna bijv bijvoorbeeld blijf blijft blijkbaar blijkt blijven boven bovendien brengen buiten buurt \"\n",
    "\"buurten compleet daadwerkelijk daar daarbij daardoor daarmee daarna daarnaast daarom daarvan daarvoor dacht dag dagen dan dat de december deel denk \"\n",
    "\"denken denkt deze dezelfde die dingen direct dit doe doen doet door doordat drie duidelijk dus duurt echt echte echter edit een eens eerder eerlijk eerst \"\n",
    "\"eerste eigen eigenlijk eind eindelijk elk elkaar elke en ene enige enkel enkele enorm er erg ergens erger ervan ervoor etc even extra februari fijn flink fout \"\n",
    "\"ga gaan gaat gebeuren gebeurt gebruik gebruiken gebruikt gedaan geef geeft geen gegaan gegeven gehad gehouden gek geldt geleden gelijk gelukkig gemaakt genoeg genomen geval \"\n",
    "\"geven gevoel gevolgen geweest gewoon geworden gezegd gezien ging gisteren goed goede gooi gooien graag groep groot grootste  grote groter haar had hadden halen half hard heb hebben \"\n",
    "\"hebt heeft heel heen helaas hele helemaal helpt hem hen het hetzelfde hier hij hoe hoeft hoeveel hoeveelheid hoeven hoger hoog hoop hoor hopelijk hopen horen hou houd houden \"\n",
    "\"houdt https huidige hun idee ieder iedere iedereen iemand iets ik in inderdaad ineens informatie ingevoerd inmiddels ipv is ivm ja januari jaren je jezelf jij jonge \"\n",
    "\"jou jouw juist juli jullie juni kan kans kant keer kijk kijken klaar klein kleine klinkt klopt kom komen komende komt kon kreeg krijg krijgen krijgt kun kunnen \"\n",
    "\"kunt kwam laag laat laatste land landen lang lange langer langs last lastig laten later leer lees lekker letterlijk leven lezen liever liggen ligt lijken lijkt \"\n",
    "\"logisch loopt lopen los m'n maak maakt maand maar maart mag maken makkelijk makkelijker man manier me mee meer meerdere meest meeste mei men mensen met \"\n",
    "\"meteen middel mij mijn minder misschien mocht moeilijk moest moet moeten mogelijk mogen moment momenteel morgen na naar naast namelijk \"\n",
    "\"natuurlijk nauwelijk nauwelijks nederland nederlander nederlanders nederlandse nee neem neemt nemen nergens net niemand niet niets nieuw nieuwe niks nl no \"\n",
    "\"nodig nog nogal nooit normaal normale nou november nu of ofzo oktober om omdat on ondanks onder ondertussen ongeveer ons onze ooit ook op openlijk open over overal \"\n",
    "\"overigens pa paar pakken pas per persoonlijk plek plekken praten precies prima probeer proberen punt qua raken redelijk relatief rest roepen rond rt samen september serieus simpelweg \"\n",
    "\"sinds slechts snap snel sneller som sommige soms soort sowieso staan staat steed steeds stel stellen sterk strak straks stuk te tegen ten terecht terug \"\n",
    "\"terwijl teveel the tijd tijden tijdens to toch toe toen tot totaal trouwens tussen twee u uit uiteindelijk uiteraard uur uw vaak vaker vallen valt van vanaf vandaag vanuit vanwege \"\n",
    "\"vast vd veel velen ver verder verhaal verkeerd verplicht verschil verschillende vertrouwen vervolgens verwacht verwachten via vind vinden vindt voelt vol \"\n",
    "\"voldoende volgen volgende volgens volledig vond voor vooral voorbij voordat voorkomen vorige vraag vrij vrijdag vrijwel vroeg vs waar waarbij waardoor waarin waarom \"\n",
    "\"waarschijnlijk waarvan wachten wanneer want waren was wat we week weer weet weg weinig wel welke wellicht werd werden werken werkt weten wie wij wil willen \"\n",
    "\"wilt woord word worden wordt x200b z'n zag zal zat ze zeer zeg zeggen zegt zei zeker zelf zelfs zetten zich zichzelf zie zien ziet zij zijn zin zit zitten zo zo'n \"\n",
    "\"zoal zoals zodat zodra zoeken zolang zonder zorgen zorgt zou zouden zoveel zover zowel zullen zwaar één\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=50, max_words=200, background_color=\"white\", width=600, height=300, collocations=False, normalize_plurals=False, relative_scaling=1, regexp=r\"\\w[\\w']+\",\n",
    "                      stopwords=(stopwords).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_OF_TOKENS = 200\n",
    "\n",
    "def make_wordcloud(posts, nbr_of_tokens=NBR_OF_TOKENS):\n",
    "    text = \" \".join(posts)\n",
    "    text = re.sub(r'\\\\n', ' ', text)\n",
    "    token_freqs = Counter(tokenize(text.lower()).split())\n",
    "    top = [(t,token_freqs[t]) for t in sorted(token_freqs.keys(),key=lambda t:token_freqs[t], reverse=True) \n",
    "                              if t not in stopwords.split() and re.search(\"[a-z]\", t) and len(t) > 1]\n",
    "    show_wordcloud(wordcloud.generate_from_frequencies(dict(top[:NBR_OF_TOKENS])))\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position(top, token_filter):\n",
    "    for i in range(0, len(top)):\n",
    "        if re.search(token_filter, top[i][0]):\n",
    "            print(f\"{i+1}. {top[i][1]} {top[i][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return(\" \".join(TweetTokenizer().tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(query, date_pattern):\n",
    "    file_names = sorted(os.listdir(DATADIR))\n",
    "    selected_file_names = [file_name for file_name in file_names if re.search(date_pattern, file_name) ]\n",
    "    posts = []\n",
    "    for file_name in selected_file_names:\n",
    "        df = pd.read_csv(DATADIR + file_name, index_col=IDSTR)\n",
    "        posts.extend(list(df[df[TEXT].str.contains(query, flags=re.IGNORECASE)].loc[:,TEXT]))\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Processing a single date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = \"20220114\"\n",
    "\n",
    "user_groups, _, _  = process_day(DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_user = show_user_groups(user_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_GROUP = user_groups[first_user]\n",
    "\n",
    "texts = read_tweets_of_user_group(DATE, USER_GROUP)\n",
    "top_group = make_wordcloud(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = read_tweets_of_user_group(DATE, USER_GROUP, reverse=True)\n",
    "top_not_group = make_wordcloud(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. t-scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/erikt/projects/newsgac/fasttext-runs\")\n",
    "import tscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscores = tscore.computeTscore({ \"totalFreq\": sum([ x[1] for x in top_group ]), \n",
    "                                 \"nbrOfWords\": len(top_group),\n",
    "                                 \"wordFreqs\": { x[0]: x[1] for x in top_group }}, \n",
    "                               { \"totalFreq\": sum([ x[1] for x in top_not_group ]), \n",
    "                                 \"nbrOfWords\": len(top_not_group),\n",
    "                                 \"wordFreqs\": { x[0]: x[1] for x in top_not_group }})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscores = [ (word, tscores[word]) for word in sorted(tscores.keys(), key=lambda x: tscores[x], reverse=True) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(wordcloud.generate_from_frequencies(dict(tscores[:NBR_OF_TOKENS])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing several dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_largest_user_group = []\n",
    "last_seen = {}\n",
    "for month in range(202112, 202201):\n",
    "    for day in range(1, 32):\n",
    "        date = str(month)+str(day).zfill(2)\n",
    "        user_groups, previous_largest_user_group, last_seen = process_day(date, previous_largest_user_group, last_seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing multiple dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATEFORMAT = \"%Y%m%d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(LOGFILE, index_col=\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTFILENAME = \"echo-chambers.png\"\n",
    "\n",
    "plt.subplots(figsize=(10,4))\n",
    "plt.plot_date([datetime.datetime.strptime(str(d),DATEFORMAT) for d in df.index],\n",
    "              [100*df.iloc[d][\"group_tweets\"]/df.iloc[d][\"all_tweets\"] for d in range(0,len(df))],\n",
    "              xdate=True,fmt=\"-\")\n",
    "plt.title(\"Percentage of tweets per day from largest echo team on Dutch Twitter\")\n",
    "plt.savefig(PLOTFILENAME)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
