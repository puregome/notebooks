{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coverage tests for collected tweets\n",
    "\n",
    "Estimate what percentage of Dutch tweets are collected by twiqs.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/home/erikt/projects/puregome/data/text-202006/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeal(text=None):\n",
    "    clear_output(wait=True)\n",
    "    if not text is None: print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extimate coverage with reply ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDSTR = \"id_str\"\n",
    "INREPLYTOSTATUSIDSTR = \"in_reply_to_status_id_str\"\n",
    "\n",
    "def getReplyPercentage(filePattern,filePatternReference):\n",
    "    files = sorted(os.listdir(DATADIR))\n",
    "    targetFiles = {}\n",
    "    counter = 0\n",
    "    for inFileName in files:\n",
    "        if re.search(filePattern,inFileName):\n",
    "            counter += 1\n",
    "            df = pd.read_csv(DATADIR+inFileName,dtype={INREPLYTOSTATUSIDSTR:object})\n",
    "            counter += len(df)\n",
    "            for idStr in df[INREPLYTOSTATUSIDSTR]:\n",
    "                if type(idStr) == type(\"abc\"):\n",
    "                    targetFiles[idStr] = False\n",
    "    for inFileName in files:\n",
    "        if re.search(filePatternReference,inFileName):\n",
    "            df = pd.read_csv(DATADIR+inFileName,dtype={IDSTR:object})\n",
    "            for idStr in df[IDSTR]: \n",
    "                if idStr in targetFiles: targetFiles[idStr] = True\n",
    "            squeal(inFileName)\n",
    "    squeal(\"\")\n",
    "    return(len([x for x in targetFiles if targetFiles[x]]),len(targetFiles),counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePattern = \"20200611\"\n",
    "lenPart,lenAll,counter = getReplyPercentage(filePattern,filePatternReference)\n",
    "print(\"target date: {0}; reference: {1}; percentage: {2}%; total count: {3}\".format(filePattern,filePatternReference,\\\n",
    "                                                                                    round(100*lenPart/lenAll,1),counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query token language coverage\n",
    "\n",
    "How many of the tweets containing a query token are written in Dutch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIRCOVERAGE = \"./\"\n",
    "FILEPATTERNCOVERAGE = \"20200701-0[0-57-9]|20200701-1[0-7]|20200701-2[12]\"\n",
    "TEXT = \"text\"\n",
    "LANG = \"lang\"\n",
    "DUTCH = \"dutch\"\n",
    "OTHER = \"other\"\n",
    "UNKNOWN = \"unknown\"\n",
    "\n",
    "def coverage(token):\n",
    "    files = sorted(os.listdir(DATADIRCOVERAGE))    \n",
    "    langs = {}\n",
    "    for inFileName in files:\n",
    "        if re.search(FILEPATTERNCOVERAGE,inFileName):\n",
    "            df = pd.read_csv(inFileName,compression=\"gzip\")\n",
    "            for i in range(0,len(df)):\n",
    "                try:\n",
    "                    if re.search(r'\\b'+token+r'\\b',df.iloc[i][TEXT]):\n",
    "                        lang = df.iloc[i][LANG]\n",
    "                        if lang in langs: langs[lang] += 1\n",
    "                        else: langs[lang] = 1\n",
    "                except: pass\n",
    "    return(langs)\n",
    "\n",
    "def coverageDutchOld(token):\n",
    "    langs = coverage(token)\n",
    "    summary = {DUTCH:0,OTHER:0}\n",
    "    for lang in langs:\n",
    "        if lang == DUTCH: summary[DUTCH] += langs[lang]\n",
    "        elif lang != UNKNOWN: summary[OTHER] += langs[lang]\n",
    "    if summary[DUTCH] == 0: return(0)\n",
    "    else: return(summary[DUTCH]/(summary[DUTCH]+summary[OTHER]))\n",
    "    \n",
    "def coverageDutch(token,tweetsLang):\n",
    "    summary = {DUTCH:0,OTHER:0}\n",
    "    for text in tweetsLang.keys():\n",
    "        if token in text.split():\n",
    "            if tweetsLang[text] == DUTCH: summary[DUTCH] += 1\n",
    "            else: summary[OTHER] += 1\n",
    "    if summary[DUTCH] == 0: return(0)\n",
    "    else: return(summary[DUTCH]/(summary[DUTCH]+summary[OTHER]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIPTOKENS = \"rt amsterdam the woof ros√© de en in is van me ben we via nos juno br staysafe spnmarais arcinho kamafotos youtube \\\n",
    "update amp bts_twt blackpink baby and omg best ygofficialblink on ig goal wonderland ten instagram top you wonwoo daniel \\\n",
    "1advancehbdmaheshbabu with open of for vlive ver to netherlands jungkook up joonie jennie from super my kpop klopp gt vlog queen \\\n",
    "nct love by boy bergwijn zoe twitter teen te stop so more master lee hahahaha gogh god ever bruyne an ameen tik this stream slap \\\n",
    "school one oh ni man level ko just im hi here help he duit deluxe day da better beef be art aameen zico yes wtf wtaf winner will \\\n",
    "vn video vc total that team superior street snoop re pubg pink oopsie no new ne namkook moots moon miss maam look link la ka juliet \\\n",
    "jinnie jinkook his hey hahahahaha goals go gemes fan een dus do black back am all aaron zombie yep yeah woop won win weekend war un \\\n",
    "tingz thank tekken taekook sterling star stan sorry snap sex sb19official robin poor pls out onvres onde omfg ok ns now not need \\\n",
    "moonbin mood min mein maap maaf live let legend left laptop ke kangen jimin jan it iq how hot hoodie holy her hee happy hahahahah \\\n",
    "haha ha groningen got gg gente funny ft ff fc engineer eh ee down donde dm damn cute chief can bro big bestie banget at as \\\n",
    "xiaojun gee advancehbdmaheshbabu weareoneexo kang tier straykids kun dream tiktok than kevin jin haechan golden boys actorvijay \\\n",
    "unnie tweets pop only mochi missing like genie don bts alice it's he's i'm let's zone tok she mo made kids howyoulikethat era el \\\n",
    "duo drop angel allah urstrulymahesh winwin exo official dance hours heels has el edit proud\".split()\n",
    "SKIPTOKENS = r\", . ! ? : ; - ( ) @ # $ / [ ] < >  ' + = | .. ... ` & \\\" \".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATTERN = \"202006\"\n",
    "TEXT = \"text\"\n",
    "BASELEXICON = \"\".split()\n",
    "ALLCOUNT = 596893 # 5623571 # 22798953 # 22798953\n",
    "LEFT200 = 61599\n",
    "LEFT100 = 80258 # 349598 # 1724488 # 1231745\n",
    "LEFT200100 = 43622 # 179089 # 801171 # 732526\n",
    "LEFT300200100 = 34583 # 130887 # 557205 # 531315\n",
    "LEFT400300200100 = 29528 # 109099 # 451231 # 439565\n",
    "LEFT500400300200100 = 0 # # 394609 # 387447\n",
    "LEFT600500400300200100 = 0 # # 357003 # 352026\n",
    "LEFT700600500400300200100 = 0 # # 329054 # 324628\n",
    "LEFT800700600500400300200100 = 0 # # 306745 # 303422\n",
    "\n",
    "results = []\n",
    "\n",
    "def cleanupText(text):\n",
    "    text = re.sub(r\"\\\\n\",\" \",text.lower())\n",
    "    text = re.sub(r\"[#@]\",\" \",text)\n",
    "    return(text)\n",
    "\n",
    "def tokenize(tokenizer,text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return(\" \".join(sorted(list(set(tokens)))))\n",
    "\n",
    "def cleanupTokens(text):\n",
    "    tokens = set(text.split())\n",
    "    newTokens = []\n",
    "    for token in tokens:\n",
    "        if not re.search(r\"http\",token) and not token in SKIPTOKENS: newTokens.append(token)\n",
    "    return(\" \".join(newTokens))\n",
    "\n",
    "def getTweetsFile(inFileName,dataDir):\n",
    "    tweets = {}\n",
    "    squeal(inFileName)\n",
    "    tokenizer = TweetTokenizer()\n",
    "    df = pd.read_csv(dataDir+inFileName)\n",
    "    for i in range(0,len(df)):\n",
    "        text = cleanupText(df.iloc[i][TEXT])\n",
    "        text = tokenize(tokenizer,text)\n",
    "        text = cleanupTokens(text)\n",
    "        if LANG in df: tweets[text] = df.iloc[i][LANG]\n",
    "        else: tweets[text] = True\n",
    "    return(tweets)\n",
    "\n",
    "def collectResult(result):\n",
    "    global results\n",
    "    results.append(result)\n",
    "\n",
    "def getTweetsParallel(filePattern=FILEPATTERN,dataDir=DATADIR):\n",
    "    global results\n",
    "    files = sorted(os.listdir(dataDir))\n",
    "    inFileNames = []\n",
    "    for inFileName in files:\n",
    "        if re.search(filePattern,inFileName): inFileNames.append(inFileName)\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply(getTweetsFile,args=[inFileName,dataDir]) for inFileName in inFileNames]\n",
    "    pool.close()\n",
    "    while len(results) > 1:\n",
    "        toBeDeleted = []\n",
    "        for i in range(0,len(results),2):\n",
    "            if i < len(results)-1: \n",
    "                results[i] = {**results[i],**results[i+1]}\n",
    "                toBeDeleted.append(i+1)\n",
    "        toBeDeleted.reverse()\n",
    "        for i in toBeDeleted: del(results[i])\n",
    "    return(results[0])\n",
    "\n",
    "def getTweets(filePattern=FILEPATTERN,dataDir=DATADIR):\n",
    "    files = sorted(os.listdir(dataDir))\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tweets = {}\n",
    "    for inFileName in files:\n",
    "        if re.search(filePattern,inFileName):\n",
    "            squeal(inFileName)\n",
    "            df = pd.read_csv(dataDir+inFileName)\n",
    "            for i in range(0,len(df)):\n",
    "                text = cleanupText(df.iloc[i][TEXT])\n",
    "                text = tokenize(tokenizer,text)\n",
    "                text = cleanupTokens(text)\n",
    "                if LANG in df: tweets[text] = df.iloc[i][LANG]\n",
    "                else: tweets[text] = True\n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200630-23.out.gz\n"
     ]
    }
   ],
   "source": [
    "tweetsLangOrg = getTweets(filePattern=FILEPATTERNCOVERAGE,dataDir=DATADIRCOVERAGE)\n",
    "tweetsOrg = getTweetsParallel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsLang = dict(tweetsLangOrg)\n",
    "tweets = dict(tweetsOrg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000/5241310 12 dat 0.8404066073697586\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 0.5\n",
    "MAXLEXICONSIZE = 100\n",
    "\n",
    "lexicon = {token:0 for token in list(BASELEXICON)}\n",
    "processedTweetCounts = []\n",
    "tokenCoverage = {}\n",
    "skipTokens = {}\n",
    "coverageDutchScore = \"\"\n",
    "while len(lexicon) < MAXLEXICONSIZE:\n",
    "    frequencies = {}\n",
    "    processedTweetCount = 0\n",
    "    counter = 0\n",
    "    toBeDeleted = []\n",
    "    if len(lexicon) > 0: lastToken = list(lexicon.keys())[-1]\n",
    "    else: lastToken = \"\"\n",
    "    for text in tweets.keys():\n",
    "        counter += 1\n",
    "        if counter%10000 == 0: squeal(str(counter)+\"/\"+str(len(tweets))+\" \"+str(len(lexicon))+\" \"+lastToken+\" \"+str(coverageDutchScore))\n",
    "        tokens = text.split()\n",
    "        if len(set(lexicon.keys()).intersection(tokens)) >= 1:\n",
    "            toBeDeleted.append(text)\n",
    "        else:\n",
    "            processedTweetCount += 1\n",
    "            for token in tokens:\n",
    "                if token in frequencies: frequencies[token] += 1\n",
    "                else: frequencies[token] = 1\n",
    "    for text in toBeDeleted:\n",
    "        del(tweets[text])\n",
    "    toBeDeletedLang = []\n",
    "    for text in tweetsLang.keys():\n",
    "        tokens = text.split()\n",
    "        if len(set(lexicon).intersection(tokens)) >= 1:\n",
    "            toBeDeletedLang.append(text)\n",
    "    for text in toBeDeletedLang:\n",
    "        del(tweetsLang[text])    \n",
    "    processedTweetCounts.append(processedTweetCount)\n",
    "    frequencies = {k:frequencies[k] for k in sorted(frequencies.keys(),key=lambda k:frequencies[k],reverse=True)}\n",
    "    for token in frequencies:\n",
    "        if not token in skipTokens:\n",
    "            coverageDutchScore = coverageDutch(token,tweetsLang)\n",
    "            squeal(str(len(lexicon))+\" \"+str(processedTweetCount)+\" \"+str(len(skipTokens))+\" \"+str(frequencies[token])+\" \"+str(coverageDutchScore)+\" \"+token)\n",
    "            if coverageDutchScore >= THRESHOLD:\n",
    "                lexicon[token] = coverageDutchScore\n",
    "                break\n",
    "            else: skipTokens[token] = coverageDutchScore\n",
    "    squeal(str(len(lexicon))+\" \"+str(processedTweetCount)+\" \"+str(len(skipTokens))+\" \"+str(frequencies[list(lexicon.keys())[-1]])+\" \"+list(lexicon.keys())[-1]+\" \"+str(coverageDutchScore))\n",
    "    \n",
    "\" \".join(list(lexicon.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'de een ik het je niet van dat voor die zijn maar geen wel ze eens waar worden hebben heb heerlijk waarom zien zonder moeten niets gelijk iedereen ziet anders geniet lijkt eindelijk ingeborgvraagt weten vester zij mogelijk elkaar volgende zeggen 000zit zitten vooral gewenst airbnbird eigenlijk beterschap geleden houden geweest gecondoleerd vreselijk alweer weinig omdat dingen ziek hoeveel wachten'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(list(lexicon.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in list(skipTokens.keys())[:20]:\n",
    "    print(skipTokens[token],token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile = open(\"tmp.txt\",\"w\")\n",
    "print(\"track=\",end=\"\",file=outFile)\n",
    "print(\",\".join([lexicon[i] for i in range(0,len(lexicon)) if i % 4 == 0 or i % 4 == 3]),file=outFile)\n",
    "print(\"track=\",end=\"\",file=outFile)\n",
    "print(\",\".join([lexicon[i] for i in range(0,len(lexicon)) if i % 4 == 1 or i % 4 == 2]),file=outFile)\n",
    "outFile.close()\n",
    "print(\" \".join([lexicon[i] for i in range(0,len(lexicon)) if i % 4 == 0 or i % 4 == 3]))\n",
    "print(\" \".join([lexicon[i] for i in range(0,len(lexicon)) if i % 4 == 1 or i % 4 == 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsCopy = tweets\n",
    "for i in range(0,len(lexicon)):\n",
    "    if i % 100 == 0: print(i,len(tweetsCopy))\n",
    "    toBeDeleted = []\n",
    "    for text in tweetsCopy.keys():\n",
    "        tokens = text.split()\n",
    "        if len(set([lexicon[i]]).intersection(tokens)) > 0:\n",
    "            toBeDeleted.append(text)\n",
    "    for text in toBeDeleted:\n",
    "        del(tweetsCopy[text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i,len(tweetsCopy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join([str(processedTweetCounts[i]) for i in range(0,800,100)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFrequencies = {k:frequencies[k] for k in sorted(frequencies.keys(),key=lambda k:frequencies[k],reverse=True)}\n",
    "topTokens800 = [k for k in sortedFrequencies.keys() if re.search(r\"[a-z0-9]\",k) or \\\n",
    "                                                       (len(k) > 1 and k != \"..\" and k != \"...\" or\n",
    "                                                       (len(k) == 1 and ord(k) >= 127000 and ord(k) < 130000))][0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\",\".join(topTokens100))\n",
    "print(\",\".join(topTokens200))\n",
    "print(\",\".join(topTokens300))\n",
    "print(\",\".join(topTokens400))\n",
    "print(\",\".join(topTokens500))\n",
    "print(\",\".join(topTokens600))\n",
    "print(\",\".join(topTokens700))\n",
    "print(\",\".join(topTokens800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in topTokens100+topTokens200+topTokens300+topTokens400: print(token,end=\",\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
